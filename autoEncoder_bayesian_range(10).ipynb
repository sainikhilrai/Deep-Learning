{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoEncoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainikhilrai/Deep-Learning/blob/master/autoEncoder_bayesian_range(10).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVBNC-wEkPF5",
        "colab_type": "code",
        "outputId": "d151bcbe-a008-454b-8227-d6923504ee28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jVnUqk9kYYI",
        "colab_type": "code",
        "outputId": "2a447f8d-19aa-46a3-bfef-22afcc753c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# OM SRI SAI RAM\n",
        "\n",
        "# Descr: Basic or experiment code to perform loss reserve prediction\n",
        "\n",
        "##############################\n",
        "# Step 1 : Reading Sample Data\n",
        "##############################\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
        "np.random.seed(7)\n",
        "random.seed(7)\n",
        "accident_years = np.arange(2000,2010,1)\n",
        "development_years = np.arange(0,10,1)\n",
        "\"\"\"\n",
        "triangle = np.array(([1232,946,520,722,316,165,48,14],\n",
        "                   [1469,1201,708,845, 461,235,56,18],\n",
        "                   [1652,1416,959,954,605,287,69,21],\n",
        "                   [1831,1634,1124,1087,725,314,79,24],\n",
        "                   [2074,1919,1330,1240,756,359,91,28],\n",
        "                   [2434,2263,1661,1540,909,432,109,33],\n",
        "                   [2810,2108,1544,1565,924,439,111,34],\n",
        "                   [3072,2614,1785,1810,1069,508,128,39]))\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntriangle = np.array(([1232,946,520,722,316,165,48,14],\\n                   [1469,1201,708,845, 461,235,56,18],\\n                   [1652,1416,959,954,605,287,69,21],\\n                   [1831,1634,1124,1087,725,314,79,24],\\n                   [2074,1919,1330,1240,756,359,91,28],\\n                   [2434,2263,1661,1540,909,432,109,33],\\n                   [2810,2108,1544,1565,924,439,111,34],\\n                   [3072,2614,1785,1810,1069,508,128,39]))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6FAnnAVMB_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "triangle = np.array(([5012,3257,2638,898,1734,2642,1828,599,54,172],\n",
        "                   [106,4179,1111,5270,3116,1817,-103,673,535,0],\n",
        "                   [3410,5582,4881,2268,2594,3479,649,603,0,0],\n",
        "                   [5655,5990,4211,5500,2159,2658,984,0,0,0],\n",
        "                   [1092,8473,6271,6333,3786,255,0,0,0,0],\n",
        "                   [1513,4932,5257,1233,2917,0,0,0,0,0],\n",
        "                   [557,3463,6926,1368,0,0,0,0,0,0],\n",
        "                   [1351,5596,6165,0,0,0,0,0,0,0],\n",
        "                   [3133,2262,0,0,0,0,0,0,0,0],\n",
        "                   [2063,0,0,0,0,0,0,0,0,0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQzOcV8XQUD4",
        "colab_type": "code",
        "outputId": "62691316-84f6-4906-cdfe-6ace26ad9ca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "triangle.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbs79f94kkHR",
        "colab_type": "code",
        "outputId": "2e0b6230-bf8a-447d-f322-476cf608dfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "print(' Accident Years',accident_years)\n",
        "print(' Developement Years',development_years)\n",
        "print(' Input', triangle)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accident Years [2000 2001 2002 2003 2004 2005 2006 2007 2008 2009]\n",
            " Developement Years [0 1 2 3 4 5 6 7 8 9]\n",
            " Input [[5012 3257 2638  898 1734 2642 1828  599   54  172]\n",
            " [ 106 4179 1111 5270 3116 1817 -103  673  535    0]\n",
            " [3410 5582 4881 2268 2594 3479  649  603    0    0]\n",
            " [5655 5990 4211 5500 2159 2658  984    0    0    0]\n",
            " [1092 8473 6271 6333 3786  255    0    0    0    0]\n",
            " [1513 4932 5257 1233 2917    0    0    0    0    0]\n",
            " [ 557 3463 6926 1368    0    0    0    0    0    0]\n",
            " [1351 5596 6165    0    0    0    0    0    0    0]\n",
            " [3133 2262    0    0    0    0    0    0    0    0]\n",
            " [2063    0    0    0    0    0    0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV9CyhyHkreC",
        "colab_type": "code",
        "outputId": "0dd87f29-c248-4d47-9017-16b774cad6c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# Cum calc\n",
        "C = np.zeros(shape=(np.shape(triangle)[0],np.shape(triangle)[1]))\n",
        "for i in range(np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(triangle)[1]):\n",
        "        C[i,j] = sum(triangle[i,:j+1])\n",
        "\n",
        "print('C',C)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C [[ 5012.  8269. 10907. 11805. 13539. 16181. 18009. 18608. 18662. 18834.]\n",
            " [  106.  4285.  5396. 10666. 13782. 15599. 15496. 16169. 16704. 16704.]\n",
            " [ 3410.  8992. 13873. 16141. 18735. 22214. 22863. 23466. 23466. 23466.]\n",
            " [ 5655. 11645. 15856. 21356. 23515. 26173. 27157. 27157. 27157. 27157.]\n",
            " [ 1092.  9565. 15836. 22169. 25955. 26210. 26210. 26210. 26210. 26210.]\n",
            " [ 1513.  6445. 11702. 12935. 15852. 15852. 15852. 15852. 15852. 15852.]\n",
            " [  557.  4020. 10946. 12314. 12314. 12314. 12314. 12314. 12314. 12314.]\n",
            " [ 1351.  6947. 13112. 13112. 13112. 13112. 13112. 13112. 13112. 13112.]\n",
            " [ 3133.  5395.  5395.  5395.  5395.  5395.  5395.  5395.  5395.  5395.]\n",
            " [ 2063.  2063.  2063.  2063.  2063.  2063.  2063.  2063.  2063.  2063.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxnU6YP_U2Qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newTriangle= np.zeros([10,10])\n",
        "for i in range(triangle.shape[0]):\n",
        "  for j in range(triangle.shape[1]-i):\n",
        "    newTriangle[i,j]= C[i,j]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHPL35WhU8oF",
        "colab_type": "code",
        "outputId": "ebc1af43-972a-433b-a410-f26c1f86ea92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "newTriangle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5012.,  8269., 10907., 11805., 13539., 16181., 18009., 18608.,\n",
              "        18662., 18834.],\n",
              "       [  106.,  4285.,  5396., 10666., 13782., 15599., 15496., 16169.,\n",
              "        16704.,     0.],\n",
              "       [ 3410.,  8992., 13873., 16141., 18735., 22214., 22863., 23466.,\n",
              "            0.,     0.],\n",
              "       [ 5655., 11645., 15856., 21356., 23515., 26173., 27157.,     0.,\n",
              "            0.,     0.],\n",
              "       [ 1092.,  9565., 15836., 22169., 25955., 26210.,     0.,     0.,\n",
              "            0.,     0.],\n",
              "       [ 1513.,  6445., 11702., 12935., 15852.,     0.,     0.,     0.,\n",
              "            0.,     0.],\n",
              "       [  557.,  4020., 10946., 12314.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.],\n",
              "       [ 1351.,  6947., 13112.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.],\n",
              "       [ 3133.,  5395.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.],\n",
              "       [ 2063.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioSsHmm7kvOY",
        "colab_type": "code",
        "outputId": "6058a1f8-40c2-45ba-fa0f-7a87cae3243b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "l_encode = LabelEncoder()\n",
        "l_encode.fit(accident_years)\n",
        "a_yr = l_encode.transform(accident_years)\n",
        "l_encode.fit(development_years)\n",
        "dev_yr = l_encode.transform(development_years)\n",
        "\n",
        "print(a_yr)\n",
        "print(dev_yr)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "[0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyery7QfkzKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = []\n",
        "for i in range(np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(triangle)[1]-i):\n",
        "        train_data.append([a_yr[i],dev_yr[j],C[i,j]])\n",
        "        \n",
        "test_data = []\n",
        "for i in range(1,np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(triangle)[1]-i,np.shape(triangle)[1]):\n",
        "        test_data.append([a_yr[i],dev_yr[j],C[i,j]])\n",
        "\n",
        " \n",
        "#convert trainData and testData into numpyArray\n",
        "#train_data = np.array(train_data)\n",
        "test_data = np.array(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95UOy6VzAcfn",
        "colab_type": "code",
        "outputId": "81204551-1f77-4e1c-83dc-0b750e2e20fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16817
        }
      },
      "source": [
        "######################################################\n",
        "############## code for the bootstrap  ###############\n",
        "######################################################\n",
        "\n",
        "\n",
        "\n",
        "#code for generating the other triangle\n",
        "\n",
        "\n",
        "for iteratrion in range(100):\n",
        "  newTriangle2= np.zeros((triangle.shape[0],triangle.shape[1]))\n",
        "  for i in range(triangle.shape[0]):\n",
        "    for j in range(triangle.shape[1]-i):\n",
        "        newTriangle2[i,j]= newTriangle[i,j] + np.random.randint(-10,10)  #add some random value\n",
        "  difference= newTriangle2-newTriangle   #take the error\n",
        "  \n",
        "  \n",
        "  \n",
        "  #get the column of the difference\n",
        "  columncount=triangle.shape[1]\n",
        "  for columnIndex in range(triangle.shape[1]):\n",
        "    column= difference[:columncount,columnIndex]   #get the column of the error matrix\n",
        "    np.random.shuffle(column)                      # shuffle the column\n",
        "    newTriangle[:columncount,columnIndex]= newTriangle[:columncount,columnIndex]+ column    #get the bootstrap sample\n",
        "    columncount= columncount-1\n",
        "    \n",
        "  #print(\"Bootstrap New Triangle:\\n\")\n",
        "  print(newTriangle)\n",
        "  \n",
        "  \"\"\"  #find the cumulative sum of new Bootstrap New Triangle\n",
        "  newC = np.zeros(shape=(np.shape(triangle)[0],np.shape(triangle)[1]))\n",
        "  for i in range(np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(newTriangle)[1]):\n",
        "        newC[i,j] = sum(newTriangle[i,:j+1])\n",
        "  \"\"\"\n",
        "\n",
        "  \n",
        "  \n",
        "  #combine the new triangle with the already existing training set\n",
        "  \n",
        "  for i in range(np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(triangle)[1]-i):\n",
        "        train_data.append([a_yr[i],dev_yr[j],newTriangle[i,j]])\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5009.  8265. 10898. 11811. 13536. 16173. 18008. 18612. 18662. 18832.]\n",
            " [  111.  4291.  5390. 10672. 13788. 15603. 15494. 16170. 16700.     0.]\n",
            " [ 3419.  8986. 13868. 16150. 18725. 22216. 22853. 23470.     0.     0.]\n",
            " [ 5651. 11639. 15849. 21355. 23505. 26175. 27155.     0.     0.     0.]\n",
            " [ 1101.  9567. 15827. 22162. 25952. 26203.     0.     0.     0.     0.]\n",
            " [ 1504.  6435. 11696. 12928. 15845.     0.     0.     0.     0.     0.]\n",
            " [  566.  4014. 10950. 12304.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1360.  6940. 13111.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3130.  5395.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2060.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5011.  8264. 10894. 11816. 13538. 16169. 18013. 18619. 18662. 18826.]\n",
            " [  108.  4293.  5391. 10674. 13785. 15604. 15494. 16175. 16695.     0.]\n",
            " [ 3423.  8985. 13874. 16150. 18720. 22223. 22851. 23462.     0.     0.]\n",
            " [ 5653. 11638. 15845. 21351. 23496. 26174. 27149.     0.     0.     0.]\n",
            " [ 1104.  9566. 15829. 22167. 25959. 26198.     0.     0.     0.     0.]\n",
            " [ 1499.  6443. 11692. 12931. 15839.     0.     0.     0.     0.     0.]\n",
            " [  565.  4007. 10940. 12296.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1361.  6947. 13109.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3139.  5391.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2058.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5001.  8271. 10891. 11823. 13530. 16159. 18014. 18620. 18664. 18832.]\n",
            " [  111.  4294.  5392. 10677. 13794. 15610. 15502. 16170. 16701.     0.]\n",
            " [ 3425.  8989. 13871. 16156. 18717. 22228. 22859. 23459.     0.     0.]\n",
            " [ 5660. 11631. 15845. 21358. 23494. 26168. 27144.     0.     0.     0.]\n",
            " [ 1110.  9567. 15828. 22161. 25964. 26197.     0.     0.     0.     0.]\n",
            " [ 1501.  6443. 11692. 12935. 15839.     0.     0.     0.     0.     0.]\n",
            " [  561.  4009. 10945. 12286.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1360.  6938. 13113.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3129.  5395.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2067.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5008.  8272. 10888. 11821. 13528. 16156. 18008. 18613. 18660. 18824.]\n",
            " [  103.  4302.  5396. 10674. 13799. 15615. 15499. 16174. 16697.     0.]\n",
            " [ 3415.  8992. 13861. 16157. 18726. 22233. 22849. 23457.     0.     0.]\n",
            " [ 5668. 11638. 15852. 21365. 23493. 26170. 27147.     0.     0.     0.]\n",
            " [ 1113.  9560. 15835. 22158. 25954. 26198.     0.     0.     0.     0.]\n",
            " [ 1496.  6444. 11684. 12926. 15832.     0.     0.     0.     0.     0.]\n",
            " [  567.  4013. 10947. 12285.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1358.  6931. 13108.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3120.  5386.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2067.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4998.  8281. 10897. 11828. 13528. 16156. 18001. 18614. 18652. 18830.]\n",
            " [  104.  4302.  5388. 10666. 13792. 15619. 15494. 16168. 16695.     0.]\n",
            " [ 3405.  8991. 13853. 16158. 18726. 22233. 22847. 23451.     0.     0.]\n",
            " [ 5668. 11632. 15857. 21367. 23488. 26164. 27155.     0.     0.     0.]\n",
            " [ 1103.  9552. 15843. 22157. 25960. 26188.     0.     0.     0.     0.]\n",
            " [ 1490.  6436. 11690. 12934. 15829.     0.     0.     0.     0.     0.]\n",
            " [  566.  4016. 10940. 12292.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1355.  6940. 13114.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3116.  5393.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2064.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5001.  8273. 10901. 11832. 13534. 16160. 17991. 18608. 18651. 18826.]\n",
            " [  111.  4297.  5380. 10674. 13794. 15623. 15503. 16176. 16692.     0.]\n",
            " [ 3405.  8996. 13856. 16149. 18734. 22227. 22850. 23450.     0.     0.]\n",
            " [ 5663. 11637. 15854. 21373. 23478. 26162. 27152.     0.     0.     0.]\n",
            " [ 1096.  9553. 15847. 22164. 25958. 26179.     0.     0.     0.     0.]\n",
            " [ 1498.  6432. 11685. 12934. 15821.     0.     0.     0.     0.     0.]\n",
            " [  557.  4012. 10941. 12283.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1356.  6943. 13121.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3110.  5397.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2057.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5003.  8277. 10891. 11828. 13531. 16150. 17990. 18605. 18645. 18821.]\n",
            " [  108.  4300.  5388. 10668. 13801. 15628. 15512. 16177. 16697.     0.]\n",
            " [ 3407.  8988. 13863. 16142. 18726. 22219. 22848. 23453.     0.     0.]\n",
            " [ 5658. 11640. 15844. 21364. 23481. 26165. 27161.     0.     0.     0.]\n",
            " [ 1096.  9559. 15848. 22159. 25951. 26184.     0.     0.     0.     0.]\n",
            " [ 1499.  6430. 11682. 12938. 15813.     0.     0.     0.     0.     0.]\n",
            " [  555.  4017. 10947. 12280.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1362.  6937. 13126.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3118.  5406.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2051.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5010.  8279. 10898. 11836. 13539. 16144. 17989. 18612. 18637. 18822.]\n",
            " [  114.  4304.  5397. 10665. 13793. 15628. 15516. 16183. 16688.     0.]\n",
            " [ 3416.  8979. 13866. 16151. 18727. 22219. 22849. 23444.     0.     0.]\n",
            " [ 5648. 11637. 15834. 21354. 23473. 26169. 27168.     0.     0.     0.]\n",
            " [ 1097.  9554. 15846. 22160. 25941. 26183.     0.     0.     0.     0.]\n",
            " [ 1493.  6425. 11681. 12942. 15820.     0.     0.     0.     0.     0.]\n",
            " [  563.  4016. 10949. 12276.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1353.  6945. 13131.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3124.  5409.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2057.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5000.  8287. 10895. 11828. 13545. 16150. 17979. 18616. 18644. 18828.]\n",
            " [  111.  4294.  5394. 10670. 13798. 15619. 15518. 16183. 16688.     0.]\n",
            " [ 3422.  8976. 13870. 16146. 18734. 22222. 22856. 23442.     0.     0.]\n",
            " [ 5646. 11630. 15830. 21361. 23479. 26176. 27164.     0.     0.     0.]\n",
            " [ 1104.  9557. 15855. 22166. 25939. 26179.     0.     0.     0.     0.]\n",
            " [ 1491.  6425. 11680. 12950. 15813.     0.     0.     0.     0.     0.]\n",
            " [  560.  4012. 10946. 12266.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1361.  6937. 13125.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3127.  5401.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2047.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5000.  8291. 10893. 11835. 13536. 16152. 17972. 18609. 18642. 18819.]\n",
            " [  107.  4300.  5387. 10672. 13798. 15617. 15524. 16188. 16688.     0.]\n",
            " [ 3416.  8973. 13865. 16140. 18736. 22214. 22865. 23448.     0.     0.]\n",
            " [ 5655. 11632. 15824. 21368. 23481. 26180. 27165.     0.     0.     0.]\n",
            " [ 1107.  9557. 15848. 22166. 25942. 26186.     0.     0.     0.     0.]\n",
            " [ 1484.  6423. 11671. 12958. 15820.     0.     0.     0.     0.     0.]\n",
            " [  562.  4008. 10953. 12265.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1357.  6943. 13131.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3130.  5410.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2042.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5003.  8292. 10892. 11839. 13540. 16144. 17968. 18603. 18639. 18815.]\n",
            " [  108.  4297.  5385. 10678. 13807. 15609. 15518. 16194. 16693.     0.]\n",
            " [ 3416.  8982. 13869. 16133. 18732. 22223. 22866. 23446.     0.     0.]\n",
            " [ 5647. 11629. 15822. 21374. 23472. 26181. 27170.     0.     0.     0.]\n",
            " [ 1104.  9553. 15852. 22169. 25940. 26194.     0.     0.     0.     0.]\n",
            " [ 1479.  6419. 11666. 12951. 15817.     0.     0.     0.     0.     0.]\n",
            " [  565.  4011. 10950. 12255.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1356.  6950. 13131.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3134.  5410.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2034.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4994.  8283. 10883. 11830. 13542. 16135. 17970. 18606. 18640. 18819.]\n",
            " [  108.  4296.  5375. 10671. 13813. 15605. 15508. 16192. 16698.     0.]\n",
            " [ 3416.  8985. 13870. 16123. 18723. 22219. 22864. 23439.     0.     0.]\n",
            " [ 5639. 11626. 15831. 21381. 23468. 26188. 27175.     0.     0.     0.]\n",
            " [ 1100.  9560. 15861. 22173. 25935. 26185.     0.     0.     0.     0.]\n",
            " [ 1477.  6422. 11675. 12957. 15826.     0.     0.     0.     0.     0.]\n",
            " [  570.  4017. 10953. 12253.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1347.  6956. 13136.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3140.  5405.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2033.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4993.  8276. 10882. 11835. 13533. 16136. 17967. 18612. 18644. 18824.]\n",
            " [  100.  4293.  5381. 10677. 13815. 15596. 15516. 16193. 16704.     0.]\n",
            " [ 3419.  8987. 13875. 16128. 18726. 22213. 22868. 23444.     0.     0.]\n",
            " [ 5643. 11625. 15837. 21388. 23459. 26185. 27166.     0.     0.     0.]\n",
            " [ 1095.  9561. 15870. 22169. 25930. 26191.     0.     0.     0.     0.]\n",
            " [ 1469.  6417. 11667. 12955. 15824.     0.     0.     0.     0.     0.]\n",
            " [  566.  4014. 10957. 12245.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1347.  6963. 13132.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3135.  5402.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2025.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4993.  8267. 10882. 11825. 13526. 16128. 17962. 18607. 18651. 18822.]\n",
            " [  106.  4299.  5376. 10685. 13807. 15590. 15523. 16193. 16700.     0.]\n",
            " [ 3415.  8993. 13879. 16137. 18726. 22220. 22876. 23445.     0.     0.]\n",
            " [ 5643. 11625. 15828. 21389. 23464. 26175. 27162.     0.     0.     0.]\n",
            " [ 1098.  9555. 15866. 22162. 25925. 26187.     0.     0.     0.     0.]\n",
            " [ 1475.  6419. 11674. 12952. 15822.     0.     0.     0.     0.     0.]\n",
            " [  571.  4010. 10951. 12249.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1352.  6961. 13141.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3141.  5403.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2019.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4999.  8270. 10879. 11829. 13526. 16127. 17953. 18606. 18641. 18815.]\n",
            " [   97.  4306.  5380. 10677. 13797. 15597. 15524. 16186. 16703.     0.]\n",
            " [ 3406.  8986. 13877. 16140. 18719. 22211. 22869. 23447.     0.     0.]\n",
            " [ 5640. 11629. 15826. 21398. 23454. 26167. 27170.     0.     0.     0.]\n",
            " [ 1092.  9560. 15865. 22166. 25925. 26195.     0.     0.     0.     0.]\n",
            " [ 1482.  6409. 11675. 12949. 15812.     0.     0.     0.     0.     0.]\n",
            " [  567.  4015. 10952. 12240.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1342.  6970. 13146.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3133.  5408.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2019.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4999.  8265. 10871. 11822. 13522. 16126. 17943. 18605. 18635. 18810.]\n",
            " [  100.  4307.  5388. 10672. 13789. 15587. 15533. 16193. 16694.     0.]\n",
            " [ 3404.  8988. 13883. 16142. 18718. 22206. 22864. 23447.     0.     0.]\n",
            " [ 5635. 11623. 15827. 21402. 23457. 26157. 27171.     0.     0.     0.]\n",
            " [ 1098.  9553. 15867. 22175. 25925. 26185.     0.     0.     0.     0.]\n",
            " [ 1473.  6411. 11671. 12943. 15808.     0.     0.     0.     0.     0.]\n",
            " [  564.  4023. 10961. 12249.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1348.  6976. 13154.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3130.  5407.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2009.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4990.  8267. 10872. 11828. 13525. 16126. 17939. 18601. 18625. 18818.]\n",
            " [  106.  4314.  5388. 10662. 13795. 15595. 15539. 16190. 16694.     0.]\n",
            " [ 3411.  8990. 13873. 16149. 18718. 22205. 22857. 23448.     0.     0.]\n",
            " [ 5632. 11617. 15833. 21400. 23464. 26150. 27166.     0.     0.     0.]\n",
            " [ 1097.  9545. 15860. 22171. 25925. 26194.     0.     0.     0.     0.]\n",
            " [ 1469.  6412. 11679. 12936. 15811.     0.     0.     0.     0.     0.]\n",
            " [  554.  4028. 10960. 12258.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1353.  6968. 13150.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3136.  5405.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2000.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4983.  8268. 10880. 11837. 13522. 16126. 17935. 18597. 18633. 18812.]\n",
            " [  103.  4319.  5384. 10661. 13786. 15592. 15533. 16181. 16686.     0.]\n",
            " [ 3417.  8984. 13871. 16156. 18719. 22201. 22861. 23447.     0.     0.]\n",
            " [ 5630. 11613. 15832. 21398. 23456. 26159. 27171.     0.     0.     0.]\n",
            " [ 1088.  9548. 15860. 22172. 25923. 26197.     0.     0.     0.     0.]\n",
            " [ 1470.  6420. 11671. 12945. 15806.     0.     0.     0.     0.     0.]\n",
            " [  544.  4022. 10968. 12248.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1361.  6977. 13145.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3127.  5406.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2002.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4973.  8264. 10886. 11843. 13519. 16135. 17927. 18601. 18629. 18815.]\n",
            " [  111.  4309.  5374. 10664. 13785. 15597. 15525. 16177. 16681.     0.]\n",
            " [ 3422.  8981. 13864. 16163. 18723. 22202. 22858. 23448.     0.     0.]\n",
            " [ 5634. 11615. 15825. 21405. 23450. 26167. 27162.     0.     0.     0.]\n",
            " [ 1089.  9544. 15859. 22169. 25926. 26196.     0.     0.     0.     0.]\n",
            " [ 1478.  6410. 11677. 12950. 15798.     0.     0.     0.     0.     0.]\n",
            " [  544.  4026. 10965. 12253.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1364.  6980. 13137.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3124.  5396.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 2001.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4980.  8264. 10892. 11841. 13527. 16142. 17919. 18594. 18624. 18819.]\n",
            " [  103.  4311.  5370. 10665. 13784. 15590. 15530. 16170. 16674.     0.]\n",
            " [ 3413.  8988. 13872. 16155. 18715. 22193. 22859. 23447.     0.     0.]\n",
            " [ 5643. 11617. 15830. 21404. 23454. 26173. 27159.     0.     0.     0.]\n",
            " [ 1094.  9547. 15866. 22177. 25931. 26195.     0.     0.     0.     0.]\n",
            " [ 1473.  6402. 11674. 12946. 15790.     0.     0.     0.     0.     0.]\n",
            " [  550.  4032. 10965. 12252.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1364.  6976. 13128.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3114.  5405.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1993.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4984.  8272. 10897. 11834. 13517. 16142. 17917. 18602. 18628. 18821.]\n",
            " [   98.  4304.  5364. 10658. 13782. 15595. 15527. 16173. 16677.     0.]\n",
            " [ 3406.  8986. 13868. 16161. 18713. 22187. 22861. 23451.     0.     0.]\n",
            " [ 5633. 11623. 15826. 21412. 23462. 26176. 27168.     0.     0.     0.]\n",
            " [ 1087.  9556. 15862. 22184. 25933. 26202.     0.     0.     0.     0.]\n",
            " [ 1471.  6403. 11682. 12955. 15792.     0.     0.     0.     0.     0.]\n",
            " [  551.  4041. 10966. 12244.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1363.  6972. 13131.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3108.  5402.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1985.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4985.  8277. 10901. 11836. 13508. 16149. 17926. 18604. 18628. 18820.]\n",
            " [  107.  4294.  5363. 10654. 13776. 15591. 15522. 16166. 16685.     0.]\n",
            " [ 3398.  8995. 13862. 16167. 18710. 22186. 22855. 23451.     0.     0.]\n",
            " [ 5623. 11621. 15817. 21405. 23453. 26183. 27176.     0.     0.     0.]\n",
            " [ 1093.  9552. 15855. 22177. 25937. 26209.     0.     0.     0.     0.]\n",
            " [ 1472.  6396. 11689. 12953. 15790.     0.     0.     0.     0.     0.]\n",
            " [  556.  4047. 10972. 12243.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1371.  6967. 13122.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3112.  5392.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1988.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4979.  8282. 10894. 11841. 13510. 16157. 17916. 18610. 18633. 18814.]\n",
            " [   97.  4285.  5364. 10653. 13785. 15599. 15531. 16172. 16689.     0.]\n",
            " [ 3388.  8991. 13870. 16173. 18717. 22181. 22858. 23443.     0.     0.]\n",
            " [ 5616. 11625. 15826. 21395. 23454. 26181. 27171.     0.     0.     0.]\n",
            " [ 1084.  9547. 15856. 22184. 25937. 26215.     0.     0.     0.     0.]\n",
            " [ 1478.  6389. 11698. 12952. 15799.     0.     0.     0.     0.     0.]\n",
            " [  547.  4044. 10972. 12252.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1373.  6964. 13120.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3102.  5389.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1987.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4982.  8275. 10903. 11832. 13513. 16147. 17909. 18617. 18627. 18813.]\n",
            " [   92.  4287.  5355. 10655. 13782. 15599. 15534. 16179. 16689.     0.]\n",
            " [ 3386.  8987. 13861. 16163. 18720. 22183. 22861. 23450.     0.     0.]\n",
            " [ 5607. 11625. 15830. 21398. 23451. 26179. 27178.     0.     0.     0.]\n",
            " [ 1083.  9537. 15853. 22181. 25936. 26218.     0.     0.     0.     0.]\n",
            " [ 1484.  6386. 11700. 12958. 15803.     0.     0.     0.     0.     0.]\n",
            " [  542.  4046. 10979. 12249.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1379.  6961. 13123.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3101.  5384.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1992.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4985.  8284. 10893. 11832. 13512. 16153. 17915. 18623. 18619. 18807.]\n",
            " [   84.  4288.  5350. 10654. 13788. 15598. 15528. 16175. 16694.     0.]\n",
            " [ 3387.  8996. 13865. 16165. 18716. 22187. 22861. 23450.     0.     0.]\n",
            " [ 5610. 11622. 15838. 21407. 23441. 26169. 27175.     0.     0.     0.]\n",
            " [ 1080.  9529. 15858. 22176. 25941. 26213.     0.     0.     0.     0.]\n",
            " [ 1488.  6380. 11701. 12963. 15796.     0.     0.     0.     0.     0.]\n",
            " [  542.  4041. 10988. 12245.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1386.  6957. 13123.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3106.  5378.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1983.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4992.  8276. 10886. 11832. 13506. 16157. 17906. 18621. 18628. 18816.]\n",
            " [   75.  4278.  5347. 10654. 13786. 15600. 15524. 16165. 16684.     0.]\n",
            " [ 3389.  9001. 13866. 16173. 18715. 22183. 22870. 23447.     0.     0.]\n",
            " [ 5613. 11626. 15828. 21401. 23435. 26168. 27184.     0.     0.     0.]\n",
            " [ 1077.  9535. 15860. 22170. 25936. 26208.     0.     0.     0.     0.]\n",
            " [ 1495.  6370. 11705. 12972. 15795.     0.     0.     0.     0.     0.]\n",
            " [  539.  4034. 10988. 12240.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1391.  6948. 13123.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3114.  5382.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1987.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4998.  8276. 10895. 11831. 13507. 16151. 17896. 18617. 18619. 18813.]\n",
            " [   83.  4268.  5344. 10656. 13779. 15607. 15522. 16155. 16674.     0.]\n",
            " [ 3395.  8994. 13870. 16163. 18724. 22188. 22877. 23454.     0.     0.]\n",
            " [ 5605. 11629. 15837. 21401. 23440. 26176. 27179.     0.     0.     0.]\n",
            " [ 1076.  9535. 15860. 22174. 25936. 26213.     0.     0.     0.     0.]\n",
            " [ 1498.  6363. 11701. 12973. 15803.     0.     0.     0.     0.     0.]\n",
            " [  539.  4031. 10992. 12230.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1389.  6949. 13117.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3112.  5378.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1985.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5001.  8280. 10902. 11830. 13510. 16149. 17901. 18612. 18627. 18822.]\n",
            " [   91.  4260.  5341. 10651. 13781. 15608. 15526. 16145. 16676.     0.]\n",
            " [ 3400.  8997. 13875. 16162. 18732. 22194. 22867. 23462.     0.     0.]\n",
            " [ 5601. 11623. 15832. 21399. 23437. 26183. 27187.     0.     0.     0.]\n",
            " [ 1071.  9531. 15854. 22183. 25929. 26219.     0.     0.     0.     0.]\n",
            " [ 1500.  6361. 11696. 12982. 15807.     0.     0.     0.     0.     0.]\n",
            " [  530.  4040. 10999. 12223.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1383.  6945. 13110.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3112.  5376.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1982.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4995.  8274. 10895. 11823. 13508. 16140. 17909. 18609. 18633. 18821.]\n",
            " [   83.  4263.  5334. 10646. 13776. 15600. 15521. 16147. 16668.     0.]\n",
            " [ 3398.  9004. 13870. 16166. 18735. 22189. 22875. 23465.     0.     0.]\n",
            " [ 5602. 11622. 15825. 21396. 23437. 26188. 27191.     0.     0.     0.]\n",
            " [ 1080.  9531. 15858. 22182. 25932. 26211.     0.     0.     0.     0.]\n",
            " [ 1499.  6352. 11690. 12985. 15799.     0.     0.     0.     0.     0.]\n",
            " [  532.  4049. 10998. 12222.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1387.  6936. 13106.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3107.  5370.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1981.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5001.  8275. 10895. 11818. 13512. 16147. 17912. 18599. 18637. 18814.]\n",
            " [   90.  4255.  5332. 10655. 13783. 15598. 15523. 16153. 16673.     0.]\n",
            " [ 3407.  9001. 13862. 16174. 18737. 22191. 22877. 23468.     0.     0.]\n",
            " [ 5598. 11617. 15815. 21398. 23437. 26191. 27200.     0.     0.     0.]\n",
            " [ 1086.  9536. 15863. 22187. 25932. 26206.     0.     0.     0.     0.]\n",
            " [ 1503.  6350. 11687. 12978. 15796.     0.     0.     0.     0.     0.]\n",
            " [  537.  4052. 11001. 12216.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1385.  6941. 13096.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3107.  5375.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1974.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5002.  8266. 10900. 11823. 13507. 16137. 17906. 18604. 18631. 18816.]\n",
            " [   83.  4249.  5324. 10651. 13788. 15599. 15519. 16154. 16668.     0.]\n",
            " [ 3406.  9005. 13859. 16175. 18745. 22197. 22877. 23461.     0.     0.]\n",
            " [ 5590. 11611. 15813. 21405. 23446. 26200. 27190.     0.     0.     0.]\n",
            " [ 1083.  9535. 15859. 22189. 25938. 26199.     0.     0.     0.     0.]\n",
            " [ 1509.  6354. 11684. 12971. 15796.     0.     0.     0.     0.     0.]\n",
            " [  539.  4060. 11003. 12214.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1384.  6942. 13092.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3099.  5366.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1966.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5006.  8275. 10903. 11816. 13512. 16138. 17912. 18597. 18631. 18819.]\n",
            " [   88.  4244.  5333. 10653. 13778. 15598. 15511. 16163. 16668.     0.]\n",
            " [ 3398.  9011. 13860. 16172. 18745. 22198. 22876. 23459.     0.     0.]\n",
            " [ 5586. 11612. 15808. 21399. 23454. 26207. 27189.     0.     0.     0.]\n",
            " [ 1089.  9532. 15862. 22179. 25939. 26200.     0.     0.     0.     0.]\n",
            " [ 1510.  6362. 11688. 12966. 15786.     0.     0.     0.     0.     0.]\n",
            " [  537.  4067. 11003. 12218.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1381.  6950. 13088.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3102.  5359.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1966.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4996.  8280. 10912. 11807. 13519. 16143. 17911. 18602. 18631. 18810.]\n",
            " [   87.  4249.  5342. 10653. 13773. 15604. 15510. 16155. 16671.     0.]\n",
            " [ 3394.  9018. 13857. 16177. 18750. 22205. 22877. 23454.     0.     0.]\n",
            " [ 5576. 11621. 15811. 21395. 23449. 26207. 27189.     0.     0.     0.]\n",
            " [ 1086.  9530. 15855. 22169. 25936. 26200.     0.     0.     0.     0.]\n",
            " [ 1516.  6363. 11696. 12972. 15790.     0.     0.     0.     0.     0.]\n",
            " [  537.  4073. 11004. 12224.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1376.  6958. 13090.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3101.  5367.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1962.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5004.  8280. 10913. 11805. 13515. 16137. 17916. 18595. 18629. 18805.]\n",
            " [   96.  4241.  5333. 10647. 13776. 15606. 15517. 16149. 16671.     0.]\n",
            " [ 3384.  9021. 13861. 16174. 18743. 22207. 22877. 23461.     0.     0.]\n",
            " [ 5581. 11619. 15802. 21398. 23456. 26200. 27194.     0.     0.     0.]\n",
            " [ 1087.  9531. 15862. 22169. 25940. 26207.     0.     0.     0.     0.]\n",
            " [ 1520.  6360. 11692. 12975. 15780.     0.     0.     0.     0.     0.]\n",
            " [  536.  4074. 11004. 12217.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1371.  6964. 13099.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3110.  5367.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1968.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4995.  8280. 10912. 11796. 13519. 16133. 17907. 18592. 18633. 18811.]\n",
            " [   88.  4250.  5328. 10646. 13768. 15612. 15525. 16158. 16675.     0.]\n",
            " [ 3389.  9018. 13868. 16175. 18748. 22215. 22881. 23455.     0.     0.]\n",
            " [ 5576. 11611. 15807. 21406. 23456. 26198. 27200.     0.     0.     0.]\n",
            " [ 1079.  9534. 15864. 22171. 25940. 26213.     0.     0.     0.     0.]\n",
            " [ 1525.  6358. 11690. 12979. 15773.     0.     0.     0.     0.     0.]\n",
            " [  528.  4066. 11004. 12209.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1361.  6970. 13107.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3100.  5363.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1965.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5000.  8270. 10914. 11805. 13516. 16126. 17910. 18592. 18625. 18808.]\n",
            " [   87.  4240.  5323. 10641. 13777. 15604. 15529. 16160. 16681.     0.]\n",
            " [ 3382.  9012. 13860. 16178. 18745. 22222. 22880. 23446.     0.     0.]\n",
            " [ 5576. 11604. 15801. 21405. 23447. 26188. 27196.     0.     0.     0.]\n",
            " [ 1072.  9533. 15867. 22175. 25944. 26215.     0.     0.     0.     0.]\n",
            " [ 1522.  6365. 11694. 12969. 15776.     0.     0.     0.     0.     0.]\n",
            " [  522.  4073. 11012. 12217.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1357.  6963. 13113.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3104.  5364.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1958.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 5001.  8274. 10906. 11801. 13511. 16118. 17909. 18600. 18617. 18810.]\n",
            " [   93.  4239.  5326. 10640. 13767. 15599. 15536. 16168. 16679.     0.]\n",
            " [ 3382.  9005. 13864. 16183. 18751. 22225. 22874. 23450.     0.     0.]\n",
            " [ 5573. 11609. 15801. 21406. 23446. 26189. 27189.     0.     0.     0.]\n",
            " [ 1064.  9536. 15857. 22167. 25953. 26207.     0.     0.     0.     0.]\n",
            " [ 1515.  6362. 11689. 12972. 15781.     0.     0.     0.     0.     0.]\n",
            " [  512.  4065. 11007. 12217.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1350.  6957. 13107.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3110.  5363.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1963.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4992.  8283. 10900. 11799. 13509. 16116. 17902. 18597. 18614. 18806.]\n",
            " [   85.  4246.  5322. 10644. 13760. 15589. 15543. 16175. 16673.     0.]\n",
            " [ 3391.  9005. 13866. 16173. 18757. 22222. 22877. 23442.     0.     0.]\n",
            " [ 5565. 11613. 15802. 21396. 23446. 26191. 27184.     0.     0.     0.]\n",
            " [ 1067.  9543. 15865. 22159. 25949. 26212.     0.     0.     0.     0.]\n",
            " [ 1523.  6355. 11697. 12964. 15784.     0.     0.     0.     0.     0.]\n",
            " [  510.  4068. 10997. 12211.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1346.  6949. 13105.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3110.  5367.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1956.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4994.  8287. 10909. 11802. 13512. 16123. 17895. 18601. 18608. 18800.]\n",
            " [   90.  4237.  5316. 10648. 13752. 15588. 15541. 16172. 16675.     0.]\n",
            " [ 3399.  9013. 13869. 16172. 18765. 22226. 22882. 23451.     0.     0.]\n",
            " [ 5564. 11611. 15792. 21400. 23439. 26182. 27185.     0.     0.     0.]\n",
            " [ 1063.  9550. 15867. 22165. 25953. 26219.     0.     0.     0.     0.]\n",
            " [ 1529.  6360. 11698. 12971. 15784.     0.     0.     0.     0.     0.]\n",
            " [  503.  4073. 10997. 12213.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1354.  6941. 13107.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3111.  5371.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1957.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4984.  8295. 10904. 11804. 13503. 16129. 17892. 18598. 18613. 18803.]\n",
            " [   89.  4239.  5312. 10641. 13746. 15580. 15534. 16163. 16684.     0.]\n",
            " [ 3402.  9014. 13863. 16165. 18759. 22223. 22881. 23443.     0.     0.]\n",
            " [ 5556. 11615. 15796. 21390. 23429. 26178. 27175.     0.     0.     0.]\n",
            " [ 1069.  9551. 15876. 22169. 25954. 26209.     0.     0.     0.     0.]\n",
            " [ 1520.  6351. 11691. 12980. 15778.     0.     0.     0.     0.     0.]\n",
            " [  495.  4066. 10989. 12211.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1357.  6949. 13100.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3102.  5372.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1952.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4983.  8299. 10913. 11794. 13498. 16132. 17895. 18592. 18608. 18801.]\n",
            " [   84.  4231.  5315. 10642. 13737. 15585. 15525. 16169. 16680.     0.]\n",
            " [ 3405.  9021. 13860. 16171. 18765. 22219. 22890. 23441.     0.     0.]\n",
            " [ 5546. 11621. 15791. 21394. 23421. 26184. 27172.     0.     0.     0.]\n",
            " [ 1066.  9552. 15881. 22163. 25958. 26216.     0.     0.     0.     0.]\n",
            " [ 1520.  6346. 11689. 12989. 15768.     0.     0.     0.     0.     0.]\n",
            " [  488.  4071. 10981. 12209.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1365.  6946. 13104.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3103.  5362.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1955.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4982.  8297. 10913. 11789. 13492. 16135. 17886. 18584. 18608. 18797.]\n",
            " [   91.  4223.  5311. 10633. 13729. 15584. 15524. 16171. 16672.     0.]\n",
            " [ 3410.  9023. 13869. 16176. 18761. 22226. 22883. 23444.     0.     0.]\n",
            " [ 5541. 11629. 15795. 21388. 23424. 26191. 27181.     0.     0.     0.]\n",
            " [ 1074.  9560. 15883. 22172. 25959. 26214.     0.     0.     0.     0.]\n",
            " [ 1527.  6346. 11685. 12984. 15764.     0.     0.     0.     0.     0.]\n",
            " [  492.  4062. 10981. 12217.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1365.  6943. 13094.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3099.  5354.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1964.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4989.  8296. 10914. 11779. 13498. 16141. 17895. 18578. 18602. 18796.]\n",
            " [   94.  4223.  5311. 10631. 13719. 15577. 15529. 16170. 16666.     0.]\n",
            " [ 3412.  9023. 13864. 16176. 18755. 22228. 22887. 23437.     0.     0.]\n",
            " [ 5538. 11631. 15794. 21395. 23414. 26198. 27174.     0.     0.     0.]\n",
            " [ 1067.  9564. 15873. 22175. 25961. 26209.     0.     0.     0.     0.]\n",
            " [ 1533.  6347. 11681. 12982. 15763.     0.     0.     0.     0.     0.]\n",
            " [  483.  4056. 10973. 12216.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1365.  6946. 13090.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3089.  5361.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1967.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4986.  8288. 10906. 11776. 13498. 16147. 17897. 18572. 18603. 18802.]\n",
            " [   92.  4225.  5308. 10625. 13717. 15569. 15530. 16166. 16656.     0.]\n",
            " [ 3415.  9016. 13854. 16175. 18762. 22226. 22886. 23439.     0.     0.]\n",
            " [ 5547. 11621. 15793. 21404. 23419. 26193. 27180.     0.     0.     0.]\n",
            " [ 1069.  9563. 15882. 22165. 25966. 26212.     0.     0.     0.     0.]\n",
            " [ 1529.  6342. 11675. 12972. 15756.     0.     0.     0.     0.     0.]\n",
            " [  477.  4063. 10982. 12214.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1360.  6946. 13085.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3094.  5358.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1960.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4976.  8296. 10896. 11774. 13504. 16149. 17902. 18566. 18598. 18802.]\n",
            " [   90.  4223.  5300. 10620. 13710. 15574. 15536. 16169. 16655.     0.]\n",
            " [ 3422.  9021. 13851. 16177. 18753. 22231. 22892. 23429.     0.     0.]\n",
            " [ 5550. 11626. 15784. 21401. 23426. 26195. 27182.     0.     0.     0.]\n",
            " [ 1075.  9559. 15884. 22168. 25969. 26219.     0.     0.     0.     0.]\n",
            " [ 1525.  6344. 11683. 12978. 15754.     0.     0.     0.     0.     0.]\n",
            " [  478.  4057. 10991. 12204.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1357.  6938. 13088.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3088.  5360.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1951.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4974.  8302. 10904. 11783. 13495. 16145. 17897. 18560. 18595. 18797.]\n",
            " [   80.  4218.  5304. 10629. 13716. 15580. 15530. 16160. 16655.     0.]\n",
            " [ 3430.  9016. 13858. 16173. 18748. 22231. 22900. 23433.     0.     0.]\n",
            " [ 5540. 11624. 15781. 21409. 23432. 26204. 27186.     0.     0.     0.]\n",
            " [ 1076.  9565. 15886. 22159. 25968. 26212.     0.     0.     0.     0.]\n",
            " [ 1518.  6336. 11689. 12986. 15763.     0.     0.     0.     0.     0.]\n",
            " [  479.  4059. 10990. 12212.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1362.  6938. 13081.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3089.  5362.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1947.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4977.  8300. 10908. 11784. 13500. 16145. 17896. 18563. 18589. 18796.]\n",
            " [   79.  4213.  5299. 10633. 13717. 15582. 15535. 16168. 16656.     0.]\n",
            " [ 3429.  9024. 13867. 16173. 18757. 22226. 22902. 23429.     0.     0.]\n",
            " [ 5533. 11630. 15777. 21405. 23433. 26200. 27182.     0.     0.     0.]\n",
            " [ 1075.  9565. 15889. 22151. 25965. 26204.     0.     0.     0.     0.]\n",
            " [ 1512.  6337. 11681. 12991. 15760.     0.     0.     0.     0.     0.]\n",
            " [  485.  4055. 10987. 12220.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1358.  6947. 13080.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3088.  5359.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1949.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4985.  8301. 10916. 11774. 13504. 16145. 17905. 18559. 18596. 18801.]\n",
            " [   78.  4203.  5294. 10629. 13711. 15583. 15539. 16173. 16650.     0.]\n",
            " [ 3429.  9028. 13864. 16171. 18757. 22227. 22901. 23433.     0.     0.]\n",
            " [ 5527. 11631. 15774. 21402. 23441. 26201. 27186.     0.     0.     0.]\n",
            " [ 1077.  9563. 15888. 22159. 25963. 26200.     0.     0.     0.     0.]\n",
            " [ 1506.  6340. 11677. 12992. 15753.     0.     0.     0.     0.     0.]\n",
            " [  482.  4049. 10979. 12217.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1357.  6938. 13082.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3078.  5361.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1940.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4978.  8306. 10907. 11764. 13494. 16140. 17905. 18551. 18592. 18809.]\n",
            " [   85.  4203.  5288. 10629. 13708. 15579. 15545. 16173. 16654.     0.]\n",
            " [ 3435.  9020. 13856. 16164. 18751. 22230. 22891. 23442.     0.     0.]\n",
            " [ 5519. 11622. 15772. 21409. 23444. 26207. 27181.     0.     0.     0.]\n",
            " [ 1069.  9562. 15886. 22168. 25953. 26203.     0.     0.     0.     0.]\n",
            " [ 1510.  6339. 11678. 12989. 15751.     0.     0.     0.     0.     0.]\n",
            " [  487.  4057. 10987. 12210.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1351.  6939. 13076.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3083.  5351.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1938.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4968.  8308. 10898. 11760. 13500. 16138. 17908. 18558. 18583. 18807.]\n",
            " [   83.  4195.  5285. 10629. 13711. 15586. 15554. 16174. 16661.     0.]\n",
            " [ 3432.  9016. 13862. 16157. 18744. 22238. 22898. 23450.     0.     0.]\n",
            " [ 5522. 11627. 15772. 21403. 23448. 26200. 27182.     0.     0.     0.]\n",
            " [ 1069.  9554. 15886. 22158. 25955. 26196.     0.     0.     0.     0.]\n",
            " [ 1509.  6341. 11671. 12985. 15757.     0.     0.     0.     0.     0.]\n",
            " [  480.  4063. 10987. 12213.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1356.  6939. 13081.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3089.  5359.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1938.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4974.  8313. 10905. 11756. 13494. 16133. 17910. 18564. 18575. 18804.]\n",
            " [   77.  4204.  5289. 10633. 13707. 15593. 15547. 16180. 16652.     0.]\n",
            " [ 3427.  9024. 13866. 16166. 18751. 22245. 22899. 23450.     0.     0.]\n",
            " [ 5526. 11636. 15768. 21412. 23445. 26204. 27184.     0.     0.     0.]\n",
            " [ 1078.  9545. 15880. 22154. 25949. 26192.     0.     0.     0.     0.]\n",
            " [ 1501.  6338. 11662. 12992. 15757.     0.     0.     0.     0.     0.]\n",
            " [  479.  4072. 10983. 12214.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1348.  6944. 13071.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3083.  5354.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1945.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4976.  8322. 10902. 11765. 13487. 16126. 17918. 18572. 18568. 18805.]\n",
            " [   85.  4212.  5292. 10640. 13703. 15583. 15553. 16173. 16642.     0.]\n",
            " [ 3434.  9019. 13872. 16162. 18741. 22242. 22898. 23443.     0.     0.]\n",
            " [ 5520. 11628. 15761. 21415. 23440. 26207. 27192.     0.     0.     0.]\n",
            " [ 1070.  9539. 15881. 22145. 25941. 26189.     0.     0.     0.     0.]\n",
            " [ 1491.  6341. 11654. 12996. 15749.     0.     0.     0.     0.     0.]\n",
            " [  487.  4075. 10979. 12204.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1347.  6943. 13076.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3088.  5344.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1935.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4969.  8316. 10892. 11755. 13488. 16127. 17910. 18580. 18559. 18807.]\n",
            " [   82.  4211.  5301. 10630. 13708. 15587. 15544. 16173. 16633.     0.]\n",
            " [ 3438.  9018. 13873. 16161. 18748. 22243. 22895. 23435.     0.     0.]\n",
            " [ 5520. 11633. 15754. 21422. 23435. 26208. 27195.     0.     0.     0.]\n",
            " [ 1072.  9531. 15875. 22152. 25945. 26193.     0.     0.     0.     0.]\n",
            " [ 1497.  6344. 11657. 12998. 15749.     0.     0.     0.     0.     0.]\n",
            " [  491.  4082. 10971. 12208.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1356.  6934. 13084.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3079.  5338.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1939.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4969.  8319. 10890. 11762. 13478. 16122. 17904. 18579. 18566. 18804.]\n",
            " [   86.  4209.  5292. 10631. 13707. 15582. 15534. 16175. 16626.     0.]\n",
            " [ 3440.  9024. 13871. 16156. 18752. 22247. 22898. 23444.     0.     0.]\n",
            " [ 5510. 11638. 15747. 21427. 23440. 26213. 27199.     0.     0.     0.]\n",
            " [ 1065.  9522. 15865. 22159. 25947. 26197.     0.     0.     0.     0.]\n",
            " [ 1502.  6349. 11660. 12995. 15741.     0.     0.     0.     0.     0.]\n",
            " [  495.  4089. 10975. 12214.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1364.  6939. 13081.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3073.  5339.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1948.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4972.  8325. 10896. 11766. 13480. 16131. 17901. 18576. 18568. 18799.]\n",
            " [   76.  4205.  5283. 10634. 13711. 15585. 15524. 16171. 16634.     0.]\n",
            " [ 3449.  9032. 13871. 16161. 18755. 22242. 22893. 23436.     0.     0.]\n",
            " [ 5504. 11644. 15752. 21420. 23446. 26209. 27189.     0.     0.     0.]\n",
            " [ 1071.  9524. 15865. 22153. 25947. 26188.     0.     0.     0.     0.]\n",
            " [ 1499.  6354. 11653. 13003. 15743.     0.     0.     0.     0.     0.]\n",
            " [  504.  4081. 10984. 12211.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1362.  6931. 13081.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3065.  5340.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1948.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4972.  8333. 10895. 11765. 13476. 16127. 17891. 18583. 18576. 18801.]\n",
            " [   79.  4209.  5280. 10642. 13719. 15592. 15518. 16172. 16632.     0.]\n",
            " [ 3446.  9025. 13873. 16169. 18745. 22241. 22883. 23442.     0.     0.]\n",
            " [ 5498. 11645. 15753. 21414. 23445. 26205. 27189.     0.     0.     0.]\n",
            " [ 1071.  9528. 15867. 22143. 25948. 26183.     0.     0.     0.     0.]\n",
            " [ 1495.  6356. 11661. 13012. 15743.     0.     0.     0.     0.     0.]\n",
            " [  500.  4074. 10992. 12216.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1360.  6929. 13078.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3071.  5339.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1944.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4971.  8323. 10895. 11758. 13469. 16123. 17883. 18580. 18573. 18807.]\n",
            " [   81.  4215.  5273. 10641. 13718. 15585. 15526. 16180. 16622.     0.]\n",
            " [ 3444.  9027. 13868. 16166. 18736. 22238. 22881. 23432.     0.     0.]\n",
            " [ 5490. 11652. 15751. 21420. 23441. 26196. 27190.     0.     0.     0.]\n",
            " [ 1076.  9524. 15871. 22138. 25939. 26185.     0.     0.     0.     0.]\n",
            " [ 1504.  6356. 11668. 13002. 15745.     0.     0.     0.     0.     0.]\n",
            " [  503.  4079. 10992. 12223.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1365.  6928. 13073.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3070.  5335.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1940.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4967.  8329. 10896. 11750. 13473. 16124. 17891. 18578. 18564. 18816.]\n",
            " [   83.  4223.  5264. 10644. 13722. 15583. 15523. 16189. 16630.     0.]\n",
            " [ 3453.  9025. 13874. 16167. 18739. 22241. 22880. 23441.     0.     0.]\n",
            " [ 5487. 11646. 15747. 21424. 23449. 26196. 27185.     0.     0.     0.]\n",
            " [ 1078.  9527. 15861. 22142. 25942. 26191.     0.     0.     0.     0.]\n",
            " [ 1507.  6357. 11664. 13000. 15737.     0.     0.     0.     0.     0.]\n",
            " [  501.  4087. 10983. 12217.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1357.  6935. 13076.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3062.  5341.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1943.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4973.  8327. 10895. 11753. 13473. 16122. 17900. 18572. 18566. 18819.]\n",
            " [   82.  4218.  5263. 10635. 13714. 15582. 15524. 16183. 16630.     0.]\n",
            " [ 3453.  9032. 13883. 16165. 18730. 22236. 22874. 23432.     0.     0.]\n",
            " [ 5483. 11646. 15747. 21431. 23456. 26200. 27187.     0.     0.     0.]\n",
            " [ 1081.  9518. 15858. 22133. 25940. 26195.     0.     0.     0.     0.]\n",
            " [ 1505.  6353. 11655. 13007. 15733.     0.     0.     0.     0.     0.]\n",
            " [  492.  4086. 10981. 12221.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1357.  6940. 13077.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3063.  5334.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1939.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4980.  8325. 10900. 11759. 13471. 16116. 17897. 18573. 18560. 18810.]\n",
            " [   78.  4224.  5261. 10637. 13720. 15591. 15531. 16182. 16625.     0.]\n",
            " [ 3459.  9026. 13891. 16157. 18739. 22239. 22876. 23437.     0.     0.]\n",
            " [ 5492. 11644. 15739. 21437. 23457. 26207. 27185.     0.     0.     0.]\n",
            " [ 1088.  9522. 15852. 22137. 25935. 26187.     0.     0.     0.     0.]\n",
            " [ 1511.  6343. 11661. 12997. 15735.     0.     0.     0.     0.     0.]\n",
            " [  484.  4083. 10973. 12213.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1366.  6932. 13078.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3070.  5333.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1938.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4988.  8328. 10906. 11754. 13464. 16106. 17887. 18567. 18550. 18808.]\n",
            " [   84.  4221.  5267. 10643. 13711. 15597. 15538. 16183. 16624.     0.]\n",
            " [ 3450.  9018. 13884. 16161. 18739. 22246. 22875. 23433.     0.     0.]\n",
            " [ 5499. 11651. 15735. 21439. 23460. 26203. 27187.     0.     0.     0.]\n",
            " [ 1090.  9526. 15845. 22141. 25944. 26196.     0.     0.     0.     0.]\n",
            " [ 1509.  6352. 11653. 12999. 15738.     0.     0.     0.     0.     0.]\n",
            " [  474.  4087. 10973. 12220.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1375.  6935. 13076.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3061.  5334.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1939.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4994.  8332. 10900. 11748. 13466. 16102. 17885. 18557. 18547. 18815.]\n",
            " [   82.  4223.  5270. 10651. 13711. 15595. 15540. 16181. 16614.     0.]\n",
            " [ 3440.  9017. 13893. 16170. 18743. 22248. 22869. 23431.     0.     0.]\n",
            " [ 5497. 11643. 15730. 21429. 23468. 26195. 27183.     0.     0.     0.]\n",
            " [ 1089.  9524. 15842. 22150. 25952. 26189.     0.     0.     0.     0.]\n",
            " [ 1518.  6345. 11645. 12991. 15734.     0.     0.     0.     0.     0.]\n",
            " [  469.  4084. 10972. 12213.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1381.  6933. 13081.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3066.  5328.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1941.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4990.  8325. 10895. 11749. 13467. 16092. 17878. 18557. 18548. 18811.]\n",
            " [   87.  4227.  5271. 10653. 13706. 15600. 15537. 16188. 16621.     0.]\n",
            " [ 3442.  9024. 13888. 16163. 18751. 22256. 22863. 23421.     0.     0.]\n",
            " [ 5505. 11634. 15733. 21419. 23477. 26197. 27192.     0.     0.     0.]\n",
            " [ 1094.  9530. 15833. 22155. 25956. 26197.     0.     0.     0.     0.]\n",
            " [ 1508.  6348. 11648. 12987. 15738.     0.     0.     0.     0.     0.]\n",
            " [  477.  4083. 10970. 12211.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1385.  6941. 13072.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3067.  5325.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1937.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4987.  8326. 10897. 11754. 13474. 16089. 17882. 18548. 18538. 18807.]\n",
            " [   89.  4235.  5261. 10661. 13703. 15594. 15546. 16194. 16630.     0.]\n",
            " [ 3441.  9026. 13895. 16169. 18745. 22263. 22867. 23412.     0.     0.]\n",
            " [ 5507. 11638. 15731. 21410. 23475. 26195. 27182.     0.     0.     0.]\n",
            " [ 1096.  9532. 15828. 22164. 25962. 26206.     0.     0.     0.     0.]\n",
            " [ 1512.  6347. 11640. 12994. 15746.     0.     0.     0.     0.     0.]\n",
            " [  478.  4073. 10970. 12218.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1385.  6939. 13080.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3074.  5328.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1944.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4987.  8320. 10904. 11756. 13470. 16079. 17886. 18553. 18536. 18806.]\n",
            " [   93.  4231.  5269. 10669. 13697. 15602. 15542. 16191. 16639.     0.]\n",
            " [ 3444.  9017. 13896. 16175. 18745. 22267. 22867. 23412.     0.     0.]\n",
            " [ 5515. 11639. 15725. 21411. 23467. 26186. 27182.     0.     0.     0.]\n",
            " [ 1100.  9541. 15826. 22155. 25959. 26202.     0.     0.     0.     0.]\n",
            " [ 1518.  6344. 11644. 12999. 15743.     0.     0.     0.     0.     0.]\n",
            " [  477.  4082. 10961. 12208.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1379.  6941. 13077.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3064.  5332.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1947.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4984.  8310. 10910. 11761. 13466. 16086. 17879. 18552. 18542. 18814.]\n",
            " [   90.  4226.  5276. 10659. 13694. 15611. 15549. 16200. 16643.     0.]\n",
            " [ 3449.  9025. 13902. 16169. 18744. 22276. 22872. 23418.     0.     0.]\n",
            " [ 5507. 11640. 15718. 21406. 23472. 26180. 27180.     0.     0.     0.]\n",
            " [ 1095.  9537. 15832. 22156. 25965. 26205.     0.     0.     0.     0.]\n",
            " [ 1523.  6342. 11649. 13004. 15741.     0.     0.     0.     0.     0.]\n",
            " [  482.  4088. 10969. 12215.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1377.  6932. 13072.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3063.  5332.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1950.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4990.  8301. 10904. 11754. 13473. 16089. 17873. 18543. 18540. 18815.]\n",
            " [   92.  4216.  5272. 10662. 13686. 15616. 15554. 16204. 16640.     0.]\n",
            " [ 3452.  9023. 13905. 16178. 18744. 22283. 22876. 23427.     0.     0.]\n",
            " [ 5513. 11636. 15708. 21408. 23477. 26180. 27186.     0.     0.     0.]\n",
            " [ 1088.  9532. 15830. 22152. 25974. 26200.     0.     0.     0.     0.]\n",
            " [ 1532.  6337. 11653. 12999. 15732.     0.     0.     0.     0.     0.]\n",
            " [  490.  4093. 10973. 12213.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1367.  6937. 13072.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3064.  5324.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1946.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4985.  8304. 10894. 11762. 13476. 16081. 17873. 18551. 18548. 18810.]\n",
            " [   94.  4219.  5266. 10654. 13687. 15610. 15544. 16205. 16647.     0.]\n",
            " [ 3449.  9013. 13905. 16173. 18736. 22290. 22874. 23426.     0.     0.]\n",
            " [ 5503. 11638. 15707. 21414. 23481. 26173. 27178.     0.     0.     0.]\n",
            " [ 1091.  9538. 15839. 22143. 25973. 26193.     0.     0.     0.     0.]\n",
            " [ 1525.  6335. 11658. 13002. 15733.     0.     0.     0.     0.     0.]\n",
            " [  498.  4101. 10965. 12222.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1369.  6940. 13076.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3060.  5320.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1955.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4991.  8300. 10884. 11757. 13468. 16073. 17867. 18541. 18539. 18808.]\n",
            " [   97.  4221.  5275. 10655. 13686. 15618. 15547. 16199. 16653.     0.]\n",
            " [ 3456.  9015. 13913. 16169. 18732. 22286. 22864. 23431.     0.     0.]\n",
            " [ 5496. 11631. 15698. 21420. 23484. 26168. 27186.     0.     0.     0.]\n",
            " [ 1093.  9528. 15829. 22150. 25964. 26195.     0.     0.     0.     0.]\n",
            " [ 1534.  6337. 11664. 12994. 15740.     0.     0.     0.     0.     0.]\n",
            " [  501.  4110. 10972. 12231.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1359.  6931. 13068.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3061.  5310.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1945.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4997.  8309. 10892. 11761. 13464. 16082. 17859. 18539. 18547. 18809.]\n",
            " [   89.  4216.  5276. 10662. 13690. 15620. 15551. 16191. 16658.     0.]\n",
            " [ 3451.  9014. 13907. 16176. 18725. 22288. 22854. 23429.     0.     0.]\n",
            " [ 5489. 11634. 15697. 21424. 23483. 26158. 27182.     0.     0.     0.]\n",
            " [ 1100.  9533. 15835. 22153. 25957. 26185.     0.     0.     0.     0.]\n",
            " [ 1527.  6330. 11658. 12991. 15732.     0.     0.     0.     0.     0.]\n",
            " [  509.  4106. 10964. 12238.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1367.  6933. 13072.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3056.  5309.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1944.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4989.  8310. 10889. 11758. 13455. 16088. 17856. 18546. 18553. 18808.]\n",
            " [   81.  4209.  5285. 10670. 13694. 15613. 15541. 16182. 16661.     0.]\n",
            " [ 3451.  9022. 13906. 16171. 18721. 22279. 22863. 23428.     0.     0.]\n",
            " [ 5498. 11630. 15696. 21424. 23492. 26163. 27183.     0.     0.     0.]\n",
            " [ 1091.  9529. 15843. 22144. 25958. 26186.     0.     0.     0.     0.]\n",
            " [ 1526.  6339. 11665. 12990. 15736.     0.     0.     0.     0.     0.]\n",
            " [  517.  4112. 10968. 12237.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1369.  6942. 13069.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3059.  5300.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1934.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4981.  8318. 10883. 11759. 13460. 16092. 17848. 18545. 18552. 18812.]\n",
            " [   75.  4204.  5280. 10662. 13688. 15610. 15549. 16189. 16662.     0.]\n",
            " [ 3446.  9022. 13901. 16171. 18721. 22277. 22861. 23420.     0.     0.]\n",
            " [ 5506. 11623. 15686. 21417. 23492. 26169. 27177.     0.     0.     0.]\n",
            " [ 1087.  9526. 15852. 22145. 25953. 26183.     0.     0.     0.     0.]\n",
            " [ 1530.  6329. 11661. 12998. 15729.     0.     0.     0.     0.     0.]\n",
            " [  517.  4106. 10976. 12229.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1366.  6945. 13073.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3057.  5298.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1925.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4974.  8320. 10873. 11764. 13467. 16083. 17845. 18540. 18553. 18818.]\n",
            " [   68.  4207.  5284. 10657. 13693. 15611. 15543. 16187. 16669.     0.]\n",
            " [ 3449.  9024. 13905. 16169. 18727. 22269. 22853. 23417.     0.     0.]\n",
            " [ 5501. 11616. 15689. 21420. 23501. 26178. 27175.     0.     0.     0.]\n",
            " [ 1081.  9523. 15843. 22153. 25950. 26188.     0.     0.     0.     0.]\n",
            " [ 1528.  6337. 11652. 12996. 15727.     0.     0.     0.     0.     0.]\n",
            " [  526.  4102. 10966. 12231.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1358.  6949. 13065.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3055.  5292.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1916.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4980.  8311. 10875. 11761. 13473. 16089. 17843. 18534. 18560. 18823.]\n",
            " [   63.  4197.  5291. 10657. 13685. 15615. 15552. 16189. 16668.     0.]\n",
            " [ 3439.  9016. 13897. 16170. 18723. 22266. 22858. 23420.     0.     0.]\n",
            " [ 5495. 11616. 15689. 21424. 23503. 26168. 27182.     0.     0.     0.]\n",
            " [ 1088.  9514. 15836. 22151. 25953. 26196.     0.     0.     0.     0.]\n",
            " [ 1530.  6339. 11647. 12994. 15730.     0.     0.     0.     0.     0.]\n",
            " [  535.  4106. 10962. 12234.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1365.  6941. 13059.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3056.  5296.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1921.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4979.  8316. 10872. 11763. 13469. 16089. 17851. 18533. 18566. 18822.]\n",
            " [   54.  4191.  5298. 10654. 13693. 15606. 15550. 16183. 16675.     0.]\n",
            " [ 3444.  9024. 13904. 16174. 18723. 22257. 22851. 23413.     0.     0.]\n",
            " [ 5493. 11610. 15682. 21432. 23502. 26175. 27179.     0.     0.     0.]\n",
            " [ 1081.  9523. 15844. 22152. 25961. 26192.     0.     0.     0.     0.]\n",
            " [ 1537.  6333. 11638. 12985. 15724.     0.     0.     0.     0.     0.]\n",
            " [  544.  4115. 10958. 12225.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1372.  6941. 13060.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3063.  5291.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1911.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4988.  8322. 10871. 11761. 13470. 16084. 17854. 18540. 18558. 18822.]\n",
            " [   58.  4186.  5290. 10655. 13689. 15603. 15541. 16179. 16672.     0.]\n",
            " [ 3441.  9028. 13908. 16171. 18727. 22257. 22843. 23412.     0.     0.]\n",
            " [ 5492. 11619. 15689. 21440. 23511. 26181. 27180.     0.     0.     0.]\n",
            " [ 1077.  9524. 15853. 22145. 25955. 26189.     0.     0.     0.     0.]\n",
            " [ 1531.  6324. 11646. 12989. 15723.     0.     0.     0.     0.     0.]\n",
            " [  550.  4115. 10959. 12223.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1378.  6949. 13064.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3062.  5283.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1913.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4996.  8324. 10873. 11753. 13469. 16084. 17863. 18540. 18565. 18816.]\n",
            " [   67.  4189.  5293. 10653. 13690. 15600. 15543. 16182. 16662.     0.]\n",
            " [ 3447.  9034. 13913. 16175. 18734. 22261. 22836. 23413.     0.     0.]\n",
            " [ 5493. 11616. 15688. 21434. 23511. 26174. 27182.     0.     0.     0.]\n",
            " [ 1072.  9520. 15856. 22136. 25953. 26190.     0.     0.     0.     0.]\n",
            " [ 1535.  6333. 11636. 12995. 15717.     0.     0.     0.     0.     0.]\n",
            " [  553.  4106. 10956. 12227.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1385.  6942. 13061.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3070.  5286.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1905.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4989.  8324. 10872. 11751. 13464. 16079. 17861. 18540. 18557. 18814.]\n",
            " [   70.  4196.  5295. 10648. 13692. 15592. 15546. 16191. 16660.     0.]\n",
            " [ 3450.  9040. 13916. 16165. 18726. 22257. 22827. 23417.     0.     0.]\n",
            " [ 5495. 11622. 15682. 21429. 23510. 26174. 27175.     0.     0.     0.]\n",
            " [ 1067.  9516. 15860. 22134. 25958. 26194.     0.     0.     0.     0.]\n",
            " [ 1543.  6334. 11640. 12987. 15726.     0.     0.     0.     0.     0.]\n",
            " [  551.  4113. 10961. 12234.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1389.  6951. 13070.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3071.  5294.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1898.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4987.  8327. 10864. 11743. 13458. 16077. 17870. 18541. 18549. 18810.]\n",
            " [   75.  4186.  5287. 10644. 13693. 15590. 15555. 16199. 16653.     0.]\n",
            " [ 3457.  9043. 13915. 16161. 18724. 22254. 22821. 23410.     0.     0.]\n",
            " [ 5491. 11613. 15682. 21435. 23511. 26178. 27169.     0.     0.     0.]\n",
            " [ 1073.  9515. 15853. 22128. 25949. 26195.     0.     0.     0.     0.]\n",
            " [ 1538.  6329. 11635. 12981. 15735.     0.     0.     0.     0.     0.]\n",
            " [  556.  4104. 10959. 12232.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1384.  6959. 13077.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3073.  5292.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1900.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4990.  8334. 10865. 11736. 13452. 16073. 17877. 18543. 18551. 18812.]\n",
            " [   69.  4186.  5294. 10642. 13696. 15584. 15546. 16192. 16657.     0.]\n",
            " [ 3461.  9035. 13906. 16155. 18732. 22261. 22824. 23418.     0.     0.]\n",
            " [ 5488. 11622. 15678. 21437. 23510. 26168. 27165.     0.     0.     0.]\n",
            " [ 1069.  9516. 15856. 22124. 25946. 26192.     0.     0.     0.     0.]\n",
            " [ 1542.  6321. 11625. 12972. 15733.     0.     0.     0.     0.     0.]\n",
            " [  564.  4098. 10963. 12233.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1379.  6966. 13070.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3082.  5283.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1892.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4980.  8330. 10866. 11737. 13454. 16073. 17869. 18534. 18547. 18802.]\n",
            " [   61.  4182.  5284. 10643. 13703. 15578. 15555. 16201. 16665.     0.]\n",
            " [ 3464.  9043. 13913. 16164. 18725. 22255. 22827. 23421.     0.     0.]\n",
            " [ 5487. 11615. 15671. 21432. 23512. 26158. 27168.     0.     0.     0.]\n",
            " [ 1071.  9515. 15850. 22120. 25953. 26188.     0.     0.     0.     0.]\n",
            " [ 1550.  6325. 11627. 12978. 15725.     0.     0.     0.     0.     0.]\n",
            " [  556.  4102. 10954. 12229.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1378.  6966. 13069.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3076.  5283.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1890.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4971.  8328. 10857. 11727. 13445. 16079. 17878. 18541. 18544. 18801.]\n",
            " [   54.  4178.  5291. 10633. 13699. 15583. 15558. 16203. 16668.     0.]\n",
            " [ 3465.  9036. 13911. 16168. 18715. 22259. 22824. 23414.     0.     0.]\n",
            " [ 5481. 11614. 15664. 21434. 23503. 26154. 27176.     0.     0.     0.]\n",
            " [ 1080.  9509. 15844. 22121. 25949. 26197.     0.     0.     0.     0.]\n",
            " [ 1542.  6329. 11627. 12984. 15717.     0.     0.     0.     0.     0.]\n",
            " [  553.  4103. 10946. 12223.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1382.  6964. 13078.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3072.  5285.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1897.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4979.  8325. 10847. 11728. 13437. 16087. 17871. 18544. 18536. 18801.]\n",
            " [   57.  4179.  5288. 10623. 13702. 15581. 15549. 16209. 16664.     0.]\n",
            " [ 3457.  9044. 13904. 16166. 18712. 22250. 22821. 23411.     0.     0.]\n",
            " [ 5488. 11611. 15659. 21436. 23501. 26152. 27180.     0.     0.     0.]\n",
            " [ 1075.  9506. 15852. 22117. 25942. 26189.     0.     0.     0.     0.]\n",
            " [ 1537.  6330. 11628. 12979. 15726.     0.     0.     0.     0.     0.]\n",
            " [  554.  4094. 10936. 12223.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1391.  6969. 13082.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3062.  5278.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1894.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4981.  8328. 10855. 11721. 13443. 16077. 17878. 18544. 18529. 18800.]\n",
            " [   60.  4183.  5295. 10620. 13699. 15587. 15549. 16208. 16668.     0.]\n",
            " [ 3454.  9035. 13905. 16166. 18714. 22244. 22819. 23405.     0.     0.]\n",
            " [ 5481. 11609. 15661. 21433. 23507. 26156. 27189.     0.     0.     0.]\n",
            " [ 1068.  9512. 15847. 22111. 25944. 26191.     0.     0.     0.     0.]\n",
            " [ 1542.  6333. 11621. 12981. 15719.     0.     0.     0.     0.     0.]\n",
            " [  547.  4096. 10933. 12230.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1382.  6963. 13088.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3070.  5282.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1902.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4981.  8324. 10845. 11715. 13440. 16068. 17877. 18541. 18525. 18796.]\n",
            " [   56.  4184.  5300. 10614. 13691. 15584. 15553. 16201. 16671.     0.]\n",
            " [ 3462.  9036. 13907. 16158. 18721. 22251. 22825. 23414.     0.     0.]\n",
            " [ 5476. 11600. 15658. 21424. 23514. 26147. 27180.     0.     0.     0.]\n",
            " [ 1076.  9508. 15856. 22103. 25949. 26192.     0.     0.     0.     0.]\n",
            " [ 1543.  6330. 11622. 12988. 15715.     0.     0.     0.     0.     0.]\n",
            " [  549.  4088. 10941. 12226.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1386.  6957. 13086.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3071.  5282.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1902.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4981.  8333. 10853. 11715. 13448. 16065. 17874. 18539. 18528. 18801.]\n",
            " [   56.  4191.  5304. 10621. 13690. 15581. 15545. 16203. 16676.     0.]\n",
            " [ 3461.  9038. 13908. 16157. 18721. 22260. 22827. 23414.     0.     0.]\n",
            " [ 5472. 11591. 15649. 21427. 23515. 26147. 27173.     0.     0.     0.]\n",
            " [ 1067.  9515. 15849. 22105. 25942. 26188.     0.     0.     0.     0.]\n",
            " [ 1534.  6339. 11624. 12988. 15711.     0.     0.     0.     0.     0.]\n",
            " [  550.  4090. 10941. 12218.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1377.  6949. 13080.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3071.  5284.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1898.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4978.  8341. 10860. 11724. 13443. 16072. 17879. 18546. 18529. 18804.]\n",
            " [   65.  4187.  5305. 10630. 13683. 15582. 15538. 16201. 16677.     0.]\n",
            " [ 3460.  9030. 13916. 16158. 18711. 22262. 22822. 23414.     0.     0.]\n",
            " [ 5476. 11600. 15647. 21419. 23518. 26138. 27172.     0.     0.     0.]\n",
            " [ 1067.  9508. 15846. 22109. 25938. 26195.     0.     0.     0.     0.]\n",
            " [ 1532.  6338. 11615. 12978. 15719.     0.     0.     0.     0.     0.]\n",
            " [  551.  4082. 10944. 12211.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1382.  6956. 13080.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3071.  5290.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1889.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4973.  8335. 10855. 11723. 13442. 16065. 17878. 18549. 18531. 18806.]\n",
            " [   60.  4195.  5298. 10629. 13680. 15590. 15531. 16202. 16684.     0.]\n",
            " [ 3460.  9028. 13907. 16161. 18703. 22260. 22815. 23416.     0.     0.]\n",
            " [ 5476. 11600. 15643. 21419. 23514. 26140. 27162.     0.     0.     0.]\n",
            " [ 1070.  9503. 15843. 22112. 25942. 26204.     0.     0.     0.     0.]\n",
            " [ 1534.  6330. 11611. 12979. 15727.     0.     0.     0.     0.     0.]\n",
            " [  545.  4091. 10937. 12220.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1378.  6955. 13078.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3078.  5299.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1893.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4970.  8338. 10847. 11721. 13435. 16061. 17871. 18547. 18524. 18797.]\n",
            " [   56.  4194.  5298. 10638. 13674. 15590. 15534. 16200. 16676.     0.]\n",
            " [ 3468.  9027. 13897. 16167. 18697. 22259. 22824. 23407.     0.     0.]\n",
            " [ 5471. 11607. 15637. 21425. 23508. 26141. 27169.     0.     0.     0.]\n",
            " [ 1066.  9507. 15843. 22118. 25932. 26199.     0.     0.     0.     0.]\n",
            " [ 1533.  6334. 11620. 12981. 15718.     0.     0.     0.     0.     0.]\n",
            " [  546.  4094. 10934. 12210.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1386.  6948. 13075.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3075.  5299.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1897.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4976.  8339. 10851. 11716. 13437. 16056. 17863. 18547. 18521. 18798.]\n",
            " [   55.  4191.  5298. 10647. 13671. 15580. 15530. 16209. 16670.     0.]\n",
            " [ 3472.  9027. 13887. 16162. 18691. 22257. 22819. 23407.     0.     0.]\n",
            " [ 5479. 11615. 15641. 21428. 23516. 26149. 27170.     0.     0.     0.]\n",
            " [ 1065.  9498. 15852. 22127. 25929. 26197.     0.     0.     0.     0.]\n",
            " [ 1538.  6336. 11618. 12971. 15725.     0.     0.     0.     0.     0.]\n",
            " [  540.  4099. 10932. 12201.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1391.  6938. 13081.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3078.  5293.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1890.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4969.  8333. 10856. 11725. 13432. 16050. 17861. 18552. 18529. 18799.]\n",
            " [   54.  4183.  5307. 10643. 13674. 15574. 15537. 16209. 16679.     0.]\n",
            " [ 3465.  9030. 13882. 16169. 18691. 22248. 22817. 23406.     0.     0.]\n",
            " [ 5471. 11623. 15631. 21425. 23507. 26150. 27162.     0.     0.     0.]\n",
            " [ 1070.  9505. 15847. 22120. 25929. 26196.     0.     0.     0.     0.]\n",
            " [ 1530.  6340. 11608. 12976. 15726.     0.     0.     0.     0.     0.]\n",
            " [  540.  4104. 10928. 12199.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1393.  6936. 13071.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3070.  5299.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1882.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4966.  8331. 10847. 11725. 13422. 16041. 17852. 18546. 18538. 18798.]\n",
            " [   58.  4177.  5297. 10643. 13683. 15582. 15528. 16213. 16676.     0.]\n",
            " [ 3463.  9037. 13889. 16170. 18691. 22240. 22819. 23410.     0.     0.]\n",
            " [ 5476. 11619. 15632. 21424. 23514. 26148. 27168.     0.     0.     0.]\n",
            " [ 1066.  9504. 15845. 22121. 25919. 26189.     0.     0.     0.     0.]\n",
            " [ 1539.  6346. 11607. 12978. 15728.     0.     0.     0.     0.     0.]\n",
            " [  541.  4095. 10937. 12193.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1396.  6937. 13078.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3077.  5300.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1878.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4964.  8336. 10856. 11732. 13421. 16038. 17848. 18549. 18530. 18789.]\n",
            " [   54.  4174.  5301. 10644. 13690. 15584. 15533. 16207. 16666.     0.]\n",
            " [ 3469.  9035. 13895. 16162. 18689. 22241. 22824. 23402.     0.     0.]\n",
            " [ 5484. 11619. 15624. 21433. 23513. 26151. 27170.     0.     0.     0.]\n",
            " [ 1071.  9496. 15850. 22118. 25922. 26197.     0.     0.     0.     0.]\n",
            " [ 1540.  6346. 11612. 12978. 15721.     0.     0.     0.     0.     0.]\n",
            " [  550.  4099. 10936. 12193.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1401.  6932. 13079.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3074.  5295.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1871.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4970.  8340. 10863. 11729. 13414. 16030. 17857. 18558. 18535. 18788.]\n",
            " [   51.  4179.  5291. 10641. 13684. 15576. 15538. 16208. 16670.     0.]\n",
            " [ 3462.  9032. 13897. 16153. 18693. 22250. 22830. 23400.     0.     0.]\n",
            " [ 5481. 11623. 15629. 21436. 23522. 26146. 27169.     0.     0.     0.]\n",
            " [ 1075.  9500. 15855. 22117. 25930. 26206.     0.     0.     0.     0.]\n",
            " [ 1540.  6355. 11607. 12981. 15713.     0.     0.     0.     0.     0.]\n",
            " [  542.  4090. 10939. 12198.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1409.  6926. 13080.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3065.  5300.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1867.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4969.  8332. 10853. 11733. 13405. 16032. 17860. 18556. 18527. 18791.]\n",
            " [   48.  4176.  5294. 10635. 13681. 15581. 15533. 16211. 16664.     0.]\n",
            " [ 3454.  9023. 13904. 16155. 18687. 22240. 22825. 23409.     0.     0.]\n",
            " [ 5489. 11631. 15629. 21430. 23530. 26152. 27176.     0.     0.     0.]\n",
            " [ 1068.  9504. 15854. 22122. 25931. 26214.     0.     0.     0.     0.]\n",
            " [ 1541.  6353. 11609. 12974. 15710.     0.     0.     0.     0.     0.]\n",
            " [  538.  4096. 10929. 12195.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1399.  6931. 13080.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3060.  5290.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1871.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4959.  8324. 10857. 11739. 13410. 16037. 17861. 18548. 18519. 18785.]\n",
            " [   49.  4170.  5300. 10632. 13680. 15580. 15532. 16215. 16664.     0.]\n",
            " [ 3452.  9017. 13906. 16150. 18683. 22247. 22834. 23406.     0.     0.]\n",
            " [ 5484. 11632. 15621. 21429. 23523. 26151. 27171.     0.     0.     0.]\n",
            " [ 1069.  9507. 15858. 22113. 25938. 26209.     0.     0.     0.     0.]\n",
            " [ 1536.  6359. 11615. 12969. 15711.     0.     0.     0.     0.     0.]\n",
            " [  542.  4094. 10930. 12193.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1392.  6931. 13073.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3063.  5285.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1878.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4962.  8326. 10853. 11744. 13405. 16041. 17862. 18548. 18511. 18788.]\n",
            " [   58.  4179.  5305. 10629. 13674. 15570. 15530. 16224. 16664.     0.]\n",
            " [ 3461.  9012. 13896. 16149. 18682. 22252. 22841. 23402.     0.     0.]\n",
            " [ 5488. 11630. 15611. 21437. 23531. 26150. 27173.     0.     0.     0.]\n",
            " [ 1071.  9513. 15862. 22119. 25937. 26201.     0.     0.     0.     0.]\n",
            " [ 1544.  6350. 11623. 12974. 15716.     0.     0.     0.     0.     0.]\n",
            " [  536.  4099. 10938. 12200.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1387.  6930. 13081.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3054.  5278.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1874.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4966.  8320. 10859. 11750. 13407. 16045. 17856. 18538. 18516. 18780.]\n",
            " [   67.  4173.  5304. 10627. 13673. 15573. 15536. 16229. 16654.     0.]\n",
            " [ 3455.  9014. 13895. 16150. 18682. 22260. 22837. 23407.     0.     0.]\n",
            " [ 5496. 11637. 15617. 21442. 23525. 26149. 27163.     0.     0.     0.]\n",
            " [ 1073.  9516. 15852. 22121. 25933. 26199.     0.     0.     0.     0.]\n",
            " [ 1543.  6359. 11632. 12975. 15715.     0.     0.     0.     0.     0.]\n",
            " [  531.  4090. 10936. 12197.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1384.  6927. 13090.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3063.  5269.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1879.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4963.  8320. 10867. 11748. 13413. 16040. 17857. 18530. 18514. 18770.]\n",
            " [   73.  4178.  5297. 10620. 13671. 15569. 15535. 16227. 16656.     0.]\n",
            " [ 3463.  9017. 13893. 16155. 18677. 22253. 22842. 23398.     0.     0.]\n",
            " [ 5495. 11643. 15617. 21447. 23534. 26156. 27170.     0.     0.     0.]\n",
            " [ 1063.  9517. 15844. 22118. 25937. 26190.     0.     0.     0.     0.]\n",
            " [ 1547.  6361. 11624. 12968. 15720.     0.     0.     0.     0.     0.]\n",
            " [  523.  4082. 10936. 12205.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1389.  6927. 13094.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3066.  5272.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1876.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n",
            "[[ 4968.  8314. 10864. 11748. 13408. 16043. 17848. 18520. 18519. 18765.]\n",
            " [   73.  4179.  5292. 10612. 13665. 15559. 15543. 16234. 16660.     0.]\n",
            " [ 3458.  9007. 13899. 16163. 18671. 22262. 22839. 23389.     0.     0.]\n",
            " [ 5504. 11635. 15626. 21437. 23527. 26163. 27174.     0.     0.     0.]\n",
            " [ 1070.  9516. 15843. 22123. 25944. 26189.     0.     0.     0.     0.]\n",
            " [ 1540.  6358. 11623. 12973. 15722.     0.     0.     0.     0.     0.]\n",
            " [  525.  4072. 10933. 12197.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1381.  6925. 13092.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 3068.  5263.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
            " [ 1869.     0.     0.     0.     0.     0.     0.     0.     0.     0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8GNst3Lzl_C",
        "colab_type": "code",
        "outputId": "84006a62-50ad-475b-82da-b9a99f55275b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data= np.array(train_data)\n",
        "train_data.shape\n",
        "#train_data[71,:]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5555, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaQ869k1vjuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############\n",
        "# define the loss function\n",
        "################\n",
        "\n",
        "def poisson_dev(y_true, y_pred):\n",
        "    return 2*K.mean(y_pred - y_true -y_true*(K.log(K.clip(y_pred,K.epsilon(),None)) -K.log(K.clip(y_true,K.epsilon(),None))),axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-E5C-Oek8sW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################\n",
        "# Step 3 : Build Model using Neural Networks\n",
        "# Note : MSE loss used here\n",
        "###############################\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras import regularizers\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import RemoteMonitor\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBoVSoy4m0pk",
        "colab_type": "code",
        "outputId": "1880bd60-76d3-4da4-bddf-eae2663cbaa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10234
        }
      },
      "source": [
        "###################\n",
        "# Auto Encoder Network Architecture #\n",
        "######################\n",
        "\n",
        "#encoder part\n",
        "inputData= Input(shape=(2,))\n",
        "encoded= Dense(20,activation='relu',kernel_initializer='normal')(inputData)\n",
        "encoded= Dense(10,activation='relu')(encoded)\n",
        "encoded= Dense(2,activation='relu')(encoded)  #latent space representation\n",
        "\n",
        "#decoder part\n",
        "decoded = Dense(10, activation='relu')(encoded)\n",
        "decoded = Dense(20, activation='relu')(decoded)\n",
        "decoded = Dense(2, activation='relu')(decoded)\n",
        "\n",
        "#combine the encoder and the decoder\n",
        "autoencoder = Model(inputData, decoded)\n",
        "\n",
        "#get only the encoder part\n",
        "encoderPart= Model(inputData,encoded)\n",
        "\n",
        "adam = optimizers.Adam(lr=0.1)\n",
        "autoencoder.compile(loss=\"mse\", optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n",
        "filepath=\"v5.best.hdf5\"\n",
        "checkpointer = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
        "\n",
        "#history = autoencoder.fit(x=train_data[:,:2], y=train_data[:,:2], batch_size=1, epochs=300, verbose=1, callbacks=[checkpointer,reduce_lr,early_stop], validation_split=0.3, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
        "history = autoencoder.fit(x=train_data[:,:2], y=train_data[:,:2], batch_size=1, epochs=300, verbose=1, callbacks=None, validation_split=0.3, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 3888 samples, validate on 1667 samples\n",
            "Epoch 1/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 4.6765 - acc: 0.5303 - val_loss: 4.2422 - val_acc: 0.5855\n",
            "Epoch 2/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 2.6334 - acc: 0.8040 - val_loss: 1.5140 - val_acc: 0.8374\n",
            "Epoch 3/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.1625 - acc: 0.8894 - val_loss: 1.2166 - val_acc: 0.9094\n",
            "Epoch 4/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.8951 - acc: 0.9120 - val_loss: 0.9383 - val_acc: 0.9460\n",
            "Epoch 5/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.8088 - acc: 0.9090 - val_loss: 0.7342 - val_acc: 0.9094\n",
            "Epoch 6/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.7881 - acc: 0.9115 - val_loss: 0.7355 - val_acc: 0.9094\n",
            "Epoch 7/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.7717 - acc: 0.9113 - val_loss: 0.7444 - val_acc: 0.9094\n",
            "Epoch 8/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.7499 - acc: 0.9082 - val_loss: 0.7729 - val_acc: 0.9094\n",
            "Epoch 9/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.7217 - acc: 0.8994 - val_loss: 0.6871 - val_acc: 0.8914\n",
            "Epoch 10/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.6966 - acc: 0.9038 - val_loss: 1.0782 - val_acc: 0.8362\n",
            "Epoch 11/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.6771 - acc: 0.9128 - val_loss: 0.6622 - val_acc: 0.9460\n",
            "Epoch 12/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.6593 - acc: 0.9174 - val_loss: 0.6487 - val_acc: 0.8908\n",
            "Epoch 13/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6459 - acc: 0.9164 - val_loss: 0.6100 - val_acc: 0.9274\n",
            "Epoch 14/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.6173 - acc: 0.9126 - val_loss: 0.6006 - val_acc: 0.9088\n",
            "Epoch 15/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5924 - acc: 0.9079 - val_loss: 0.5695 - val_acc: 0.9088\n",
            "Epoch 16/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5879 - acc: 0.9072 - val_loss: 0.5930 - val_acc: 0.9094\n",
            "Epoch 17/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5847 - acc: 0.9090 - val_loss: 0.5578 - val_acc: 0.9274\n",
            "Epoch 18/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5855 - acc: 0.9035 - val_loss: 0.5612 - val_acc: 0.9094\n",
            "Epoch 19/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5604 - acc: 0.9228 - val_loss: 0.5883 - val_acc: 0.9454\n",
            "Epoch 20/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5627 - acc: 0.9223 - val_loss: 0.5529 - val_acc: 0.9094\n",
            "Epoch 21/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5594 - acc: 0.9144 - val_loss: 0.5626 - val_acc: 0.9268\n",
            "Epoch 22/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5593 - acc: 0.9208 - val_loss: 0.5431 - val_acc: 0.8908\n",
            "Epoch 23/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5616 - acc: 0.9231 - val_loss: 0.5380 - val_acc: 0.9088\n",
            "Epoch 24/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.5587 - acc: 0.9231 - val_loss: 0.5312 - val_acc: 0.9088\n",
            "Epoch 25/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5585 - acc: 0.9151 - val_loss: 0.5781 - val_acc: 0.9088\n",
            "Epoch 26/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5522 - acc: 0.9162 - val_loss: 0.5484 - val_acc: 0.9088\n",
            "Epoch 27/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.5484 - acc: 0.9298 - val_loss: 0.5332 - val_acc: 0.9448\n",
            "Epoch 28/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5452 - acc: 0.9429 - val_loss: 0.5315 - val_acc: 0.9448\n",
            "Epoch 29/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5466 - acc: 0.9437 - val_loss: 0.5346 - val_acc: 0.9448\n",
            "Epoch 30/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6082 - acc: 0.9504 - val_loss: 0.6600 - val_acc: 0.9634\n",
            "Epoch 31/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6511 - acc: 0.9522 - val_loss: 0.6429 - val_acc: 0.9454\n",
            "Epoch 32/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.6429 - acc: 0.9498 - val_loss: 0.6450 - val_acc: 0.9274\n",
            "Epoch 33/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6380 - acc: 0.9493 - val_loss: 0.6373 - val_acc: 0.9454\n",
            "Epoch 34/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.6351 - acc: 0.9493 - val_loss: 0.6347 - val_acc: 0.9634\n",
            "Epoch 35/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.6398 - acc: 0.9493 - val_loss: 0.6368 - val_acc: 0.9454\n",
            "Epoch 36/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6311 - acc: 0.9480 - val_loss: 0.6362 - val_acc: 0.9634\n",
            "Epoch 37/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.6359 - acc: 0.9414 - val_loss: 0.6314 - val_acc: 0.9454\n",
            "Epoch 38/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6335 - acc: 0.9465 - val_loss: 0.6315 - val_acc: 0.9274\n",
            "Epoch 39/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6332 - acc: 0.9452 - val_loss: 0.6323 - val_acc: 0.9454\n",
            "Epoch 40/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.8247 - acc: 0.9450 - val_loss: 1.0743 - val_acc: 0.9454\n",
            "Epoch 41/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0428 - acc: 0.9455 - val_loss: 1.0675 - val_acc: 0.9454\n",
            "Epoch 42/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0394 - acc: 0.9452 - val_loss: 1.0578 - val_acc: 0.9454\n",
            "Epoch 43/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0388 - acc: 0.9455 - val_loss: 1.0510 - val_acc: 0.9454\n",
            "Epoch 44/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0444 - acc: 0.9457 - val_loss: 1.0820 - val_acc: 0.9454\n",
            "Epoch 45/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0391 - acc: 0.9465 - val_loss: 1.0611 - val_acc: 0.9454\n",
            "Epoch 46/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0425 - acc: 0.9452 - val_loss: 1.0518 - val_acc: 0.9454\n",
            "Epoch 47/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0398 - acc: 0.9452 - val_loss: 1.0504 - val_acc: 0.9454\n",
            "Epoch 48/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0389 - acc: 0.9452 - val_loss: 1.0749 - val_acc: 0.9454\n",
            "Epoch 49/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0375 - acc: 0.9452 - val_loss: 1.0508 - val_acc: 0.9454\n",
            "Epoch 50/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0375 - acc: 0.9447 - val_loss: 1.0545 - val_acc: 0.9454\n",
            "Epoch 51/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0382 - acc: 0.9452 - val_loss: 1.0555 - val_acc: 0.9454\n",
            "Epoch 52/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0407 - acc: 0.9450 - val_loss: 1.0505 - val_acc: 0.9454\n",
            "Epoch 53/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 1.0371 - acc: 0.9450 - val_loss: 1.0532 - val_acc: 0.9454\n",
            "Epoch 54/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0405 - acc: 0.9434 - val_loss: 1.0535 - val_acc: 0.9454\n",
            "Epoch 55/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0367 - acc: 0.9460 - val_loss: 1.0502 - val_acc: 0.9454\n",
            "Epoch 56/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0375 - acc: 0.9447 - val_loss: 1.0639 - val_acc: 0.9454\n",
            "Epoch 57/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0357 - acc: 0.9450 - val_loss: 1.0595 - val_acc: 0.9454\n",
            "Epoch 58/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0460 - acc: 0.9452 - val_loss: 1.0509 - val_acc: 0.9454\n",
            "Epoch 59/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0328 - acc: 0.9455 - val_loss: 1.0604 - val_acc: 0.9454\n",
            "Epoch 60/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0344 - acc: 0.9452 - val_loss: 1.0496 - val_acc: 0.9454\n",
            "Epoch 61/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0455 - acc: 0.9444 - val_loss: 1.0520 - val_acc: 0.9454\n",
            "Epoch 62/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0342 - acc: 0.9444 - val_loss: 1.0516 - val_acc: 0.9454\n",
            "Epoch 63/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0417 - acc: 0.9452 - val_loss: 1.0655 - val_acc: 0.9454\n",
            "Epoch 64/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0455 - acc: 0.9447 - val_loss: 1.0578 - val_acc: 0.9454\n",
            "Epoch 65/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0327 - acc: 0.9457 - val_loss: 1.0495 - val_acc: 0.9454\n",
            "Epoch 66/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 1.0342 - acc: 0.9455 - val_loss: 1.0517 - val_acc: 0.9454\n",
            "Epoch 67/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0382 - acc: 0.9450 - val_loss: 1.0483 - val_acc: 0.9454\n",
            "Epoch 68/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0364 - acc: 0.9447 - val_loss: 1.0499 - val_acc: 0.9454\n",
            "Epoch 69/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0352 - acc: 0.9457 - val_loss: 1.0631 - val_acc: 0.9454\n",
            "Epoch 70/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0401 - acc: 0.9457 - val_loss: 1.0497 - val_acc: 0.9454\n",
            "Epoch 71/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0370 - acc: 0.9450 - val_loss: 1.0623 - val_acc: 0.9454\n",
            "Epoch 72/300\n",
            "3888/3888 [==============================] - 15s 4ms/step - loss: 1.0423 - acc: 0.9450 - val_loss: 1.1311 - val_acc: 0.9454\n",
            "Epoch 73/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0417 - acc: 0.9450 - val_loss: 1.0579 - val_acc: 0.9454\n",
            "Epoch 74/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0422 - acc: 0.9455 - val_loss: 1.0785 - val_acc: 0.9454\n",
            "Epoch 75/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0402 - acc: 0.9447 - val_loss: 1.0567 - val_acc: 0.9454\n",
            "Epoch 76/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0448 - acc: 0.9447 - val_loss: 1.0868 - val_acc: 0.9454\n",
            "Epoch 77/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0399 - acc: 0.9455 - val_loss: 1.0591 - val_acc: 0.9454\n",
            "Epoch 78/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0900 - acc: 0.9460 - val_loss: 1.0607 - val_acc: 0.9454\n",
            "Epoch 79/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0437 - acc: 0.9444 - val_loss: 1.0577 - val_acc: 0.9454\n",
            "Epoch 80/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0436 - acc: 0.9447 - val_loss: 1.0584 - val_acc: 0.9454\n",
            "Epoch 81/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0371 - acc: 0.9450 - val_loss: 1.0599 - val_acc: 0.9454\n",
            "Epoch 82/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0426 - acc: 0.9450 - val_loss: 1.0530 - val_acc: 0.9454\n",
            "Epoch 83/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0366 - acc: 0.9450 - val_loss: 1.0552 - val_acc: 0.9454\n",
            "Epoch 84/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0389 - acc: 0.9452 - val_loss: 1.0559 - val_acc: 0.9454\n",
            "Epoch 85/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0351 - acc: 0.9455 - val_loss: 1.0635 - val_acc: 0.9454\n",
            "Epoch 86/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0417 - acc: 0.9455 - val_loss: 1.0674 - val_acc: 0.9454\n",
            "Epoch 87/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0402 - acc: 0.9442 - val_loss: 1.0633 - val_acc: 0.9454\n",
            "Epoch 88/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0338 - acc: 0.9439 - val_loss: 1.0530 - val_acc: 0.9454\n",
            "Epoch 89/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0418 - acc: 0.9439 - val_loss: 1.0545 - val_acc: 0.9454\n",
            "Epoch 90/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0332 - acc: 0.9452 - val_loss: 1.0525 - val_acc: 0.9454\n",
            "Epoch 91/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0401 - acc: 0.9442 - val_loss: 1.0510 - val_acc: 0.9454\n",
            "Epoch 92/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0340 - acc: 0.9437 - val_loss: 1.0572 - val_acc: 0.9454\n",
            "Epoch 93/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0344 - acc: 0.9437 - val_loss: 1.0525 - val_acc: 0.9454\n",
            "Epoch 94/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0660 - acc: 0.9473 - val_loss: 1.0876 - val_acc: 0.9454\n",
            "Epoch 95/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0504 - acc: 0.9455 - val_loss: 1.0535 - val_acc: 0.9454\n",
            "Epoch 96/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0476 - acc: 0.9439 - val_loss: 1.0606 - val_acc: 0.9454\n",
            "Epoch 97/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0413 - acc: 0.9447 - val_loss: 1.0542 - val_acc: 0.9454\n",
            "Epoch 98/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0300 - acc: 0.9444 - val_loss: 1.0525 - val_acc: 0.9454\n",
            "Epoch 99/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0594 - acc: 0.9447 - val_loss: 1.0835 - val_acc: 0.9454\n",
            "Epoch 100/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0577 - acc: 0.9450 - val_loss: 1.0605 - val_acc: 0.9454\n",
            "Epoch 101/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0398 - acc: 0.9439 - val_loss: 1.0493 - val_acc: 0.9454\n",
            "Epoch 102/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0288 - acc: 0.9447 - val_loss: 1.0537 - val_acc: 0.9454\n",
            "Epoch 103/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0360 - acc: 0.9437 - val_loss: 1.0505 - val_acc: 0.9454\n",
            "Epoch 104/300\n",
            "3888/3888 [==============================] - 10s 2ms/step - loss: 1.0301 - acc: 0.9447 - val_loss: 1.0535 - val_acc: 0.9454\n",
            "Epoch 105/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0410 - acc: 0.9444 - val_loss: 1.0595 - val_acc: 0.9454\n",
            "Epoch 106/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0305 - acc: 0.9447 - val_loss: 1.0505 - val_acc: 0.9454\n",
            "Epoch 107/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0320 - acc: 0.9450 - val_loss: 1.0477 - val_acc: 0.9454\n",
            "Epoch 108/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0293 - acc: 0.9444 - val_loss: 1.0626 - val_acc: 0.9454\n",
            "Epoch 109/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0322 - acc: 0.9439 - val_loss: 1.0488 - val_acc: 0.9454\n",
            "Epoch 110/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0321 - acc: 0.9442 - val_loss: 1.0473 - val_acc: 0.9454\n",
            "Epoch 111/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0389 - acc: 0.9424 - val_loss: 1.0722 - val_acc: 0.9454\n",
            "Epoch 112/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0371 - acc: 0.9439 - val_loss: 1.0618 - val_acc: 0.9454\n",
            "Epoch 113/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0470 - acc: 0.9444 - val_loss: 1.0654 - val_acc: 0.9454\n",
            "Epoch 114/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0344 - acc: 0.9444 - val_loss: 1.0509 - val_acc: 0.9454\n",
            "Epoch 115/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0291 - acc: 0.9437 - val_loss: 1.0469 - val_acc: 0.9454\n",
            "Epoch 116/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0355 - acc: 0.9439 - val_loss: 1.0530 - val_acc: 0.9454\n",
            "Epoch 117/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0304 - acc: 0.9439 - val_loss: 1.0498 - val_acc: 0.9274\n",
            "Epoch 118/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0315 - acc: 0.9432 - val_loss: 1.0496 - val_acc: 0.9454\n",
            "Epoch 119/300\n",
            "3888/3888 [==============================] - 10s 2ms/step - loss: 1.0344 - acc: 0.9447 - val_loss: 1.0501 - val_acc: 0.9454\n",
            "Epoch 120/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0285 - acc: 0.9442 - val_loss: 1.0478 - val_acc: 0.9454\n",
            "Epoch 121/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0289 - acc: 0.9447 - val_loss: 1.0534 - val_acc: 0.9454\n",
            "Epoch 122/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0657 - acc: 0.9424 - val_loss: 1.0534 - val_acc: 0.9454\n",
            "Epoch 123/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0744 - acc: 0.9468 - val_loss: 1.0850 - val_acc: 0.9454\n",
            "Epoch 124/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0513 - acc: 0.9439 - val_loss: 1.0519 - val_acc: 0.9454\n",
            "Epoch 125/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0345 - acc: 0.9434 - val_loss: 1.0512 - val_acc: 0.9454\n",
            "Epoch 126/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0340 - acc: 0.9429 - val_loss: 1.0617 - val_acc: 0.9454\n",
            "Epoch 127/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 1.0296 - acc: 0.9442 - val_loss: 1.0491 - val_acc: 0.9454\n",
            "Epoch 128/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0384 - acc: 0.9450 - val_loss: 1.0820 - val_acc: 0.9454\n",
            "Epoch 129/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0486 - acc: 0.9434 - val_loss: 1.0534 - val_acc: 0.9454\n",
            "Epoch 130/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0325 - acc: 0.9426 - val_loss: 1.0543 - val_acc: 0.9454\n",
            "Epoch 131/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0292 - acc: 0.9444 - val_loss: 1.0504 - val_acc: 0.9454\n",
            "Epoch 132/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0289 - acc: 0.9434 - val_loss: 1.0482 - val_acc: 0.9454\n",
            "Epoch 133/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0297 - acc: 0.9442 - val_loss: 1.0485 - val_acc: 0.9454\n",
            "Epoch 134/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0296 - acc: 0.9444 - val_loss: 1.0515 - val_acc: 0.9454\n",
            "Epoch 135/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0300 - acc: 0.9439 - val_loss: 1.0487 - val_acc: 0.9454\n",
            "Epoch 136/300\n",
            "3888/3888 [==============================] - 5s 1ms/step - loss: 1.0367 - acc: 0.9444 - val_loss: 1.0483 - val_acc: 0.9454\n",
            "Epoch 137/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0306 - acc: 0.9437 - val_loss: 1.0518 - val_acc: 0.9454\n",
            "Epoch 138/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0280 - acc: 0.9439 - val_loss: 1.0479 - val_acc: 0.9454\n",
            "Epoch 139/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0348 - acc: 0.9429 - val_loss: 1.0517 - val_acc: 0.9454\n",
            "Epoch 140/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 1.0336 - acc: 0.9442 - val_loss: 1.0512 - val_acc: 0.9454\n",
            "Epoch 141/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0305 - acc: 0.9429 - val_loss: 1.0499 - val_acc: 0.9454\n",
            "Epoch 142/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0328 - acc: 0.9406 - val_loss: 1.0474 - val_acc: 0.9454\n",
            "Epoch 143/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 1.0277 - acc: 0.9444 - val_loss: 1.0506 - val_acc: 0.9454\n",
            "Epoch 144/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0291 - acc: 0.9450 - val_loss: 1.0482 - val_acc: 0.9454\n",
            "Epoch 145/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0331 - acc: 0.9437 - val_loss: 1.0565 - val_acc: 0.9454\n",
            "Epoch 146/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0288 - acc: 0.9444 - val_loss: 1.0476 - val_acc: 0.9454\n",
            "Epoch 147/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.9550 - acc: 0.9414 - val_loss: 0.6377 - val_acc: 0.9274\n",
            "Epoch 148/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6269 - acc: 0.9257 - val_loss: 0.6325 - val_acc: 0.9274\n",
            "Epoch 149/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6457 - acc: 0.9303 - val_loss: 0.7015 - val_acc: 0.9454\n",
            "Epoch 150/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6982 - acc: 0.9452 - val_loss: 0.7148 - val_acc: 0.9454\n",
            "Epoch 151/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6690 - acc: 0.9347 - val_loss: 0.6350 - val_acc: 0.9274\n",
            "Epoch 152/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6247 - acc: 0.9226 - val_loss: 0.6334 - val_acc: 0.9454\n",
            "Epoch 153/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6500 - acc: 0.9285 - val_loss: 0.7053 - val_acc: 0.9454\n",
            "Epoch 154/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6554 - acc: 0.9290 - val_loss: 0.6361 - val_acc: 0.9274\n",
            "Epoch 155/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6202 - acc: 0.9172 - val_loss: 0.6276 - val_acc: 0.9274\n",
            "Epoch 156/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6236 - acc: 0.9218 - val_loss: 0.6283 - val_acc: 0.9094\n",
            "Epoch 157/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4895 - acc: 0.9231 - val_loss: 0.4372 - val_acc: 0.9082\n",
            "Epoch 158/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4068 - acc: 0.9275 - val_loss: 0.4074 - val_acc: 0.9460\n",
            "Epoch 159/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4067 - acc: 0.9334 - val_loss: 0.4102 - val_acc: 0.9460\n",
            "Epoch 160/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.4080 - acc: 0.9344 - val_loss: 0.4144 - val_acc: 0.8908\n",
            "Epoch 161/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.5039 - acc: 0.9450 - val_loss: 0.6293 - val_acc: 0.9460\n",
            "Epoch 162/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6030 - acc: 0.9434 - val_loss: 0.6062 - val_acc: 0.9454\n",
            "Epoch 163/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6105 - acc: 0.9375 - val_loss: 0.6278 - val_acc: 0.9094\n",
            "Epoch 164/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6164 - acc: 0.9234 - val_loss: 0.6227 - val_acc: 0.9454\n",
            "Epoch 165/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6156 - acc: 0.9298 - val_loss: 0.6410 - val_acc: 0.9274\n",
            "Epoch 166/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6159 - acc: 0.9306 - val_loss: 0.6184 - val_acc: 0.8914\n",
            "Epoch 167/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.6089 - acc: 0.9336 - val_loss: 0.6136 - val_acc: 0.9454\n",
            "Epoch 168/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6146 - acc: 0.9329 - val_loss: 0.6151 - val_acc: 0.9454\n",
            "Epoch 169/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6070 - acc: 0.9375 - val_loss: 0.6136 - val_acc: 0.9454\n",
            "Epoch 170/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.7092 - acc: 0.9331 - val_loss: 0.6982 - val_acc: 0.9454\n",
            "Epoch 171/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6540 - acc: 0.9326 - val_loss: 0.6182 - val_acc: 0.9454\n",
            "Epoch 172/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.6076 - acc: 0.9342 - val_loss: 0.6135 - val_acc: 0.9274\n",
            "Epoch 173/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.6068 - acc: 0.9388 - val_loss: 0.6295 - val_acc: 0.9640\n",
            "Epoch 174/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6055 - acc: 0.9375 - val_loss: 0.4901 - val_acc: 0.9274\n",
            "Epoch 175/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.5139 - acc: 0.9360 - val_loss: 0.6946 - val_acc: 0.9454\n",
            "Epoch 176/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5091 - acc: 0.9385 - val_loss: 0.4749 - val_acc: 0.9454\n",
            "Epoch 177/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4746 - acc: 0.9354 - val_loss: 0.4906 - val_acc: 0.9088\n",
            "Epoch 178/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4749 - acc: 0.9414 - val_loss: 0.4861 - val_acc: 0.9454\n",
            "Epoch 179/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.4780 - acc: 0.9396 - val_loss: 0.4809 - val_acc: 0.9454\n",
            "Epoch 180/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4736 - acc: 0.9393 - val_loss: 0.4811 - val_acc: 0.9460\n",
            "Epoch 181/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4744 - acc: 0.9411 - val_loss: 0.4736 - val_acc: 0.9460\n",
            "Epoch 182/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4723 - acc: 0.9406 - val_loss: 0.4831 - val_acc: 0.9640\n",
            "Epoch 183/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4749 - acc: 0.9398 - val_loss: 0.6648 - val_acc: 0.9640\n",
            "Epoch 184/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6927 - acc: 0.9349 - val_loss: 0.7297 - val_acc: 0.9454\n",
            "Epoch 185/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6284 - acc: 0.9617 - val_loss: 0.5070 - val_acc: 0.9454\n",
            "Epoch 186/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.4897 - acc: 0.9455 - val_loss: 0.4783 - val_acc: 0.9094\n",
            "Epoch 187/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4755 - acc: 0.9406 - val_loss: 0.4785 - val_acc: 0.9454\n",
            "Epoch 188/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4740 - acc: 0.9434 - val_loss: 0.4849 - val_acc: 0.9454\n",
            "Epoch 189/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4783 - acc: 0.9434 - val_loss: 0.4825 - val_acc: 0.9454\n",
            "Epoch 190/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.4783 - acc: 0.9419 - val_loss: 0.5274 - val_acc: 0.9274\n",
            "Epoch 191/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4715 - acc: 0.9419 - val_loss: 0.4903 - val_acc: 0.9268\n",
            "Epoch 192/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4729 - acc: 0.9419 - val_loss: 0.4733 - val_acc: 0.9454\n",
            "Epoch 193/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4733 - acc: 0.9429 - val_loss: 0.4810 - val_acc: 0.9454\n",
            "Epoch 194/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4727 - acc: 0.9396 - val_loss: 0.4988 - val_acc: 0.9640\n",
            "Epoch 195/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4794 - acc: 0.9414 - val_loss: 0.4810 - val_acc: 0.9274\n",
            "Epoch 196/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4748 - acc: 0.9408 - val_loss: 0.4793 - val_acc: 0.9274\n",
            "Epoch 197/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4766 - acc: 0.9411 - val_loss: 0.4802 - val_acc: 0.9460\n",
            "Epoch 198/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4744 - acc: 0.9393 - val_loss: 0.4788 - val_acc: 0.9094\n",
            "Epoch 199/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4792 - acc: 0.9385 - val_loss: 0.4823 - val_acc: 0.9274\n",
            "Epoch 200/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.4736 - acc: 0.9432 - val_loss: 0.4779 - val_acc: 0.9274\n",
            "Epoch 201/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4719 - acc: 0.9403 - val_loss: 0.5101 - val_acc: 0.9274\n",
            "Epoch 202/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4721 - acc: 0.9429 - val_loss: 0.4762 - val_acc: 0.9454\n",
            "Epoch 203/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4819 - acc: 0.9401 - val_loss: 0.4736 - val_acc: 0.9454\n",
            "Epoch 204/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.4709 - acc: 0.9408 - val_loss: 0.4769 - val_acc: 0.9640\n",
            "Epoch 205/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.4723 - acc: 0.9434 - val_loss: 0.4786 - val_acc: 0.9454\n",
            "Epoch 206/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4716 - acc: 0.9437 - val_loss: 0.4716 - val_acc: 0.9640\n",
            "Epoch 207/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.4977 - acc: 0.9408 - val_loss: 0.4729 - val_acc: 0.9454\n",
            "Epoch 208/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4720 - acc: 0.9401 - val_loss: 0.4810 - val_acc: 0.9640\n",
            "Epoch 209/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.4714 - acc: 0.9393 - val_loss: 0.4779 - val_acc: 0.9640\n",
            "Epoch 210/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.4743 - acc: 0.9396 - val_loss: 0.4739 - val_acc: 0.9274\n",
            "Epoch 211/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.4709 - acc: 0.9432 - val_loss: 0.4796 - val_acc: 0.9460\n",
            "Epoch 212/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4793 - acc: 0.9385 - val_loss: 0.4720 - val_acc: 0.9454\n",
            "Epoch 213/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.4724 - acc: 0.9388 - val_loss: 0.4773 - val_acc: 0.9640\n",
            "Epoch 214/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.4763 - acc: 0.9408 - val_loss: 0.4718 - val_acc: 0.9454\n",
            "Epoch 215/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4705 - acc: 0.9403 - val_loss: 0.4750 - val_acc: 0.9274\n",
            "Epoch 216/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.4725 - acc: 0.9421 - val_loss: 0.4770 - val_acc: 0.9454\n",
            "Epoch 217/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6725 - acc: 0.9444 - val_loss: 0.7245 - val_acc: 0.9274\n",
            "Epoch 218/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.7145 - acc: 0.9403 - val_loss: 0.7204 - val_acc: 0.9454\n",
            "Epoch 219/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.7142 - acc: 0.9367 - val_loss: 0.7362 - val_acc: 0.9274\n",
            "Epoch 220/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.7927 - acc: 0.9442 - val_loss: 0.8830 - val_acc: 0.9640\n",
            "Epoch 221/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.8551 - acc: 0.9498 - val_loss: 0.8700 - val_acc: 0.9460\n",
            "Epoch 222/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.1156 - acc: 0.9504 - val_loss: 1.1642 - val_acc: 0.9454\n",
            "Epoch 223/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.1437 - acc: 0.9514 - val_loss: 1.1634 - val_acc: 0.9634\n",
            "Epoch 224/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.1431 - acc: 0.9545 - val_loss: 1.1633 - val_acc: 0.9634\n",
            "Epoch 225/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.1432 - acc: 0.9527 - val_loss: 1.1983 - val_acc: 0.9274\n",
            "Epoch 226/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.1360 - acc: 0.9522 - val_loss: 1.1526 - val_acc: 0.9634\n",
            "Epoch 227/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.1344 - acc: 0.9534 - val_loss: 1.1526 - val_acc: 0.9634\n",
            "Epoch 228/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 1.0561 - acc: 0.9491 - val_loss: 0.6990 - val_acc: 0.9094\n",
            "Epoch 229/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.6787 - acc: 0.9426 - val_loss: 0.6984 - val_acc: 0.9274\n",
            "Epoch 230/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6765 - acc: 0.9439 - val_loss: 0.6871 - val_acc: 0.9454\n",
            "Epoch 231/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6933 - acc: 0.9434 - val_loss: 0.7169 - val_acc: 0.9454\n",
            "Epoch 232/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.9741 - acc: 0.9439 - val_loss: 1.0521 - val_acc: 0.9454\n",
            "Epoch 233/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.8789 - acc: 0.9406 - val_loss: 0.6319 - val_acc: 0.9460\n",
            "Epoch 234/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.6073 - acc: 0.9514 - val_loss: 0.5855 - val_acc: 0.9454\n",
            "Epoch 235/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.5206 - acc: 0.9596 - val_loss: 0.3015 - val_acc: 0.9454\n",
            "Epoch 236/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.2910 - acc: 0.9601 - val_loss: 0.2973 - val_acc: 0.9820\n",
            "Epoch 237/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.2845 - acc: 0.9606 - val_loss: 0.2832 - val_acc: 0.9820\n",
            "Epoch 238/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.2745 - acc: 0.9604 - val_loss: 0.1560 - val_acc: 0.9454\n",
            "Epoch 239/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.1585 - acc: 0.9573 - val_loss: 0.1566 - val_acc: 0.9820\n",
            "Epoch 240/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.1541 - acc: 0.9604 - val_loss: 0.1602 - val_acc: 0.9274\n",
            "Epoch 241/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.1291 - acc: 0.9558 - val_loss: 0.1083 - val_acc: 0.9274\n",
            "Epoch 242/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.1106 - acc: 0.9560 - val_loss: 0.1147 - val_acc: 0.9460\n",
            "Epoch 243/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0829 - acc: 0.9560 - val_loss: 0.0732 - val_acc: 0.9460\n",
            "Epoch 244/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0730 - acc: 0.9570 - val_loss: 0.0706 - val_acc: 0.9820\n",
            "Epoch 245/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0727 - acc: 0.9576 - val_loss: 0.0700 - val_acc: 0.9820\n",
            "Epoch 246/300\n",
            "3888/3888 [==============================] - 11s 3ms/step - loss: 0.0661 - acc: 0.9565 - val_loss: 0.0600 - val_acc: 0.9634\n",
            "Epoch 247/300\n",
            "3888/3888 [==============================] - 18s 5ms/step - loss: 0.0555 - acc: 0.9542 - val_loss: 0.0525 - val_acc: 0.9820\n",
            "Epoch 248/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0640 - acc: 0.9555 - val_loss: 0.0981 - val_acc: 0.9274\n",
            "Epoch 249/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0619 - acc: 0.9496 - val_loss: 0.0535 - val_acc: 0.9820\n",
            "Epoch 250/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0533 - acc: 0.9547 - val_loss: 0.0627 - val_acc: 0.9820\n",
            "Epoch 251/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0480 - acc: 0.9509 - val_loss: 0.0435 - val_acc: 0.9454\n",
            "Epoch 252/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0453 - acc: 0.9527 - val_loss: 0.0426 - val_acc: 0.9454\n",
            "Epoch 253/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0457 - acc: 0.9527 - val_loss: 0.0523 - val_acc: 0.9274\n",
            "Epoch 254/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0453 - acc: 0.9532 - val_loss: 0.0407 - val_acc: 0.9634\n",
            "Epoch 255/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0461 - acc: 0.9552 - val_loss: 0.0413 - val_acc: 0.9640\n",
            "Epoch 256/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0451 - acc: 0.9555 - val_loss: 0.0439 - val_acc: 0.9460\n",
            "Epoch 257/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0449 - acc: 0.9519 - val_loss: 0.0440 - val_acc: 0.9820\n",
            "Epoch 258/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0447 - acc: 0.9532 - val_loss: 0.0439 - val_acc: 0.9274\n",
            "Epoch 259/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0450 - acc: 0.9555 - val_loss: 0.0417 - val_acc: 0.9820\n",
            "Epoch 260/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0447 - acc: 0.9514 - val_loss: 0.0418 - val_acc: 0.9274\n",
            "Epoch 261/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0484 - acc: 0.9509 - val_loss: 0.0434 - val_acc: 0.9640\n",
            "Epoch 262/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0457 - acc: 0.9475 - val_loss: 0.0442 - val_acc: 0.9274\n",
            "Epoch 263/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0457 - acc: 0.9516 - val_loss: 0.0447 - val_acc: 0.9454\n",
            "Epoch 264/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0451 - acc: 0.9504 - val_loss: 0.0413 - val_acc: 0.9274\n",
            "Epoch 265/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0463 - acc: 0.9506 - val_loss: 0.0442 - val_acc: 0.9634\n",
            "Epoch 266/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0464 - acc: 0.9506 - val_loss: 0.0449 - val_acc: 0.9274\n",
            "Epoch 267/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0476 - acc: 0.9534 - val_loss: 0.0443 - val_acc: 0.9820\n",
            "Epoch 268/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0459 - acc: 0.9534 - val_loss: 0.0448 - val_acc: 0.9454\n",
            "Epoch 269/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0460 - acc: 0.9506 - val_loss: 0.0446 - val_acc: 0.9274\n",
            "Epoch 270/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0469 - acc: 0.9486 - val_loss: 0.0425 - val_acc: 0.9454\n",
            "Epoch 271/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.0462 - acc: 0.9527 - val_loss: 0.0426 - val_acc: 0.9820\n",
            "Epoch 272/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0461 - acc: 0.9537 - val_loss: 0.0414 - val_acc: 0.9820\n",
            "Epoch 273/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0462 - acc: 0.9506 - val_loss: 0.0435 - val_acc: 0.9820\n",
            "Epoch 274/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.1269 - acc: 0.9578 - val_loss: 0.1018 - val_acc: 0.9454\n",
            "Epoch 275/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0652 - acc: 0.9578 - val_loss: 0.0483 - val_acc: 0.9820\n",
            "Epoch 276/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0435 - acc: 0.9529 - val_loss: 0.0368 - val_acc: 0.9274\n",
            "Epoch 277/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0338 - acc: 0.9604 - val_loss: 0.0223 - val_acc: 0.9820\n",
            "Epoch 278/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0287 - acc: 0.9542 - val_loss: 0.0149 - val_acc: 0.9454\n",
            "Epoch 279/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0158 - acc: 0.9570 - val_loss: 0.0155 - val_acc: 0.9820\n",
            "Epoch 280/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0141 - acc: 0.9565 - val_loss: 0.0192 - val_acc: 0.9820\n",
            "Epoch 281/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0145 - acc: 0.9570 - val_loss: 0.0139 - val_acc: 0.9274\n",
            "Epoch 282/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0134 - acc: 0.9583 - val_loss: 0.0117 - val_acc: 0.9820\n",
            "Epoch 283/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0137 - acc: 0.9576 - val_loss: 0.0184 - val_acc: 0.9274\n",
            "Epoch 284/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.0133 - acc: 0.9565 - val_loss: 0.0107 - val_acc: 0.9640\n",
            "Epoch 285/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0132 - acc: 0.9547 - val_loss: 0.0128 - val_acc: 0.9454\n",
            "Epoch 286/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0129 - acc: 0.9558 - val_loss: 0.0106 - val_acc: 0.9820\n",
            "Epoch 287/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0166 - acc: 0.9527 - val_loss: 0.0097 - val_acc: 0.9460\n",
            "Epoch 288/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0122 - acc: 0.9524 - val_loss: 0.0160 - val_acc: 0.9274\n",
            "Epoch 289/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0128 - acc: 0.9578 - val_loss: 0.0100 - val_acc: 0.9274\n",
            "Epoch 290/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0122 - acc: 0.9560 - val_loss: 0.0208 - val_acc: 0.9820\n",
            "Epoch 291/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0124 - acc: 0.9514 - val_loss: 0.0095 - val_acc: 0.9274\n",
            "Epoch 292/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0119 - acc: 0.9573 - val_loss: 0.0131 - val_acc: 0.9820\n",
            "Epoch 293/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0127 - acc: 0.9534 - val_loss: 0.0129 - val_acc: 0.9274\n",
            "Epoch 294/300\n",
            "3888/3888 [==============================] - 7s 2ms/step - loss: 0.0120 - acc: 0.9534 - val_loss: 0.0108 - val_acc: 0.9460\n",
            "Epoch 295/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0121 - acc: 0.9537 - val_loss: 0.0122 - val_acc: 0.9274\n",
            "Epoch 296/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0121 - acc: 0.9529 - val_loss: 0.0107 - val_acc: 0.9460\n",
            "Epoch 297/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0142 - acc: 0.9524 - val_loss: 0.0101 - val_acc: 0.9820\n",
            "Epoch 298/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0109 - acc: 0.9486 - val_loss: 0.0112 - val_acc: 0.9640\n",
            "Epoch 299/300\n",
            "3888/3888 [==============================] - 8s 2ms/step - loss: 0.0120 - acc: 0.9550 - val_loss: 0.0091 - val_acc: 0.9274\n",
            "Epoch 300/300\n",
            "3888/3888 [==============================] - 9s 2ms/step - loss: 0.0119 - acc: 0.9555 - val_loss: 0.0571 - val_acc: 0.9274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iiyrefWy-c_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the encoded representation of the train Data and test Data\n",
        "\n",
        "train_data_Encoded= encoderPart.predict(train_data[:,:2])\n",
        "test_data_Encoded= encoderPart.predict(test_data[:,:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnFXGpob30Z2",
        "colab_type": "code",
        "outputId": "4c7e0a5f-0559-41b2-abff-c7cacbe9f7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data[0,2]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5012.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD4krONu36bH",
        "colab_type": "code",
        "outputId": "03a1af64-e750-4842-fc90-1d974a6dd775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_Encoded[0,1]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDM-0Atc4Bny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make new train data and test Data\n",
        "newTrainData= []\n",
        "newTestData= []\n",
        "for i in range(train_data.shape[0]):\n",
        "  newTrainData.append([train_data_Encoded[i,0],train_data_Encoded[i,1],train_data[i,2]])\n",
        "  \n",
        "for i in range(test_data.shape[0]):\n",
        "  newTestData.append([test_data_Encoded[i,0],test_data_Encoded[i,1],test_data[i,2]])\n",
        "  \n",
        "newTrainData= np.array(newTrainData)\n",
        "newTestData= np.array(newTestData)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89pr8s-v-mUN",
        "colab_type": "code",
        "outputId": "a28c403e-c7f6-4edd-89f8-c4348fef7f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "newTrainData.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5555, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gAJ2TdQ_J60",
        "colab_type": "code",
        "outputId": "dd6c9192-d59e-48d0-ffd0-9ad2d8e56c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "newTestData.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svje7_5T_SdK",
        "colab_type": "code",
        "outputId": "b773936a-6d6f-426e-cf73-0e8ebf7cefd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "newTrainData[0,]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   9.22684765,    4.78407955, 1232.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsTXAOfz_XiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build the neural Network for prediction\n",
        "\n",
        "neuralNetwork = Sequential()\n",
        "ip_dim = 2\n",
        "#model.add(Dropout(0.1, input_shape=(ip_dim,))\n",
        "neuralNetwork.add(Dense(10, input_dim=ip_dim, kernel_initializer='normal', activation='relu'))\n",
        "neuralNetwork.add(Dense(5, kernel_initializer='normal', activation='relu'))\n",
        "#neuralNetwork.add(Dropout(0.1))\n",
        "neuralNetwork.add(Dense(2, kernel_initializer='normal', activation='relu'))\n",
        "#neuralNetwork.add(Dropout(0.1))\n",
        "#neuralNetwork.add(Dense(10, kernel_initializer='normal', activation='relu'))\n",
        "neuralNetwork.add(Dense(1, kernel_initializer='normal',activation=\"exponential\"))\n",
        "# Compile model\n",
        "neuralNetwork.compile(loss=poisson_dev, optimizer='adam',metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l8ImIMnAPKs",
        "colab_type": "code",
        "outputId": "0dd35850-301c-4bd6-bde9-b16c90b94af9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16854
        }
      },
      "source": [
        "history1 = neuralNetwork.fit(x=newTrainData[:,:ip_dim], y=newTrainData[:,ip_dim], batch_size=1, epochs=500, verbose=1, callbacks=None, validation_split=0.33, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3721 samples, validate on 1834 samples\n",
            "Epoch 1/500\n",
            "3721/3721 [==============================] - 9s 2ms/step - loss: 74912.3947 - acc: 0.0000e+00 - val_loss: 44450.4332 - val_acc: 0.0000e+00\n",
            "Epoch 2/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 23407.7066 - acc: 0.0000e+00 - val_loss: 5456.2832 - val_acc: 0.0000e+00\n",
            "Epoch 3/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3808.1528 - acc: 0.0000e+00 - val_loss: 3632.1807 - val_acc: 0.0000e+00\n",
            "Epoch 4/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3650.1221 - acc: 0.0000e+00 - val_loss: 4405.1707 - val_acc: 0.0000e+00\n",
            "Epoch 5/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3674.0719 - acc: 0.0000e+00 - val_loss: 3550.0159 - val_acc: 0.0000e+00\n",
            "Epoch 6/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3697.7130 - acc: 0.0000e+00 - val_loss: 3632.5996 - val_acc: 0.0000e+00\n",
            "Epoch 7/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3676.9774 - acc: 0.0000e+00 - val_loss: 3560.6939 - val_acc: 0.0000e+00\n",
            "Epoch 8/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3663.8535 - acc: 0.0000e+00 - val_loss: 3679.4880 - val_acc: 0.0000e+00\n",
            "Epoch 9/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3666.8135 - acc: 0.0000e+00 - val_loss: 3626.7874 - val_acc: 0.0000e+00\n",
            "Epoch 10/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3697.0968 - acc: 0.0000e+00 - val_loss: 3729.7999 - val_acc: 0.0000e+00\n",
            "Epoch 11/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3653.0639 - acc: 0.0000e+00 - val_loss: 3553.8419 - val_acc: 0.0000e+00\n",
            "Epoch 12/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3676.1887 - acc: 0.0000e+00 - val_loss: 3554.5082 - val_acc: 0.0000e+00\n",
            "Epoch 13/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3681.9866 - acc: 2.6874e-04 - val_loss: 3567.2677 - val_acc: 0.0000e+00\n",
            "Epoch 14/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3670.4548 - acc: 0.0000e+00 - val_loss: 3699.6365 - val_acc: 0.0000e+00\n",
            "Epoch 15/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3669.0709 - acc: 0.0000e+00 - val_loss: 3711.6583 - val_acc: 0.0000e+00\n",
            "Epoch 16/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3661.7412 - acc: 0.0000e+00 - val_loss: 3639.7855 - val_acc: 0.0000e+00\n",
            "Epoch 17/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3686.0148 - acc: 0.0000e+00 - val_loss: 3692.0530 - val_acc: 0.0000e+00\n",
            "Epoch 18/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3677.4599 - acc: 0.0000e+00 - val_loss: 3641.4862 - val_acc: 0.0000e+00\n",
            "Epoch 19/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3642.0449 - acc: 0.0000e+00 - val_loss: 3589.6251 - val_acc: 0.0011\n",
            "Epoch 20/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3678.2656 - acc: 0.0000e+00 - val_loss: 3590.5784 - val_acc: 0.0000e+00\n",
            "Epoch 21/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3679.6629 - acc: 2.6874e-04 - val_loss: 4068.3202 - val_acc: 0.0000e+00\n",
            "Epoch 22/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3670.0894 - acc: 2.6874e-04 - val_loss: 3569.6653 - val_acc: 0.0000e+00\n",
            "Epoch 23/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3683.0694 - acc: 2.6874e-04 - val_loss: 3834.3057 - val_acc: 0.0000e+00\n",
            "Epoch 24/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3675.9901 - acc: 2.6874e-04 - val_loss: 3555.2056 - val_acc: 0.0000e+00\n",
            "Epoch 25/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3682.5543 - acc: 0.0000e+00 - val_loss: 3601.7509 - val_acc: 0.0000e+00\n",
            "Epoch 26/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3678.0103 - acc: 0.0000e+00 - val_loss: 3586.6722 - val_acc: 0.0000e+00\n",
            "Epoch 27/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3651.3398 - acc: 2.6874e-04 - val_loss: 3588.2990 - val_acc: 0.0000e+00\n",
            "Epoch 28/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3682.1743 - acc: 0.0000e+00 - val_loss: 4028.8022 - val_acc: 0.0000e+00\n",
            "Epoch 29/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3662.7684 - acc: 0.0000e+00 - val_loss: 4169.6376 - val_acc: 0.0000e+00\n",
            "Epoch 30/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3661.9723 - acc: 0.0000e+00 - val_loss: 3593.1235 - val_acc: 0.0000e+00\n",
            "Epoch 31/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3655.2268 - acc: 0.0000e+00 - val_loss: 3630.2640 - val_acc: 0.0000e+00\n",
            "Epoch 32/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3692.9374 - acc: 0.0000e+00 - val_loss: 3618.1844 - val_acc: 0.0000e+00\n",
            "Epoch 33/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3679.7281 - acc: 0.0000e+00 - val_loss: 3572.2159 - val_acc: 0.0000e+00\n",
            "Epoch 34/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3655.2120 - acc: 0.0000e+00 - val_loss: 3584.4391 - val_acc: 0.0000e+00\n",
            "Epoch 35/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.7072 - acc: 0.0000e+00 - val_loss: 3848.8865 - val_acc: 0.0016\n",
            "Epoch 36/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3679.3020 - acc: 0.0000e+00 - val_loss: 3729.1426 - val_acc: 0.0000e+00\n",
            "Epoch 37/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3694.5548 - acc: 0.0000e+00 - val_loss: 3656.2538 - val_acc: 0.0000e+00\n",
            "Epoch 38/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3660.7994 - acc: 0.0000e+00 - val_loss: 3568.6191 - val_acc: 0.0000e+00\n",
            "Epoch 39/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3678.3018 - acc: 2.6874e-04 - val_loss: 3571.8089 - val_acc: 0.0000e+00\n",
            "Epoch 40/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3662.9584 - acc: 0.0000e+00 - val_loss: 3637.2382 - val_acc: 0.0000e+00\n",
            "Epoch 41/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3671.4854 - acc: 2.6874e-04 - val_loss: 3680.2522 - val_acc: 0.0000e+00\n",
            "Epoch 42/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3688.5004 - acc: 2.6874e-04 - val_loss: 3595.7508 - val_acc: 0.0000e+00\n",
            "Epoch 43/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3665.6493 - acc: 0.0000e+00 - val_loss: 3553.3284 - val_acc: 0.0000e+00\n",
            "Epoch 44/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3661.8239 - acc: 0.0000e+00 - val_loss: 3640.6950 - val_acc: 0.0000e+00\n",
            "Epoch 45/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3683.2194 - acc: 0.0000e+00 - val_loss: 3592.1728 - val_acc: 0.0000e+00\n",
            "Epoch 46/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3673.0786 - acc: 0.0000e+00 - val_loss: 3566.4827 - val_acc: 0.0000e+00\n",
            "Epoch 47/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3664.3471 - acc: 0.0000e+00 - val_loss: 3557.1713 - val_acc: 0.0000e+00\n",
            "Epoch 48/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3677.3810 - acc: 0.0000e+00 - val_loss: 3582.7597 - val_acc: 0.0000e+00\n",
            "Epoch 49/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 3669.1153 - acc: 0.0000e+00 - val_loss: 3549.0824 - val_acc: 0.0000e+00\n",
            "Epoch 50/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3683.5752 - acc: 2.6874e-04 - val_loss: 3569.6981 - val_acc: 0.0000e+00\n",
            "Epoch 51/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3673.7701 - acc: 0.0000e+00 - val_loss: 3550.7151 - val_acc: 0.0000e+00\n",
            "Epoch 52/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3659.3505 - acc: 0.0000e+00 - val_loss: 3575.1687 - val_acc: 0.0000e+00\n",
            "Epoch 53/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3671.2579 - acc: 0.0000e+00 - val_loss: 3568.7712 - val_acc: 0.0000e+00\n",
            "Epoch 54/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3663.2838 - acc: 0.0000e+00 - val_loss: 3642.5021 - val_acc: 0.0000e+00\n",
            "Epoch 55/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 3657.1550 - acc: 0.0000e+00 - val_loss: 3791.5302 - val_acc: 0.0000e+00\n",
            "Epoch 56/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3678.3433 - acc: 0.0000e+00 - val_loss: 3587.8876 - val_acc: 0.0000e+00\n",
            "Epoch 57/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3672.0386 - acc: 0.0000e+00 - val_loss: 3607.1402 - val_acc: 0.0000e+00\n",
            "Epoch 58/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3684.4874 - acc: 0.0000e+00 - val_loss: 3848.3888 - val_acc: 0.0000e+00\n",
            "Epoch 59/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3664.9435 - acc: 0.0000e+00 - val_loss: 3569.0810 - val_acc: 0.0000e+00\n",
            "Epoch 60/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3662.4257 - acc: 0.0000e+00 - val_loss: 3565.2284 - val_acc: 0.0000e+00\n",
            "Epoch 61/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3666.8820 - acc: 0.0000e+00 - val_loss: 3551.7395 - val_acc: 0.0000e+00\n",
            "Epoch 62/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3640.5040 - acc: 0.0000e+00 - val_loss: 3573.6609 - val_acc: 0.0000e+00\n",
            "Epoch 63/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3645.6111 - acc: 0.0000e+00 - val_loss: 3764.2455 - val_acc: 5.4526e-04\n",
            "Epoch 64/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3687.0828 - acc: 0.0000e+00 - val_loss: 3617.0774 - val_acc: 0.0000e+00\n",
            "Epoch 65/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3652.9129 - acc: 0.0000e+00 - val_loss: 3635.4443 - val_acc: 0.0000e+00\n",
            "Epoch 66/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3657.1891 - acc: 0.0000e+00 - val_loss: 3558.8290 - val_acc: 0.0000e+00\n",
            "Epoch 67/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3640.8635 - acc: 0.0000e+00 - val_loss: 3591.7744 - val_acc: 0.0000e+00\n",
            "Epoch 68/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3651.5858 - acc: 0.0000e+00 - val_loss: 3553.3011 - val_acc: 0.0000e+00\n",
            "Epoch 69/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3668.6703 - acc: 0.0000e+00 - val_loss: 3791.5795 - val_acc: 0.0000e+00\n",
            "Epoch 70/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3661.3883 - acc: 0.0000e+00 - val_loss: 3555.5623 - val_acc: 0.0000e+00\n",
            "Epoch 71/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3664.7335 - acc: 0.0000e+00 - val_loss: 3662.0953 - val_acc: 0.0000e+00\n",
            "Epoch 72/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3669.1936 - acc: 0.0000e+00 - val_loss: 3566.3437 - val_acc: 0.0000e+00\n",
            "Epoch 73/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3659.6323 - acc: 2.6874e-04 - val_loss: 3617.6210 - val_acc: 0.0000e+00\n",
            "Epoch 74/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3656.9999 - acc: 0.0000e+00 - val_loss: 3564.4018 - val_acc: 0.0000e+00\n",
            "Epoch 75/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3643.5124 - acc: 0.0000e+00 - val_loss: 3560.2721 - val_acc: 0.0000e+00\n",
            "Epoch 76/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3659.9744 - acc: 0.0000e+00 - val_loss: 3581.6508 - val_acc: 0.0000e+00\n",
            "Epoch 77/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3652.8761 - acc: 0.0000e+00 - val_loss: 3567.1744 - val_acc: 0.0000e+00\n",
            "Epoch 78/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3675.7604 - acc: 0.0000e+00 - val_loss: 3642.5448 - val_acc: 0.0000e+00\n",
            "Epoch 79/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 3645.8016 - acc: 0.0000e+00 - val_loss: 3620.3791 - val_acc: 0.0000e+00\n",
            "Epoch 80/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 3663.7538 - acc: 0.0000e+00 - val_loss: 3623.8813 - val_acc: 0.0000e+00\n",
            "Epoch 81/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3659.6467 - acc: 0.0000e+00 - val_loss: 3551.1887 - val_acc: 0.0000e+00\n",
            "Epoch 82/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.8965 - acc: 2.6874e-04 - val_loss: 3797.5105 - val_acc: 0.0000e+00\n",
            "Epoch 83/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3649.1477 - acc: 0.0000e+00 - val_loss: 3665.8539 - val_acc: 0.0000e+00\n",
            "Epoch 84/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3662.5680 - acc: 0.0000e+00 - val_loss: 3552.0464 - val_acc: 0.0000e+00\n",
            "Epoch 85/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3634.6588 - acc: 0.0000e+00 - val_loss: 3578.3177 - val_acc: 0.0000e+00\n",
            "Epoch 86/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3670.6015 - acc: 0.0000e+00 - val_loss: 3553.2711 - val_acc: 0.0000e+00\n",
            "Epoch 87/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3656.9621 - acc: 0.0000e+00 - val_loss: 3837.2655 - val_acc: 0.0000e+00\n",
            "Epoch 88/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3653.7493 - acc: 0.0000e+00 - val_loss: 3558.4975 - val_acc: 0.0000e+00\n",
            "Epoch 89/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3653.7151 - acc: 0.0000e+00 - val_loss: 3552.4278 - val_acc: 0.0000e+00\n",
            "Epoch 90/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3664.6492 - acc: 0.0000e+00 - val_loss: 3690.4879 - val_acc: 0.0000e+00\n",
            "Epoch 91/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3671.0306 - acc: 0.0000e+00 - val_loss: 3710.1962 - val_acc: 0.0000e+00\n",
            "Epoch 92/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3652.4578 - acc: 2.6874e-04 - val_loss: 3566.1346 - val_acc: 0.0000e+00\n",
            "Epoch 93/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3637.9187 - acc: 0.0000e+00 - val_loss: 3677.4439 - val_acc: 0.0000e+00\n",
            "Epoch 94/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3666.2767 - acc: 0.0000e+00 - val_loss: 3921.8860 - val_acc: 0.0000e+00\n",
            "Epoch 95/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3632.6291 - acc: 0.0000e+00 - val_loss: 3627.0174 - val_acc: 0.0000e+00\n",
            "Epoch 96/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3655.2515 - acc: 5.3749e-04 - val_loss: 3616.2459 - val_acc: 0.0000e+00\n",
            "Epoch 97/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3675.2254 - acc: 0.0000e+00 - val_loss: 3571.0687 - val_acc: 0.0000e+00\n",
            "Epoch 98/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3636.1631 - acc: 0.0000e+00 - val_loss: 3698.5876 - val_acc: 0.0000e+00\n",
            "Epoch 99/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3665.8472 - acc: 2.6874e-04 - val_loss: 3580.5679 - val_acc: 0.0000e+00\n",
            "Epoch 100/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3661.6086 - acc: 0.0000e+00 - val_loss: 3592.9400 - val_acc: 0.0000e+00\n",
            "Epoch 101/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3655.9909 - acc: 0.0000e+00 - val_loss: 3794.9193 - val_acc: 0.0000e+00\n",
            "Epoch 102/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3642.6814 - acc: 0.0000e+00 - val_loss: 3981.0028 - val_acc: 0.0000e+00\n",
            "Epoch 103/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3655.4200 - acc: 2.6874e-04 - val_loss: 3595.0313 - val_acc: 0.0000e+00\n",
            "Epoch 104/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3645.4111 - acc: 0.0000e+00 - val_loss: 3572.0764 - val_acc: 0.0000e+00\n",
            "Epoch 105/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3664.3377 - acc: 0.0000e+00 - val_loss: 3619.8248 - val_acc: 0.0000e+00\n",
            "Epoch 106/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3664.4233 - acc: 0.0000e+00 - val_loss: 3569.8325 - val_acc: 0.0000e+00\n",
            "Epoch 107/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3661.3410 - acc: 0.0000e+00 - val_loss: 3593.5776 - val_acc: 0.0000e+00\n",
            "Epoch 108/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3654.3134 - acc: 0.0000e+00 - val_loss: 4118.7745 - val_acc: 0.0000e+00\n",
            "Epoch 109/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3662.5033 - acc: 0.0000e+00 - val_loss: 3573.2988 - val_acc: 0.0000e+00\n",
            "Epoch 110/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3637.4377 - acc: 2.6874e-04 - val_loss: 3568.4203 - val_acc: 0.0000e+00\n",
            "Epoch 111/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3649.8427 - acc: 0.0000e+00 - val_loss: 3589.5562 - val_acc: 0.0000e+00\n",
            "Epoch 112/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3648.5601 - acc: 0.0000e+00 - val_loss: 3616.3140 - val_acc: 0.0000e+00\n",
            "Epoch 113/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3656.3973 - acc: 2.6874e-04 - val_loss: 3684.1274 - val_acc: 0.0000e+00\n",
            "Epoch 114/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.7966 - acc: 2.6874e-04 - val_loss: 3549.6873 - val_acc: 0.0000e+00\n",
            "Epoch 115/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3642.0741 - acc: 0.0000e+00 - val_loss: 3607.1254 - val_acc: 0.0000e+00\n",
            "Epoch 116/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3656.5491 - acc: 0.0000e+00 - val_loss: 3596.8837 - val_acc: 0.0000e+00\n",
            "Epoch 117/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3652.0207 - acc: 0.0000e+00 - val_loss: 3644.9256 - val_acc: 0.0000e+00\n",
            "Epoch 118/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.5385 - acc: 0.0000e+00 - val_loss: 3693.4548 - val_acc: 0.0000e+00\n",
            "Epoch 119/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3664.3053 - acc: 0.0000e+00 - val_loss: 3562.1093 - val_acc: 0.0000e+00\n",
            "Epoch 120/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3659.6169 - acc: 0.0000e+00 - val_loss: 3611.1849 - val_acc: 0.0000e+00\n",
            "Epoch 121/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.1129 - acc: 2.6874e-04 - val_loss: 3663.4008 - val_acc: 0.0000e+00\n",
            "Epoch 122/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3649.6951 - acc: 0.0000e+00 - val_loss: 3728.2818 - val_acc: 0.0000e+00\n",
            "Epoch 123/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3652.6348 - acc: 0.0000e+00 - val_loss: 3654.8079 - val_acc: 0.0000e+00\n",
            "Epoch 124/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3645.5657 - acc: 0.0000e+00 - val_loss: 3550.5457 - val_acc: 0.0000e+00\n",
            "Epoch 125/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3645.8642 - acc: 0.0000e+00 - val_loss: 3729.3642 - val_acc: 0.0016\n",
            "Epoch 126/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3650.4414 - acc: 2.6874e-04 - val_loss: 3598.3839 - val_acc: 0.0000e+00\n",
            "Epoch 127/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3651.5784 - acc: 0.0000e+00 - val_loss: 3655.8176 - val_acc: 0.0000e+00\n",
            "Epoch 128/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3644.7030 - acc: 2.6874e-04 - val_loss: 3561.3280 - val_acc: 0.0000e+00\n",
            "Epoch 129/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3634.7859 - acc: 0.0000e+00 - val_loss: 3568.2111 - val_acc: 0.0000e+00\n",
            "Epoch 130/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3644.0034 - acc: 0.0000e+00 - val_loss: 3696.6906 - val_acc: 0.0000e+00\n",
            "Epoch 131/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3620.4858 - acc: 0.0000e+00 - val_loss: 3707.9917 - val_acc: 0.0000e+00\n",
            "Epoch 132/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.9074 - acc: 0.0000e+00 - val_loss: 3573.5492 - val_acc: 0.0000e+00\n",
            "Epoch 133/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3652.5036 - acc: 0.0000e+00 - val_loss: 3563.5287 - val_acc: 0.0000e+00\n",
            "Epoch 134/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3628.4501 - acc: 2.6874e-04 - val_loss: 3550.4118 - val_acc: 0.0000e+00\n",
            "Epoch 135/500\n",
            "3721/3721 [==============================] - 6s 2ms/step - loss: 3628.9201 - acc: 0.0000e+00 - val_loss: 3550.4126 - val_acc: 0.0000e+00\n",
            "Epoch 136/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3646.9093 - acc: 2.6874e-04 - val_loss: 3600.4656 - val_acc: 0.0000e+00\n",
            "Epoch 137/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3652.0340 - acc: 0.0000e+00 - val_loss: 3868.1112 - val_acc: 0.0000e+00\n",
            "Epoch 138/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3651.2686 - acc: 2.6874e-04 - val_loss: 3628.9805 - val_acc: 0.0000e+00\n",
            "Epoch 139/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3636.5655 - acc: 2.6874e-04 - val_loss: 4204.2127 - val_acc: 0.0000e+00\n",
            "Epoch 140/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3640.5341 - acc: 0.0000e+00 - val_loss: 3556.7363 - val_acc: 0.0000e+00\n",
            "Epoch 141/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3635.4135 - acc: 0.0000e+00 - val_loss: 3601.3076 - val_acc: 0.0000e+00\n",
            "Epoch 142/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.8542 - acc: 0.0000e+00 - val_loss: 3613.2392 - val_acc: 0.0000e+00\n",
            "Epoch 143/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3632.3740 - acc: 0.0000e+00 - val_loss: 3556.5917 - val_acc: 0.0000e+00\n",
            "Epoch 144/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3642.2638 - acc: 0.0000e+00 - val_loss: 3732.0311 - val_acc: 0.0000e+00\n",
            "Epoch 145/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3662.9875 - acc: 0.0000e+00 - val_loss: 3747.0867 - val_acc: 0.0000e+00\n",
            "Epoch 146/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3640.0592 - acc: 2.6874e-04 - val_loss: 3565.8659 - val_acc: 0.0000e+00\n",
            "Epoch 147/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3639.6965 - acc: 0.0000e+00 - val_loss: 3550.5342 - val_acc: 0.0000e+00\n",
            "Epoch 148/500\n",
            "3721/3721 [==============================] - 6s 2ms/step - loss: 3649.3057 - acc: 0.0000e+00 - val_loss: 3775.4421 - val_acc: 0.0000e+00\n",
            "Epoch 149/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.8110 - acc: 0.0000e+00 - val_loss: 3555.7308 - val_acc: 0.0000e+00\n",
            "Epoch 150/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3648.0676 - acc: 0.0000e+00 - val_loss: 3627.6365 - val_acc: 0.0000e+00\n",
            "Epoch 151/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3639.5810 - acc: 0.0000e+00 - val_loss: 4124.3218 - val_acc: 0.0000e+00\n",
            "Epoch 152/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3641.3607 - acc: 2.6874e-04 - val_loss: 3983.4886 - val_acc: 0.0011\n",
            "Epoch 153/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3635.2910 - acc: 0.0000e+00 - val_loss: 4034.8913 - val_acc: 0.0000e+00\n",
            "Epoch 154/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3630.7055 - acc: 0.0000e+00 - val_loss: 3612.0869 - val_acc: 0.0000e+00\n",
            "Epoch 155/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3650.3304 - acc: 0.0000e+00 - val_loss: 3633.1804 - val_acc: 0.0000e+00\n",
            "Epoch 156/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3656.9572 - acc: 0.0000e+00 - val_loss: 3550.2517 - val_acc: 0.0000e+00\n",
            "Epoch 157/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3628.5945 - acc: 2.6874e-04 - val_loss: 3557.8953 - val_acc: 0.0000e+00\n",
            "Epoch 158/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3632.5136 - acc: 0.0000e+00 - val_loss: 3628.1129 - val_acc: 0.0000e+00\n",
            "Epoch 159/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3634.2622 - acc: 2.6874e-04 - val_loss: 3562.5256 - val_acc: 0.0000e+00\n",
            "Epoch 160/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3644.6841 - acc: 2.6874e-04 - val_loss: 3657.4145 - val_acc: 0.0000e+00\n",
            "Epoch 161/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3643.0154 - acc: 0.0000e+00 - val_loss: 3572.1043 - val_acc: 0.0000e+00\n",
            "Epoch 162/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3638.0341 - acc: 2.6874e-04 - val_loss: 3572.2437 - val_acc: 0.0000e+00\n",
            "Epoch 163/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3648.6988 - acc: 0.0000e+00 - val_loss: 3650.1783 - val_acc: 0.0000e+00\n",
            "Epoch 164/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3648.1269 - acc: 0.0000e+00 - val_loss: 3590.3161 - val_acc: 0.0000e+00\n",
            "Epoch 165/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3657.2915 - acc: 0.0000e+00 - val_loss: 3657.4695 - val_acc: 0.0000e+00\n",
            "Epoch 166/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3641.6074 - acc: 0.0000e+00 - val_loss: 3657.9583 - val_acc: 0.0000e+00\n",
            "Epoch 167/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3655.9665 - acc: 0.0000e+00 - val_loss: 3779.3857 - val_acc: 0.0000e+00\n",
            "Epoch 168/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3652.5077 - acc: 2.6874e-04 - val_loss: 3552.1033 - val_acc: 0.0000e+00\n",
            "Epoch 169/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.5787 - acc: 0.0000e+00 - val_loss: 3553.5951 - val_acc: 0.0000e+00\n",
            "Epoch 170/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3645.5324 - acc: 0.0000e+00 - val_loss: 3564.5498 - val_acc: 0.0000e+00\n",
            "Epoch 171/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3643.3403 - acc: 0.0000e+00 - val_loss: 3607.3499 - val_acc: 0.0000e+00\n",
            "Epoch 172/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3617.0693 - acc: 0.0000e+00 - val_loss: 3730.8885 - val_acc: 0.0000e+00\n",
            "Epoch 173/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3633.0778 - acc: 0.0000e+00 - val_loss: 3835.1010 - val_acc: 0.0000e+00\n",
            "Epoch 174/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3640.5676 - acc: 0.0000e+00 - val_loss: 3550.3168 - val_acc: 0.0000e+00\n",
            "Epoch 175/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.8623 - acc: 0.0000e+00 - val_loss: 3551.2831 - val_acc: 0.0000e+00\n",
            "Epoch 176/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3634.5781 - acc: 0.0000e+00 - val_loss: 3789.3999 - val_acc: 0.0000e+00\n",
            "Epoch 177/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3623.3378 - acc: 0.0000e+00 - val_loss: 3598.8921 - val_acc: 0.0000e+00\n",
            "Epoch 178/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3640.1260 - acc: 0.0000e+00 - val_loss: 3582.7064 - val_acc: 0.0000e+00\n",
            "Epoch 179/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.6295 - acc: 0.0000e+00 - val_loss: 3569.5700 - val_acc: 0.0000e+00\n",
            "Epoch 180/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3637.8952 - acc: 0.0000e+00 - val_loss: 3572.5829 - val_acc: 0.0000e+00\n",
            "Epoch 181/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3635.9947 - acc: 0.0000e+00 - val_loss: 3562.5216 - val_acc: 0.0000e+00\n",
            "Epoch 182/500\n",
            "3721/3721 [==============================] - 6s 2ms/step - loss: 3623.8979 - acc: 0.0000e+00 - val_loss: 3784.4615 - val_acc: 0.0000e+00\n",
            "Epoch 183/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3621.2790 - acc: 0.0000e+00 - val_loss: 3817.4048 - val_acc: 0.0000e+00\n",
            "Epoch 184/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3620.4077 - acc: 0.0000e+00 - val_loss: 3555.9688 - val_acc: 0.0000e+00\n",
            "Epoch 185/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3633.8185 - acc: 0.0000e+00 - val_loss: 3584.4780 - val_acc: 0.0000e+00\n",
            "Epoch 186/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3640.1390 - acc: 5.3749e-04 - val_loss: 3568.0547 - val_acc: 0.0000e+00\n",
            "Epoch 187/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3633.9026 - acc: 0.0000e+00 - val_loss: 3641.6373 - val_acc: 0.0000e+00\n",
            "Epoch 188/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3658.9030 - acc: 0.0000e+00 - val_loss: 3617.4999 - val_acc: 0.0000e+00\n",
            "Epoch 189/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3631.3572 - acc: 0.0000e+00 - val_loss: 3604.0945 - val_acc: 0.0000e+00\n",
            "Epoch 190/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3638.4883 - acc: 0.0000e+00 - val_loss: 3593.0428 - val_acc: 0.0000e+00\n",
            "Epoch 191/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3629.3479 - acc: 0.0000e+00 - val_loss: 3631.7279 - val_acc: 0.0000e+00\n",
            "Epoch 192/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.1674 - acc: 0.0000e+00 - val_loss: 3555.6951 - val_acc: 0.0000e+00\n",
            "Epoch 193/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.7427 - acc: 0.0000e+00 - val_loss: 3560.9098 - val_acc: 0.0000e+00\n",
            "Epoch 194/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3642.6188 - acc: 0.0000e+00 - val_loss: 3560.4059 - val_acc: 0.0000e+00\n",
            "Epoch 195/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3642.9789 - acc: 0.0000e+00 - val_loss: 3561.5737 - val_acc: 0.0000e+00\n",
            "Epoch 196/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3633.6613 - acc: 0.0000e+00 - val_loss: 3583.3336 - val_acc: 0.0000e+00\n",
            "Epoch 197/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.5479 - acc: 2.6874e-04 - val_loss: 3551.9688 - val_acc: 0.0000e+00\n",
            "Epoch 198/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3637.1693 - acc: 0.0000e+00 - val_loss: 3562.1345 - val_acc: 0.0000e+00\n",
            "Epoch 199/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3627.0577 - acc: 0.0000e+00 - val_loss: 3563.9736 - val_acc: 0.0000e+00\n",
            "Epoch 200/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3632.9440 - acc: 0.0000e+00 - val_loss: 3562.6344 - val_acc: 0.0000e+00\n",
            "Epoch 201/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3618.6885 - acc: 0.0000e+00 - val_loss: 3735.1473 - val_acc: 0.0011\n",
            "Epoch 202/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3613.6918 - acc: 0.0000e+00 - val_loss: 3722.2426 - val_acc: 0.0000e+00\n",
            "Epoch 203/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3644.5747 - acc: 0.0000e+00 - val_loss: 3636.6676 - val_acc: 0.0000e+00\n",
            "Epoch 204/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.6806 - acc: 0.0000e+00 - val_loss: 3569.0687 - val_acc: 0.0000e+00\n",
            "Epoch 205/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3636.5494 - acc: 0.0000e+00 - val_loss: 3673.6529 - val_acc: 0.0000e+00\n",
            "Epoch 206/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3636.3439 - acc: 0.0000e+00 - val_loss: 3619.5139 - val_acc: 0.0000e+00\n",
            "Epoch 207/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3639.3192 - acc: 0.0000e+00 - val_loss: 3554.3715 - val_acc: 0.0000e+00\n",
            "Epoch 208/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3636.7536 - acc: 0.0000e+00 - val_loss: 3592.7562 - val_acc: 0.0000e+00\n",
            "Epoch 209/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3634.0823 - acc: 0.0000e+00 - val_loss: 3892.8753 - val_acc: 0.0000e+00\n",
            "Epoch 210/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3641.9576 - acc: 0.0000e+00 - val_loss: 3699.4443 - val_acc: 0.0000e+00\n",
            "Epoch 211/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3628.0099 - acc: 0.0000e+00 - val_loss: 3574.7088 - val_acc: 0.0000e+00\n",
            "Epoch 212/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3642.3234 - acc: 0.0000e+00 - val_loss: 3590.1729 - val_acc: 0.0000e+00\n",
            "Epoch 213/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3640.1462 - acc: 0.0000e+00 - val_loss: 3601.0793 - val_acc: 0.0000e+00\n",
            "Epoch 214/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3638.6423 - acc: 0.0000e+00 - val_loss: 3575.8910 - val_acc: 0.0000e+00\n",
            "Epoch 215/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3634.7985 - acc: 0.0000e+00 - val_loss: 3559.0546 - val_acc: 0.0000e+00\n",
            "Epoch 216/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3619.2528 - acc: 0.0000e+00 - val_loss: 4033.9438 - val_acc: 0.0000e+00\n",
            "Epoch 217/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3658.9655 - acc: 0.0000e+00 - val_loss: 3554.7501 - val_acc: 0.0000e+00\n",
            "Epoch 218/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3646.5853 - acc: 0.0000e+00 - val_loss: 3574.2790 - val_acc: 0.0000e+00\n",
            "Epoch 219/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3634.4800 - acc: 2.6874e-04 - val_loss: 3602.9218 - val_acc: 0.0000e+00\n",
            "Epoch 220/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3641.6315 - acc: 0.0000e+00 - val_loss: 3556.9945 - val_acc: 0.0000e+00\n",
            "Epoch 221/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3636.2639 - acc: 2.6874e-04 - val_loss: 3757.7070 - val_acc: 0.0000e+00\n",
            "Epoch 222/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3629.5904 - acc: 0.0000e+00 - val_loss: 3673.6614 - val_acc: 0.0000e+00\n",
            "Epoch 223/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3649.5386 - acc: 0.0000e+00 - val_loss: 3581.1371 - val_acc: 5.4526e-04\n",
            "Epoch 224/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3625.3640 - acc: 0.0000e+00 - val_loss: 3685.4291 - val_acc: 0.0000e+00\n",
            "Epoch 225/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3627.2827 - acc: 0.0000e+00 - val_loss: 4083.6587 - val_acc: 0.0000e+00\n",
            "Epoch 226/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3641.3663 - acc: 0.0000e+00 - val_loss: 3809.3364 - val_acc: 0.0000e+00\n",
            "Epoch 227/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3629.8477 - acc: 0.0000e+00 - val_loss: 3703.6245 - val_acc: 0.0000e+00\n",
            "Epoch 228/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3648.7838 - acc: 0.0000e+00 - val_loss: 3570.0401 - val_acc: 0.0000e+00\n",
            "Epoch 229/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3631.7792 - acc: 0.0000e+00 - val_loss: 3559.3622 - val_acc: 0.0000e+00\n",
            "Epoch 230/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3642.5390 - acc: 0.0000e+00 - val_loss: 3550.2087 - val_acc: 0.0000e+00\n",
            "Epoch 231/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3644.7546 - acc: 0.0000e+00 - val_loss: 3712.6922 - val_acc: 0.0000e+00\n",
            "Epoch 232/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3630.7196 - acc: 0.0000e+00 - val_loss: 3580.5414 - val_acc: 0.0000e+00\n",
            "Epoch 233/500\n",
            "3721/3721 [==============================] - 6s 2ms/step - loss: 3629.6581 - acc: 0.0000e+00 - val_loss: 3615.5846 - val_acc: 0.0000e+00\n",
            "Epoch 234/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.6351 - acc: 0.0000e+00 - val_loss: 3642.0775 - val_acc: 0.0000e+00\n",
            "Epoch 235/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3636.2104 - acc: 0.0000e+00 - val_loss: 3651.3443 - val_acc: 0.0000e+00\n",
            "Epoch 236/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3638.1950 - acc: 0.0000e+00 - val_loss: 3671.1255 - val_acc: 0.0000e+00\n",
            "Epoch 237/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3633.2319 - acc: 0.0000e+00 - val_loss: 3599.7964 - val_acc: 0.0000e+00\n",
            "Epoch 238/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3640.3553 - acc: 2.6874e-04 - val_loss: 3597.9541 - val_acc: 0.0000e+00\n",
            "Epoch 239/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3623.5242 - acc: 0.0000e+00 - val_loss: 3693.3138 - val_acc: 0.0000e+00\n",
            "Epoch 240/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3631.8740 - acc: 0.0000e+00 - val_loss: 3563.0238 - val_acc: 0.0000e+00\n",
            "Epoch 241/500\n",
            "3721/3721 [==============================] - 6s 2ms/step - loss: 3635.0923 - acc: 2.6874e-04 - val_loss: 3667.2868 - val_acc: 0.0000e+00\n",
            "Epoch 242/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3647.0632 - acc: 0.0000e+00 - val_loss: 3647.2461 - val_acc: 0.0000e+00\n",
            "Epoch 243/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3623.8521 - acc: 0.0000e+00 - val_loss: 3562.4324 - val_acc: 0.0000e+00\n",
            "Epoch 244/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3634.4992 - acc: 0.0000e+00 - val_loss: 3561.9878 - val_acc: 0.0000e+00\n",
            "Epoch 245/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3638.8803 - acc: 0.0000e+00 - val_loss: 3581.0254 - val_acc: 0.0000e+00\n",
            "Epoch 246/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3630.1687 - acc: 0.0000e+00 - val_loss: 3556.9107 - val_acc: 0.0000e+00\n",
            "Epoch 247/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3631.3009 - acc: 0.0000e+00 - val_loss: 3643.8029 - val_acc: 0.0000e+00\n",
            "Epoch 248/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3620.5599 - acc: 0.0000e+00 - val_loss: 3570.9952 - val_acc: 0.0000e+00\n",
            "Epoch 249/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3635.1249 - acc: 0.0000e+00 - val_loss: 3574.0750 - val_acc: 0.0000e+00\n",
            "Epoch 250/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3627.0773 - acc: 0.0000e+00 - val_loss: 3658.3132 - val_acc: 0.0000e+00\n",
            "Epoch 251/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3632.1402 - acc: 0.0000e+00 - val_loss: 3829.7063 - val_acc: 5.4526e-04\n",
            "Epoch 252/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3638.6028 - acc: 0.0000e+00 - val_loss: 3898.9201 - val_acc: 0.0000e+00\n",
            "Epoch 253/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3629.7963 - acc: 0.0000e+00 - val_loss: 3552.2818 - val_acc: 0.0000e+00\n",
            "Epoch 254/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3639.7083 - acc: 0.0000e+00 - val_loss: 3550.7370 - val_acc: 0.0000e+00\n",
            "Epoch 255/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3628.1259 - acc: 0.0000e+00 - val_loss: 3638.6720 - val_acc: 0.0000e+00\n",
            "Epoch 256/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3642.5065 - acc: 0.0000e+00 - val_loss: 3576.7285 - val_acc: 0.0000e+00\n",
            "Epoch 257/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3645.9684 - acc: 0.0000e+00 - val_loss: 3553.1309 - val_acc: 0.0000e+00\n",
            "Epoch 258/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3639.6844 - acc: 0.0000e+00 - val_loss: 3550.4191 - val_acc: 0.0000e+00\n",
            "Epoch 259/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3644.1765 - acc: 0.0000e+00 - val_loss: 3567.5259 - val_acc: 0.0000e+00\n",
            "Epoch 260/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3640.4161 - acc: 0.0000e+00 - val_loss: 3575.3136 - val_acc: 0.0000e+00\n",
            "Epoch 261/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3645.7279 - acc: 2.6874e-04 - val_loss: 3628.4075 - val_acc: 0.0000e+00\n",
            "Epoch 262/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3637.1233 - acc: 0.0000e+00 - val_loss: 3560.5366 - val_acc: 0.0000e+00\n",
            "Epoch 263/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3644.1255 - acc: 0.0000e+00 - val_loss: 3627.1398 - val_acc: 0.0000e+00\n",
            "Epoch 264/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3625.3105 - acc: 0.0000e+00 - val_loss: 3590.6758 - val_acc: 0.0000e+00\n",
            "Epoch 265/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3628.6763 - acc: 0.0000e+00 - val_loss: 3707.0122 - val_acc: 0.0000e+00\n",
            "Epoch 266/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3628.7409 - acc: 0.0000e+00 - val_loss: 3989.0583 - val_acc: 0.0000e+00\n",
            "Epoch 267/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3616.1402 - acc: 0.0000e+00 - val_loss: 3550.1305 - val_acc: 0.0000e+00\n",
            "Epoch 268/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3624.4878 - acc: 0.0000e+00 - val_loss: 3691.2016 - val_acc: 0.0000e+00\n",
            "Epoch 269/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3638.8911 - acc: 0.0000e+00 - val_loss: 3554.1172 - val_acc: 0.0000e+00\n",
            "Epoch 270/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 3636.7463 - acc: 0.0000e+00 - val_loss: 3566.7324 - val_acc: 0.0000e+00\n",
            "Epoch 271/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 3639.0498 - acc: 5.3749e-04 - val_loss: 3620.5457 - val_acc: 0.0000e+00\n",
            "Epoch 272/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 2789.3618 - acc: 0.0000e+00 - val_loss: 1673.9383 - val_acc: 5.4526e-04\n",
            "Epoch 273/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 1350.5819 - acc: 0.0000e+00 - val_loss: 1182.7407 - val_acc: 0.0000e+00\n",
            "Epoch 274/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 1076.6587 - acc: 2.6874e-04 - val_loss: 1048.9195 - val_acc: 0.0000e+00\n",
            "Epoch 275/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 999.5063 - acc: 0.0000e+00 - val_loss: 938.0049 - val_acc: 0.0000e+00\n",
            "Epoch 276/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 973.7033 - acc: 0.0000e+00 - val_loss: 925.8554 - val_acc: 5.4526e-04\n",
            "Epoch 277/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 948.5750 - acc: 0.0000e+00 - val_loss: 897.5145 - val_acc: 0.0000e+00\n",
            "Epoch 278/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 948.0557 - acc: 5.3749e-04 - val_loss: 893.0252 - val_acc: 0.0000e+00\n",
            "Epoch 279/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 933.6000 - acc: 2.6874e-04 - val_loss: 880.4299 - val_acc: 0.0000e+00\n",
            "Epoch 280/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 930.6635 - acc: 0.0000e+00 - val_loss: 891.6881 - val_acc: 0.0000e+00\n",
            "Epoch 281/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 925.0807 - acc: 2.6874e-04 - val_loss: 1103.9124 - val_acc: 0.0000e+00\n",
            "Epoch 282/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 932.5194 - acc: 0.0000e+00 - val_loss: 887.2422 - val_acc: 0.0011\n",
            "Epoch 283/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 929.1537 - acc: 2.6874e-04 - val_loss: 880.3017 - val_acc: 0.0000e+00\n",
            "Epoch 284/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 924.0874 - acc: 0.0000e+00 - val_loss: 1106.1827 - val_acc: 0.0000e+00\n",
            "Epoch 285/500\n",
            "3721/3721 [==============================] - 8s 2ms/step - loss: 937.9671 - acc: 0.0000e+00 - val_loss: 878.6141 - val_acc: 0.0000e+00\n",
            "Epoch 286/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 928.9959 - acc: 0.0000e+00 - val_loss: 961.0581 - val_acc: 0.0000e+00\n",
            "Epoch 287/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 925.9408 - acc: 5.3749e-04 - val_loss: 1005.3791 - val_acc: 0.0000e+00\n",
            "Epoch 288/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 934.6856 - acc: 0.0000e+00 - val_loss: 1069.9118 - val_acc: 0.0000e+00\n",
            "Epoch 289/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 930.5571 - acc: 0.0000e+00 - val_loss: 1172.7236 - val_acc: 0.0000e+00\n",
            "Epoch 290/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 934.6648 - acc: 2.6874e-04 - val_loss: 893.8402 - val_acc: 0.0000e+00\n",
            "Epoch 291/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 935.9189 - acc: 0.0000e+00 - val_loss: 886.6369 - val_acc: 5.4526e-04\n",
            "Epoch 292/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 925.5968 - acc: 0.0000e+00 - val_loss: 875.1967 - val_acc: 5.4526e-04\n",
            "Epoch 293/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 929.3537 - acc: 2.6874e-04 - val_loss: 921.7059 - val_acc: 0.0000e+00\n",
            "Epoch 294/500\n",
            "3721/3721 [==============================] - 7s 2ms/step - loss: 927.1802 - acc: 2.6874e-04 - val_loss: 880.9237 - val_acc: 0.0000e+00\n",
            "Epoch 295/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 930.2877 - acc: 0.0000e+00 - val_loss: 910.9177 - val_acc: 5.4526e-04\n",
            "Epoch 296/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.2954 - acc: 0.0000e+00 - val_loss: 1141.1023 - val_acc: 0.0000e+00\n",
            "Epoch 297/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.3820 - acc: 2.6874e-04 - val_loss: 900.9517 - val_acc: 5.4526e-04\n",
            "Epoch 298/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.7780 - acc: 0.0000e+00 - val_loss: 1073.5934 - val_acc: 5.4526e-04\n",
            "Epoch 299/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 929.2178 - acc: 2.6874e-04 - val_loss: 922.3166 - val_acc: 0.0000e+00\n",
            "Epoch 300/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.1185 - acc: 0.0000e+00 - val_loss: 886.0981 - val_acc: 5.4526e-04\n",
            "Epoch 301/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.4599 - acc: 0.0000e+00 - val_loss: 877.2964 - val_acc: 0.0000e+00\n",
            "Epoch 302/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.1207 - acc: 0.0000e+00 - val_loss: 901.0362 - val_acc: 0.0011\n",
            "Epoch 303/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 936.1431 - acc: 2.6874e-04 - val_loss: 895.8565 - val_acc: 0.0000e+00\n",
            "Epoch 304/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 930.7366 - acc: 5.3749e-04 - val_loss: 890.0034 - val_acc: 0.0000e+00\n",
            "Epoch 305/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 926.3175 - acc: 0.0000e+00 - val_loss: 890.2914 - val_acc: 5.4526e-04\n",
            "Epoch 306/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 934.4434 - acc: 2.6874e-04 - val_loss: 904.7139 - val_acc: 0.0000e+00\n",
            "Epoch 307/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.7759 - acc: 0.0000e+00 - val_loss: 985.5536 - val_acc: 0.0000e+00\n",
            "Epoch 308/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.4904 - acc: 0.0000e+00 - val_loss: 920.9204 - val_acc: 5.4526e-04\n",
            "Epoch 309/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 932.6973 - acc: 2.6874e-04 - val_loss: 1029.9846 - val_acc: 0.0000e+00\n",
            "Epoch 310/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 923.1350 - acc: 0.0000e+00 - val_loss: 1123.7093 - val_acc: 0.0000e+00\n",
            "Epoch 311/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 930.0177 - acc: 0.0000e+00 - val_loss: 1030.0602 - val_acc: 0.0000e+00\n",
            "Epoch 312/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 931.6779 - acc: 0.0000e+00 - val_loss: 901.7877 - val_acc: 0.0000e+00\n",
            "Epoch 313/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 931.0980 - acc: 0.0000e+00 - val_loss: 877.6248 - val_acc: 0.0000e+00\n",
            "Epoch 314/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.7606 - acc: 2.6874e-04 - val_loss: 879.4637 - val_acc: 0.0000e+00\n",
            "Epoch 315/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 932.3771 - acc: 0.0000e+00 - val_loss: 926.5226 - val_acc: 0.0000e+00\n",
            "Epoch 316/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.8121 - acc: 2.6874e-04 - val_loss: 1041.6231 - val_acc: 0.0000e+00\n",
            "Epoch 317/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 920.5110 - acc: 0.0000e+00 - val_loss: 883.7014 - val_acc: 0.0000e+00\n",
            "Epoch 318/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.4201 - acc: 0.0000e+00 - val_loss: 917.5000 - val_acc: 0.0000e+00\n",
            "Epoch 319/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.6662 - acc: 0.0000e+00 - val_loss: 893.1352 - val_acc: 0.0000e+00\n",
            "Epoch 320/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.5530 - acc: 0.0000e+00 - val_loss: 886.1204 - val_acc: 0.0000e+00\n",
            "Epoch 321/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.3205 - acc: 0.0000e+00 - val_loss: 884.0379 - val_acc: 0.0011\n",
            "Epoch 322/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.6948 - acc: 2.6874e-04 - val_loss: 890.4624 - val_acc: 0.0000e+00\n",
            "Epoch 323/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.8099 - acc: 0.0000e+00 - val_loss: 873.5356 - val_acc: 0.0000e+00\n",
            "Epoch 324/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.4751 - acc: 0.0000e+00 - val_loss: 921.8559 - val_acc: 0.0000e+00\n",
            "Epoch 325/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.2398 - acc: 0.0000e+00 - val_loss: 895.9866 - val_acc: 0.0000e+00\n",
            "Epoch 326/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 931.4220 - acc: 0.0000e+00 - val_loss: 877.6232 - val_acc: 0.0000e+00\n",
            "Epoch 327/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 929.2082 - acc: 0.0000e+00 - val_loss: 896.9984 - val_acc: 0.0000e+00\n",
            "Epoch 328/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 933.3428 - acc: 2.6874e-04 - val_loss: 877.4014 - val_acc: 0.0000e+00\n",
            "Epoch 329/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 931.4969 - acc: 0.0000e+00 - val_loss: 873.1392 - val_acc: 0.0000e+00\n",
            "Epoch 330/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 939.8772 - acc: 0.0000e+00 - val_loss: 918.8788 - val_acc: 0.0000e+00\n",
            "Epoch 331/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 931.0856 - acc: 2.6874e-04 - val_loss: 904.8787 - val_acc: 0.0000e+00\n",
            "Epoch 332/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.7592 - acc: 0.0000e+00 - val_loss: 900.1936 - val_acc: 0.0011\n",
            "Epoch 333/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.9504 - acc: 0.0000e+00 - val_loss: 881.2702 - val_acc: 0.0000e+00\n",
            "Epoch 334/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 934.9271 - acc: 0.0000e+00 - val_loss: 910.9341 - val_acc: 0.0000e+00\n",
            "Epoch 335/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.2142 - acc: 0.0000e+00 - val_loss: 891.2679 - val_acc: 5.4526e-04\n",
            "Epoch 336/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.0880 - acc: 2.6874e-04 - val_loss: 906.1487 - val_acc: 0.0000e+00\n",
            "Epoch 337/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.4933 - acc: 2.6874e-04 - val_loss: 1008.8447 - val_acc: 0.0000e+00\n",
            "Epoch 338/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.6150 - acc: 0.0000e+00 - val_loss: 937.2749 - val_acc: 0.0000e+00\n",
            "Epoch 339/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 915.9824 - acc: 0.0000e+00 - val_loss: 893.4026 - val_acc: 0.0000e+00\n",
            "Epoch 340/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.5054 - acc: 2.6874e-04 - val_loss: 883.3697 - val_acc: 0.0000e+00\n",
            "Epoch 341/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.3098 - acc: 2.6874e-04 - val_loss: 890.7863 - val_acc: 0.0000e+00\n",
            "Epoch 342/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.1001 - acc: 2.6874e-04 - val_loss: 879.0528 - val_acc: 0.0000e+00\n",
            "Epoch 343/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.1082 - acc: 5.3749e-04 - val_loss: 873.2003 - val_acc: 0.0000e+00\n",
            "Epoch 344/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 931.9528 - acc: 2.6874e-04 - val_loss: 912.7637 - val_acc: 5.4526e-04\n",
            "Epoch 345/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 916.1220 - acc: 2.6874e-04 - val_loss: 885.6035 - val_acc: 0.0000e+00\n",
            "Epoch 346/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.2017 - acc: 0.0000e+00 - val_loss: 887.5933 - val_acc: 0.0000e+00\n",
            "Epoch 347/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.0020 - acc: 0.0000e+00 - val_loss: 983.0364 - val_acc: 0.0000e+00\n",
            "Epoch 348/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.5463 - acc: 0.0000e+00 - val_loss: 898.1682 - val_acc: 0.0000e+00\n",
            "Epoch 349/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 918.6279 - acc: 2.6874e-04 - val_loss: 1224.8774 - val_acc: 0.0000e+00\n",
            "Epoch 350/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 932.4290 - acc: 2.6874e-04 - val_loss: 923.3577 - val_acc: 0.0000e+00\n",
            "Epoch 351/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.7450 - acc: 0.0000e+00 - val_loss: 920.7312 - val_acc: 0.0000e+00\n",
            "Epoch 352/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.0820 - acc: 0.0000e+00 - val_loss: 1003.2307 - val_acc: 0.0000e+00\n",
            "Epoch 353/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 932.1593 - acc: 0.0000e+00 - val_loss: 884.4236 - val_acc: 0.0000e+00\n",
            "Epoch 354/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.4012 - acc: 2.6874e-04 - val_loss: 873.1804 - val_acc: 0.0000e+00\n",
            "Epoch 355/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.8307 - acc: 0.0000e+00 - val_loss: 882.4445 - val_acc: 0.0000e+00\n",
            "Epoch 356/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.2149 - acc: 0.0000e+00 - val_loss: 929.9990 - val_acc: 0.0000e+00\n",
            "Epoch 357/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.3731 - acc: 2.6874e-04 - val_loss: 940.1240 - val_acc: 0.0000e+00\n",
            "Epoch 358/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.9114 - acc: 2.6874e-04 - val_loss: 888.7262 - val_acc: 0.0000e+00\n",
            "Epoch 359/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 932.4347 - acc: 0.0000e+00 - val_loss: 873.8394 - val_acc: 0.0000e+00\n",
            "Epoch 360/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 935.1244 - acc: 0.0000e+00 - val_loss: 901.0274 - val_acc: 0.0000e+00\n",
            "Epoch 361/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 920.3649 - acc: 0.0000e+00 - val_loss: 889.9497 - val_acc: 0.0000e+00\n",
            "Epoch 362/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.1007 - acc: 0.0000e+00 - val_loss: 991.1887 - val_acc: 0.0000e+00\n",
            "Epoch 363/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 933.5877 - acc: 2.6874e-04 - val_loss: 889.5941 - val_acc: 0.0000e+00\n",
            "Epoch 364/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 929.3767 - acc: 0.0000e+00 - val_loss: 964.6366 - val_acc: 5.4526e-04\n",
            "Epoch 365/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.7053 - acc: 0.0000e+00 - val_loss: 874.2417 - val_acc: 0.0000e+00\n",
            "Epoch 366/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.5899 - acc: 2.6874e-04 - val_loss: 1108.5064 - val_acc: 0.0000e+00\n",
            "Epoch 367/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.0465 - acc: 2.6874e-04 - val_loss: 939.9748 - val_acc: 0.0000e+00\n",
            "Epoch 368/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.8125 - acc: 2.6874e-04 - val_loss: 902.0934 - val_acc: 0.0000e+00\n",
            "Epoch 369/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.2020 - acc: 0.0000e+00 - val_loss: 1043.8375 - val_acc: 0.0016\n",
            "Epoch 370/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.1042 - acc: 2.6874e-04 - val_loss: 886.7376 - val_acc: 0.0000e+00\n",
            "Epoch 371/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.6711 - acc: 0.0000e+00 - val_loss: 903.8326 - val_acc: 0.0000e+00\n",
            "Epoch 372/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.9082 - acc: 0.0000e+00 - val_loss: 884.1759 - val_acc: 0.0000e+00\n",
            "Epoch 373/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.6171 - acc: 2.6874e-04 - val_loss: 912.7868 - val_acc: 0.0000e+00\n",
            "Epoch 374/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.5945 - acc: 0.0000e+00 - val_loss: 934.2207 - val_acc: 0.0000e+00\n",
            "Epoch 375/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.9994 - acc: 0.0000e+00 - val_loss: 880.7326 - val_acc: 0.0000e+00\n",
            "Epoch 376/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 920.7507 - acc: 0.0000e+00 - val_loss: 875.2078 - val_acc: 0.0000e+00\n",
            "Epoch 377/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 928.5358 - acc: 0.0000e+00 - val_loss: 893.1603 - val_acc: 0.0000e+00\n",
            "Epoch 378/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.1287 - acc: 2.6874e-04 - val_loss: 886.5904 - val_acc: 0.0000e+00\n",
            "Epoch 379/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.9546 - acc: 0.0000e+00 - val_loss: 914.3454 - val_acc: 0.0000e+00\n",
            "Epoch 380/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.5145 - acc: 2.6874e-04 - val_loss: 896.0819 - val_acc: 0.0000e+00\n",
            "Epoch 381/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 918.9288 - acc: 2.6874e-04 - val_loss: 989.3831 - val_acc: 0.0000e+00\n",
            "Epoch 382/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 922.4622 - acc: 2.6874e-04 - val_loss: 883.1873 - val_acc: 5.4526e-04\n",
            "Epoch 383/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.7161 - acc: 0.0000e+00 - val_loss: 1068.9257 - val_acc: 0.0000e+00\n",
            "Epoch 384/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.0791 - acc: 0.0000e+00 - val_loss: 935.0278 - val_acc: 0.0000e+00\n",
            "Epoch 385/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 918.6592 - acc: 2.6874e-04 - val_loss: 893.3468 - val_acc: 0.0000e+00\n",
            "Epoch 386/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.6114 - acc: 0.0000e+00 - val_loss: 897.8774 - val_acc: 0.0000e+00\n",
            "Epoch 387/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.1267 - acc: 0.0000e+00 - val_loss: 981.5229 - val_acc: 0.0000e+00\n",
            "Epoch 388/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.8014 - acc: 0.0000e+00 - val_loss: 881.1955 - val_acc: 0.0000e+00\n",
            "Epoch 389/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 914.7159 - acc: 0.0000e+00 - val_loss: 1001.9338 - val_acc: 0.0000e+00\n",
            "Epoch 390/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 918.8498 - acc: 2.6874e-04 - val_loss: 880.0735 - val_acc: 0.0000e+00\n",
            "Epoch 391/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.5329 - acc: 0.0000e+00 - val_loss: 910.0117 - val_acc: 0.0000e+00\n",
            "Epoch 392/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.0529 - acc: 0.0000e+00 - val_loss: 885.6380 - val_acc: 0.0000e+00\n",
            "Epoch 393/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.9577 - acc: 2.6874e-04 - val_loss: 884.5067 - val_acc: 0.0000e+00\n",
            "Epoch 394/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.0929 - acc: 2.6874e-04 - val_loss: 971.9677 - val_acc: 0.0000e+00\n",
            "Epoch 395/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.7271 - acc: 2.6874e-04 - val_loss: 894.7919 - val_acc: 0.0000e+00\n",
            "Epoch 396/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.5725 - acc: 0.0000e+00 - val_loss: 908.0555 - val_acc: 0.0000e+00\n",
            "Epoch 397/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.0482 - acc: 0.0000e+00 - val_loss: 1021.5048 - val_acc: 0.0011\n",
            "Epoch 398/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.9648 - acc: 2.6874e-04 - val_loss: 1008.7276 - val_acc: 0.0000e+00\n",
            "Epoch 399/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 917.3478 - acc: 0.0000e+00 - val_loss: 943.8291 - val_acc: 0.0000e+00\n",
            "Epoch 400/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.2408 - acc: 2.6874e-04 - val_loss: 956.2528 - val_acc: 0.0000e+00\n",
            "Epoch 401/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.7825 - acc: 0.0000e+00 - val_loss: 904.1726 - val_acc: 0.0000e+00\n",
            "Epoch 402/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 928.7826 - acc: 0.0000e+00 - val_loss: 874.6334 - val_acc: 0.0022\n",
            "Epoch 403/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 928.2906 - acc: 2.6874e-04 - val_loss: 876.7102 - val_acc: 0.0000e+00\n",
            "Epoch 404/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.2668 - acc: 0.0000e+00 - val_loss: 879.6219 - val_acc: 0.0027\n",
            "Epoch 405/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.7602 - acc: 2.6874e-04 - val_loss: 909.7418 - val_acc: 0.0000e+00\n",
            "Epoch 406/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.2027 - acc: 0.0000e+00 - val_loss: 947.6538 - val_acc: 0.0011\n",
            "Epoch 407/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 930.1743 - acc: 5.3749e-04 - val_loss: 880.6969 - val_acc: 0.0000e+00\n",
            "Epoch 408/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.5803 - acc: 2.6874e-04 - val_loss: 881.0922 - val_acc: 5.4526e-04\n",
            "Epoch 409/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.6189 - acc: 0.0000e+00 - val_loss: 903.4051 - val_acc: 0.0000e+00\n",
            "Epoch 410/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.8291 - acc: 2.6874e-04 - val_loss: 882.0228 - val_acc: 0.0000e+00\n",
            "Epoch 411/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.3630 - acc: 0.0000e+00 - val_loss: 889.5716 - val_acc: 0.0000e+00\n",
            "Epoch 412/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.1677 - acc: 2.6874e-04 - val_loss: 889.1793 - val_acc: 5.4526e-04\n",
            "Epoch 413/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 929.7964 - acc: 0.0000e+00 - val_loss: 891.3019 - val_acc: 0.0000e+00\n",
            "Epoch 414/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.3061 - acc: 0.0000e+00 - val_loss: 1038.4930 - val_acc: 0.0000e+00\n",
            "Epoch 415/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 928.0860 - acc: 0.0000e+00 - val_loss: 881.5074 - val_acc: 5.4526e-04\n",
            "Epoch 416/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 928.0038 - acc: 0.0000e+00 - val_loss: 897.9723 - val_acc: 0.0000e+00\n",
            "Epoch 417/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.4639 - acc: 2.6874e-04 - val_loss: 877.2017 - val_acc: 5.4526e-04\n",
            "Epoch 418/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 920.4844 - acc: 2.6874e-04 - val_loss: 902.2566 - val_acc: 0.0000e+00\n",
            "Epoch 419/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.1135 - acc: 5.3749e-04 - val_loss: 878.9628 - val_acc: 0.0000e+00\n",
            "Epoch 420/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.7362 - acc: 0.0000e+00 - val_loss: 936.8382 - val_acc: 0.0000e+00\n",
            "Epoch 421/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.8098 - acc: 0.0000e+00 - val_loss: 882.8719 - val_acc: 0.0000e+00\n",
            "Epoch 422/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.7623 - acc: 8.0623e-04 - val_loss: 884.8178 - val_acc: 0.0000e+00\n",
            "Epoch 423/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.1148 - acc: 0.0000e+00 - val_loss: 888.6654 - val_acc: 0.0000e+00\n",
            "Epoch 424/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 929.9560 - acc: 0.0000e+00 - val_loss: 877.1130 - val_acc: 5.4526e-04\n",
            "Epoch 425/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 918.0930 - acc: 2.6874e-04 - val_loss: 911.1165 - val_acc: 0.0000e+00\n",
            "Epoch 426/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.8728 - acc: 0.0000e+00 - val_loss: 900.2140 - val_acc: 0.0000e+00\n",
            "Epoch 427/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.1387 - acc: 5.3749e-04 - val_loss: 876.4635 - val_acc: 0.0011\n",
            "Epoch 428/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 928.6109 - acc: 2.6874e-04 - val_loss: 874.5012 - val_acc: 5.4526e-04\n",
            "Epoch 429/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.5381 - acc: 2.6874e-04 - val_loss: 941.7559 - val_acc: 0.0000e+00\n",
            "Epoch 430/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.2424 - acc: 2.6874e-04 - val_loss: 946.7356 - val_acc: 0.0000e+00\n",
            "Epoch 431/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 915.5055 - acc: 0.0000e+00 - val_loss: 993.0968 - val_acc: 0.0000e+00\n",
            "Epoch 432/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.1435 - acc: 2.6874e-04 - val_loss: 883.9527 - val_acc: 0.0000e+00\n",
            "Epoch 433/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.6303 - acc: 2.6874e-04 - val_loss: 886.9126 - val_acc: 0.0000e+00\n",
            "Epoch 434/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.4079 - acc: 0.0000e+00 - val_loss: 907.4132 - val_acc: 0.0000e+00\n",
            "Epoch 435/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.9020 - acc: 2.6874e-04 - val_loss: 1000.9772 - val_acc: 0.0011\n",
            "Epoch 436/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 928.7266 - acc: 2.6874e-04 - val_loss: 891.1434 - val_acc: 0.0000e+00\n",
            "Epoch 437/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.9592 - acc: 2.6874e-04 - val_loss: 874.7693 - val_acc: 0.0000e+00\n",
            "Epoch 438/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.8498 - acc: 0.0000e+00 - val_loss: 885.3222 - val_acc: 0.0000e+00\n",
            "Epoch 439/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 914.3007 - acc: 2.6874e-04 - val_loss: 896.1706 - val_acc: 0.0000e+00\n",
            "Epoch 440/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.1128 - acc: 2.6874e-04 - val_loss: 873.9108 - val_acc: 0.0000e+00\n",
            "Epoch 441/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 920.6964 - acc: 0.0000e+00 - val_loss: 876.2488 - val_acc: 0.0000e+00\n",
            "Epoch 442/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.6995 - acc: 2.6874e-04 - val_loss: 879.6979 - val_acc: 0.0000e+00\n",
            "Epoch 443/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.9970 - acc: 0.0000e+00 - val_loss: 879.3102 - val_acc: 0.0000e+00\n",
            "Epoch 444/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.2103 - acc: 2.6874e-04 - val_loss: 937.0027 - val_acc: 0.0000e+00\n",
            "Epoch 445/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.0321 - acc: 0.0000e+00 - val_loss: 877.8008 - val_acc: 0.0000e+00\n",
            "Epoch 446/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 914.7973 - acc: 2.6874e-04 - val_loss: 884.4705 - val_acc: 0.0000e+00\n",
            "Epoch 447/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 920.4306 - acc: 2.6874e-04 - val_loss: 889.9500 - val_acc: 0.0000e+00\n",
            "Epoch 448/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 928.0479 - acc: 0.0000e+00 - val_loss: 876.0043 - val_acc: 0.0000e+00\n",
            "Epoch 449/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 915.9353 - acc: 0.0000e+00 - val_loss: 922.9807 - val_acc: 0.0000e+00\n",
            "Epoch 450/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.8903 - acc: 0.0000e+00 - val_loss: 876.4308 - val_acc: 0.0000e+00\n",
            "Epoch 451/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.9772 - acc: 5.3749e-04 - val_loss: 914.5292 - val_acc: 0.0000e+00\n",
            "Epoch 452/500\n",
            "3721/3721 [==============================] - 5s 1ms/step - loss: 921.9370 - acc: 0.0000e+00 - val_loss: 987.2097 - val_acc: 0.0000e+00\n",
            "Epoch 453/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.6396 - acc: 0.0000e+00 - val_loss: 943.1822 - val_acc: 0.0000e+00\n",
            "Epoch 454/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 916.7403 - acc: 0.0000e+00 - val_loss: 911.6381 - val_acc: 0.0000e+00\n",
            "Epoch 455/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.8213 - acc: 2.6874e-04 - val_loss: 873.1003 - val_acc: 0.0000e+00\n",
            "Epoch 456/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.2879 - acc: 2.6874e-04 - val_loss: 883.6812 - val_acc: 0.0000e+00\n",
            "Epoch 457/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.6634 - acc: 5.3749e-04 - val_loss: 955.0746 - val_acc: 0.0000e+00\n",
            "Epoch 458/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 918.5076 - acc: 2.6874e-04 - val_loss: 937.8824 - val_acc: 0.0000e+00\n",
            "Epoch 459/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.7113 - acc: 0.0000e+00 - val_loss: 987.9764 - val_acc: 0.0000e+00\n",
            "Epoch 460/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 919.8432 - acc: 5.3749e-04 - val_loss: 949.9168 - val_acc: 0.0000e+00\n",
            "Epoch 461/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.4570 - acc: 0.0000e+00 - val_loss: 882.6678 - val_acc: 0.0000e+00\n",
            "Epoch 462/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.4761 - acc: 2.6874e-04 - val_loss: 974.7517 - val_acc: 0.0000e+00\n",
            "Epoch 463/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.0607 - acc: 2.6874e-04 - val_loss: 1089.0699 - val_acc: 0.0000e+00\n",
            "Epoch 464/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.6856 - acc: 0.0000e+00 - val_loss: 880.5858 - val_acc: 0.0000e+00\n",
            "Epoch 465/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 915.9085 - acc: 0.0000e+00 - val_loss: 879.4529 - val_acc: 0.0000e+00\n",
            "Epoch 466/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 907.7112 - acc: 2.6874e-04 - val_loss: 935.5801 - val_acc: 0.0000e+00\n",
            "Epoch 467/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 918.7707 - acc: 0.0000e+00 - val_loss: 911.7899 - val_acc: 0.0000e+00\n",
            "Epoch 468/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.3879 - acc: 2.6874e-04 - val_loss: 889.7120 - val_acc: 0.0000e+00\n",
            "Epoch 469/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.3421 - acc: 0.0000e+00 - val_loss: 897.4558 - val_acc: 0.0000e+00\n",
            "Epoch 470/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 922.2076 - acc: 0.0000e+00 - val_loss: 894.8231 - val_acc: 5.4526e-04\n",
            "Epoch 471/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 919.7907 - acc: 0.0000e+00 - val_loss: 913.5169 - val_acc: 0.0000e+00\n",
            "Epoch 472/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.2075 - acc: 5.3749e-04 - val_loss: 969.7173 - val_acc: 0.0000e+00\n",
            "Epoch 473/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 914.9171 - acc: 0.0000e+00 - val_loss: 903.7804 - val_acc: 0.0000e+00\n",
            "Epoch 474/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 932.0419 - acc: 0.0000e+00 - val_loss: 875.6284 - val_acc: 5.4526e-04\n",
            "Epoch 475/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.9061 - acc: 0.0000e+00 - val_loss: 1105.3947 - val_acc: 0.0000e+00\n",
            "Epoch 476/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.7740 - acc: 0.0000e+00 - val_loss: 891.5021 - val_acc: 0.0000e+00\n",
            "Epoch 477/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 920.9818 - acc: 2.6874e-04 - val_loss: 984.0634 - val_acc: 0.0000e+00\n",
            "Epoch 478/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 918.2961 - acc: 0.0000e+00 - val_loss: 873.8293 - val_acc: 5.4526e-04\n",
            "Epoch 479/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 917.7464 - acc: 0.0000e+00 - val_loss: 918.4049 - val_acc: 0.0000e+00\n",
            "Epoch 480/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.8173 - acc: 2.6874e-04 - val_loss: 1021.8805 - val_acc: 0.0000e+00\n",
            "Epoch 481/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.9505 - acc: 0.0000e+00 - val_loss: 889.4470 - val_acc: 0.0000e+00\n",
            "Epoch 482/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 920.2034 - acc: 0.0000e+00 - val_loss: 988.0100 - val_acc: 0.0000e+00\n",
            "Epoch 483/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 917.6532 - acc: 0.0000e+00 - val_loss: 951.9271 - val_acc: 5.4526e-04\n",
            "Epoch 484/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 928.7542 - acc: 2.6874e-04 - val_loss: 875.7869 - val_acc: 0.0000e+00\n",
            "Epoch 485/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.2790 - acc: 5.3749e-04 - val_loss: 875.7908 - val_acc: 0.0000e+00\n",
            "Epoch 486/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 919.1207 - acc: 2.6874e-04 - val_loss: 970.1104 - val_acc: 0.0000e+00\n",
            "Epoch 487/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.2111 - acc: 0.0000e+00 - val_loss: 891.0932 - val_acc: 0.0000e+00\n",
            "Epoch 488/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 917.7850 - acc: 2.6874e-04 - val_loss: 878.7140 - val_acc: 0.0000e+00\n",
            "Epoch 489/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 923.0730 - acc: 0.0000e+00 - val_loss: 1028.6617 - val_acc: 0.0000e+00\n",
            "Epoch 490/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 917.7817 - acc: 0.0000e+00 - val_loss: 1053.9750 - val_acc: 0.0000e+00\n",
            "Epoch 491/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.2637 - acc: 0.0000e+00 - val_loss: 941.9791 - val_acc: 0.0000e+00\n",
            "Epoch 492/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 925.8778 - acc: 2.6874e-04 - val_loss: 884.2485 - val_acc: 0.0000e+00\n",
            "Epoch 493/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 919.7783 - acc: 0.0000e+00 - val_loss: 878.3345 - val_acc: 0.0000e+00\n",
            "Epoch 494/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.1941 - acc: 0.0000e+00 - val_loss: 934.7342 - val_acc: 0.0000e+00\n",
            "Epoch 495/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 920.1497 - acc: 0.0000e+00 - val_loss: 946.1359 - val_acc: 0.0000e+00\n",
            "Epoch 496/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 926.4323 - acc: 2.6874e-04 - val_loss: 879.8085 - val_acc: 0.0000e+00\n",
            "Epoch 497/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 918.7252 - acc: 0.0000e+00 - val_loss: 920.0897 - val_acc: 0.0000e+00\n",
            "Epoch 498/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 924.7726 - acc: 0.0000e+00 - val_loss: 1016.3869 - val_acc: 0.0000e+00\n",
            "Epoch 499/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 927.4823 - acc: 0.0000e+00 - val_loss: 1064.3802 - val_acc: 0.0000e+00\n",
            "Epoch 500/500\n",
            "3721/3721 [==============================] - 4s 1ms/step - loss: 921.3450 - acc: 0.0000e+00 - val_loss: 878.5214 - val_acc: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWkDqj1dA0iw",
        "colab_type": "code",
        "outputId": "82a2f413-6c0c-478d-c77f-224a83f46a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        }
      },
      "source": [
        "###########################################\n",
        "# Step 4: Model Prediction\n",
        "#############################################\n",
        "out = neuralNetwork.predict(newTestData[:,:2])\n",
        "print(out)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[19466.133 ]\n",
            " [24484.531 ]\n",
            " [25349.21  ]\n",
            " [30796.709 ]\n",
            " [31884.307 ]\n",
            " [33010.312 ]\n",
            " [36146.125 ]\n",
            " [38707.453 ]\n",
            " [37992.32  ]\n",
            " [37290.4   ]\n",
            " [29065.287 ]\n",
            " [34219.46  ]\n",
            " [33587.375 ]\n",
            " [32966.96  ]\n",
            " [32358.008 ]\n",
            " [23371.547 ]\n",
            " [29365.244 ]\n",
            " [29693.717 ]\n",
            " [29145.227 ]\n",
            " [28606.867 ]\n",
            " [28078.453 ]\n",
            " [14911.373 ]\n",
            " [23612.7   ]\n",
            " [26251.438 ]\n",
            " [25766.531 ]\n",
            " [25290.582 ]\n",
            " [24823.402 ]\n",
            " [24364.896 ]\n",
            " [ 7641.6733]\n",
            " [13009.02  ]\n",
            " [22148.426 ]\n",
            " [22779.494 ]\n",
            " [22358.742 ]\n",
            " [21945.74  ]\n",
            " [21540.367 ]\n",
            " [21142.46  ]\n",
            " [ 3916.1538]\n",
            " [ 6666.769 ]\n",
            " [11349.3955]\n",
            " [19127.473 ]\n",
            " [19766.76  ]\n",
            " [19401.637 ]\n",
            " [19043.256 ]\n",
            " [18691.498 ]\n",
            " [18346.236 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsDwXVAABHDF",
        "colab_type": "code",
        "outputId": "95b9a3ec-0cf1-4430-8b9d-950bd08ad26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#################################\n",
        "# Step 5 : Reserve calculation\n",
        "#################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#print(' Test Data',test_data)\n",
        "#print('pred', model.predict([2,]))\n",
        "#print('C',C)\n",
        "true_reserve = 0\n",
        "for i in range(1,np.shape(triangle)[0]):\n",
        "    j = np.shape(triangle)[1]-1-i\n",
        "    #print(i,j)\n",
        "    #print('last known',C[i,j])\n",
        "    #print(' last estimate',C[i,np.shape(triangle)[1]-1])\n",
        "    true_reserve += (C[i,np.shape(triangle)[1]-1] - C[i,j])\n",
        "    #print(true_reserve)\n",
        "print(\" True reserve\",true_reserve)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " True reserve 17352.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ3X4QakBct6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "appended= np.array(list(zip(test_data,np.ravel(out))))\n",
        "len(appended)\n",
        "\n",
        "out_dict = {}\n",
        "for i in range(len(appended)):\n",
        "    #print(tuple(appended[i,0][:2]))\n",
        "    #print(appended[i,1])\n",
        "    out_dict[tuple(appended[i,0][:2])] = appended[i,1]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wQkKmhECMvg",
        "colab_type": "code",
        "outputId": "8635983f-9a91-4871-ede8-51100a7e1ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "appended[0,0][:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 7.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUQKDOqqBkGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_reserve = 0\n",
        "final_pred = []\n",
        "actuals = []\n",
        "for i in range(1,np.shape(triangle)[0]):\n",
        "    j = int(np.shape(triangle)[1]-1-i)    \n",
        "    #print(i,j)\n",
        "    #print('last known',C[i,j])\n",
        "    #print(' last pred', out_dict[(i,np.shape(triangle)[1]-1)])\n",
        "    #print(' last estimate',C[i,np.shape(triangle)[1]-1])\n",
        "    final_pred.append(out_dict[(i,np.shape(triangle)[1]-1)] - C[i,j])\n",
        "    actuals.append(C[i,np.shape(triangle)[1]-1] - C[i,j])\n",
        "    pred_reserve +=(out_dict[(i,np.shape(triangle)[1]-1)] - C[i,j])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsYABGSNBq_7",
        "colab_type": "code",
        "outputId": "660d293b-e625-49ca-d5bc-0f0a646baf7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\" Predicted reserve\",pred_reserve)\n",
        "#print(' Bias',pred_reserve - true_reserve)\n",
        "#print('Bias pct',(pred_reserve-true_reserve)/pred_reserve)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Predicted reserve 97133.109375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bLrRyChBv7-",
        "colab_type": "code",
        "outputId": "fa4d61b6-48de-49b2-d4a5-29684d291552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "#ploting the loss of Neural Network\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#training loss and validation loss\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.legend(['train','validation'],loc='upper right')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiJFo1AREZGi+f9CR7dDvK23NwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIJE88AV9PWH",
        "colab_type": "code",
        "outputId": "d90d6095-0fa9-4932-8889-4927253c5204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "#plot showing predicted value vs actual value\n",
        "plt.figure()\n",
        "plt.plot(actuals, label = \"Test Originals\")\n",
        "plt.plot(final_pred, label =\"Test Predictions\")\n",
        "plt.ylabel(\"Reserve Value\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "infjaZWG-725",
        "colab_type": "code",
        "outputId": "457e7fdf-a076-46fc-8077-946414583b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_data[0,]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.000e+00, 7.000e+00, 4.993e+03])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFXoRgKh_1Np",
        "colab_type": "code",
        "outputId": "ddc456ad-26ec-444d-8ecb-fcb39258c743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5001.1714],\n",
              "       [6076.9854],\n",
              "       [6462.7324],\n",
              "       [6657.912 ],\n",
              "       [7011.3906],\n",
              "       [7118.674 ],\n",
              "       [7211.494 ],\n",
              "       [7344.077 ],\n",
              "       [7456.4434],\n",
              "       [7570.5435],\n",
              "       [7054.5913],\n",
              "       [7628.1304],\n",
              "       [7810.2476],\n",
              "       [7929.7544],\n",
              "       [8051.0894],\n",
              "       [6532.3125],\n",
              "       [7462.17  ],\n",
              "       [8068.845 ],\n",
              "       [8306.009 ],\n",
              "       [8433.101 ],\n",
              "       [8562.146 ],\n",
              "       [5339.2217],\n",
              "       [7020.49  ],\n",
              "       [7893.289 ],\n",
              "       [8535.022 ],\n",
              "       [8833.247 ],\n",
              "       [8968.406 ],\n",
              "       [9144.037 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psT4A56C_5AJ",
        "colab_type": "code",
        "outputId": "3af90e29-c630-461c-f754-c06982a3edbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(test_data)\n",
        "type(out)\n",
        "out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te7QmCDDAVYQ",
        "colab_type": "code",
        "outputId": "1c26a92f-71cf-4bea-9c70-e2b1de728ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "#plot for the predicted and the test lower triangle\n",
        "plt.plot(test_data[:,2],label='Test Value')\n",
        "plt.plot(out,label='Predicted Value')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1e6c1907f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvaL4bpsA59N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}