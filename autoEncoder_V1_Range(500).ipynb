{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoEncoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainikhilrai/Deep-Learning/blob/master/autoEncoder_V1_Range(500).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVBNC-wEkPF5",
        "colab_type": "code",
        "outputId": "ff58cc2a-731c-4794-c59f-a99b9a6eac0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jVnUqk9kYYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OM SRI SAI RAM\n",
        "\n",
        "# Descr: Basic or experiment code to perform loss reserve prediction\n",
        "\n",
        "##############################\n",
        "# Step 1 : Reading Sample Data\n",
        "##############################\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sklearn\n",
        "import os\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
        "np.random.seed(7)\n",
        "random.seed(7)\n",
        "accident_years = np.arange(2005,2013,1)\n",
        "development_years = np.arange(0,8,1)\n",
        "triangle = np.array(([1232,946,520,722,316,165,48,14],\n",
        "                   [1469,1201,708,845, 461,235,56,18],\n",
        "                   [1652,1416,959,954,605,287,69,21],\n",
        "                   [1831,1634,1124,1087,725,314,79,24],\n",
        "                   [2074,1919,1330,1240,756,359,91,28],\n",
        "                   [2434,2263,1661,1540,909,432,109,33],\n",
        "                   [2810,2108,1544,1565,924,439,111,34],\n",
        "                   [3072,2614,1785,1810,1069,508,128,39]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbs79f94kkHR",
        "colab_type": "code",
        "outputId": "7d99ef14-c3fd-4d70-b0e0-9d8e364581c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "print(' Accident Years',accident_years)\n",
        "print(' Developement Years',development_years)\n",
        "print(' Input', triangle)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accident Years [2005 2006 2007 2008 2009 2010 2011 2012]\n",
            " Developement Years [0 1 2 3 4 5 6 7]\n",
            " Input [[1232  946  520  722  316  165   48   14]\n",
            " [1469 1201  708  845  461  235   56   18]\n",
            " [1652 1416  959  954  605  287   69   21]\n",
            " [1831 1634 1124 1087  725  314   79   24]\n",
            " [2074 1919 1330 1240  756  359   91   28]\n",
            " [2434 2263 1661 1540  909  432  109   33]\n",
            " [2810 2108 1544 1565  924  439  111   34]\n",
            " [3072 2614 1785 1810 1069  508  128   39]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5IuQnkDGDVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "newTriangle= np.zeros([8,8])\n",
        "for i in range(triangle.shape[0]):\n",
        "  for j in range(triangle.shape[1]-i):\n",
        "    newTriangle[i,j]= triangle[i,j]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srjb5tnPGleb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ff6b6faf-c93d-494b-f1f5-107364c1d97e"
      },
      "source": [
        "print(newTriangle)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1232.  946.  520.  722.  316.  165.   48.   14.]\n",
            " [1469. 1201.  708.  845.  461.  235.   56.    0.]\n",
            " [1652. 1416.  959.  954.  605.  287.    0.    0.]\n",
            " [1831. 1634. 1124. 1087.  725.    0.    0.    0.]\n",
            " [2074. 1919. 1330. 1240.    0.    0.    0.    0.]\n",
            " [2434. 2263. 1661.    0.    0.    0.    0.    0.]\n",
            " [2810. 2108.    0.    0.    0.    0.    0.    0.]\n",
            " [3072.    0.    0.    0.    0.    0.    0.    0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV9CyhyHkreC",
        "colab_type": "code",
        "outputId": "88ceb489-61df-41fd-9878-5ed670e31361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Cum calc\n",
        "C = np.zeros(shape=(np.shape(triangle)[0],np.shape(triangle)[1]))\n",
        "for i in range(np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(triangle)[1]):\n",
        "        C[i,j] = sum(triangle[i,:j+1])\n",
        "\n",
        "print('C',C)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C [[ 1232.  2178.  2698.  3420.  3736.  3901.  3949.  3963.]\n",
            " [ 1469.  2670.  3378.  4223.  4684.  4919.  4975.  4993.]\n",
            " [ 1652.  3068.  4027.  4981.  5586.  5873.  5942.  5963.]\n",
            " [ 1831.  3465.  4589.  5676.  6401.  6715.  6794.  6818.]\n",
            " [ 2074.  3993.  5323.  6563.  7319.  7678.  7769.  7797.]\n",
            " [ 2434.  4697.  6358.  7898.  8807.  9239.  9348.  9381.]\n",
            " [ 2810.  4918.  6462.  8027.  8951.  9390.  9501.  9535.]\n",
            " [ 3072.  5686.  7471.  9281. 10350. 10858. 10986. 11025.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioSsHmm7kvOY",
        "colab_type": "code",
        "outputId": "b9bec358-ec7a-4592-ca3e-240ba8e0e00c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "l_encode = LabelEncoder()\n",
        "l_encode.fit(accident_years)\n",
        "a_yr = l_encode.transform(accident_years)\n",
        "l_encode.fit(development_years)\n",
        "dev_yr = l_encode.transform(development_years)\n",
        "\n",
        "print(a_yr)\n",
        "print(dev_yr)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7]\n",
            "[0 1 2 3 4 5 6 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyery7QfkzKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = []\n",
        "for i in range(np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(triangle)[1]-i):\n",
        "        train_data.append([a_yr[i],dev_yr[j],C[i,j]])\n",
        "        \n",
        "test_data = []\n",
        "for i in range(1,np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(triangle)[1]-i,np.shape(triangle)[1]):\n",
        "        test_data.append([a_yr[i],dev_yr[j],C[i,j]])\n",
        "\n",
        " \n",
        "#convert trainData and testData into numpyArray\n",
        "#train_data = np.array(train_data)\n",
        "test_data = np.array(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95UOy6VzAcfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code for generating the other triangle\n",
        "\n",
        "for iteratrion in range(100):\n",
        "  newTriangle2= np.zeros((triangle.shape[0],triangle.shape[1]))\n",
        "  for i in range(triangle.shape[0]):\n",
        "    for j in range(triangle.shape[1]-i):\n",
        "        newTriangle2[i,j]= newTriangle[i,j] + np.random.randint(-10,10)\n",
        "  difference= newTriangle2-newTriangle\n",
        "  \n",
        "  #print(\"New Triangle:\\n\",newTriangle)\n",
        "  #print(\"New Triangle2:\\n\",newTriangle2)\n",
        "  #print(\"difference:\\n\",difference)\n",
        "  \n",
        "  \n",
        "  #get the column of the difference\n",
        "  columncount=triangle.shape[1]\n",
        "  for columnIndex in range(triangle.shape[1]):\n",
        "    column= difference[:columncount,columnIndex]\n",
        "    np.random.shuffle(column)\n",
        "    newTriangle[:columncount,columnIndex]= newTriangle[:columncount,columnIndex]+ column\n",
        "    columncount= columncount-1\n",
        "    \n",
        "  #print(\"Bootstrap New Triangle:\\n\")\n",
        "  #print(newTriangle)\n",
        "  \n",
        "  #find the cumulative sum of new Bootstrap New Triangle\n",
        "  # Cum calc\n",
        "  C = np.zeros(shape=(np.shape(triangle)[0],np.shape(triangle)[1]))\n",
        "  for i in range(np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(newTriangle)[1]):\n",
        "        C[i,j] = sum(newTriangle[i,:j+1])\n",
        "\n",
        "  \n",
        "  \n",
        "  #combine the new triangle with the already existing training set\n",
        "  \n",
        "  for i in range(np.shape(triangle)[0]):\n",
        "    for j in range(np.shape(triangle)[1]-i):\n",
        "        train_data.append([a_yr[i],dev_yr[j],C[i,j]])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8GNst3Lzl_C",
        "colab_type": "code",
        "outputId": "2a48102b-9c46-4480-bb67-50766a4b2438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data= np.array(train_data)\n",
        "train_data.shape\n",
        "#train_data[71,:]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3636, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaQ869k1vjuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############\n",
        "# define the loss function\n",
        "################\n",
        "\n",
        "def poisson_dev(y_true, y_pred):\n",
        "    return 2*K.mean(y_pred - y_true -y_true*(K.log(K.clip(y_pred,K.epsilon(),None)) -K.log(K.clip(y_true,K.epsilon(),None))),axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-E5C-Oek8sW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################\n",
        "# Step 3 : Build Model using Neural Networks\n",
        "# Note : MSE loss used here\n",
        "###############################\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras import regularizers\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import RemoteMonitor\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBoVSoy4m0pk",
        "colab_type": "code",
        "outputId": "28da3589-c970-4ce4-a7a9-49d0ad8826e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10559
        }
      },
      "source": [
        "###################\n",
        "# Auto Encoder Network Architecture #\n",
        "######################\n",
        "\n",
        "#encoder part\n",
        "inputData= Input(shape=(2,))\n",
        "encoded= Dense(20,activation='relu',kernel_initializer='normal')(inputData)\n",
        "encoded= Dense(10,activation='relu')(encoded)\n",
        "encoded= Dense(2,activation='relu')(encoded)  #latent space representation\n",
        "\n",
        "#decoder part\n",
        "decoded = Dense(10, activation='relu')(encoded)\n",
        "decoded = Dense(20, activation='relu')(decoded)\n",
        "decoded = Dense(2, activation='relu')(decoded)\n",
        "\n",
        "#combine the encoder and the decoder\n",
        "autoencoder = Model(inputData, decoded)\n",
        "\n",
        "#get only the encoder part\n",
        "encoderPart= Model(inputData,encoded)\n",
        "\n",
        "adam = optimizers.Adam(lr=0.1)\n",
        "autoencoder.compile(loss=\"mse\", optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001)\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=0)\n",
        "filepath=\"v5.best.hdf5\"\n",
        "checkpointer = ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
        "\n",
        "#history = autoencoder.fit(x=train_data[:,:2], y=train_data[:,:2], batch_size=1, epochs=300, verbose=1, callbacks=[checkpointer,reduce_lr,early_stop], validation_split=0.3, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n",
        "history = autoencoder.fit(x=train_data[:,:2], y=train_data[:,:2], batch_size=1, epochs=300, verbose=1, callbacks=None, validation_split=0.3, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 2545 samples, validate on 1091 samples\n",
            "Epoch 1/300\n",
            "2545/2545 [==============================] - 4s 2ms/step - loss: 4.8064 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 2/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6808 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 3/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6812 - acc: 0.5540 - val_loss: 4.6372 - val_acc: 0.5591\n",
            "Epoch 4/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6804 - acc: 0.5540 - val_loss: 4.6393 - val_acc: 0.5591\n",
            "Epoch 5/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6809 - acc: 0.5540 - val_loss: 4.6399 - val_acc: 0.5591\n",
            "Epoch 6/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 7/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 8/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6805 - acc: 0.5540 - val_loss: 4.6376 - val_acc: 0.5591\n",
            "Epoch 9/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6804 - acc: 0.5540 - val_loss: 4.6391 - val_acc: 0.5591\n",
            "Epoch 10/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6805 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 11/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6807 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 12/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 13/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6804 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 14/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6804 - acc: 0.5540 - val_loss: 4.6363 - val_acc: 0.5591\n",
            "Epoch 15/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 16/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6807 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 17/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 18/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 19/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6804 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 20/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 21/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 22/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 23/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 24/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 25/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6804 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 26/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 27/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6376 - val_acc: 0.5591\n",
            "Epoch 28/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 29/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 30/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 31/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 32/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6805 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 33/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 34/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 35/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 36/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 37/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 38/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 39/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 40/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 41/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 42/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6367 - val_acc: 0.5591\n",
            "Epoch 43/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 44/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 45/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 46/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 47/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 48/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6808 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 49/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 50/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 51/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6805 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 52/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 53/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 54/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 55/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6369 - val_acc: 0.5591\n",
            "Epoch 56/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 57/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 58/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 59/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6806 - acc: 0.5540 - val_loss: 4.6398 - val_acc: 0.5591\n",
            "Epoch 60/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 61/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 62/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 63/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 64/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 65/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6364 - val_acc: 0.5591\n",
            "Epoch 66/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 67/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 68/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 69/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 70/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 71/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 72/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 73/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 74/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 75/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 76/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 77/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 78/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 79/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 80/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 81/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 82/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 83/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 84/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 85/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 86/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 87/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 88/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 89/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 90/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 91/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 92/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 93/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 94/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 95/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 96/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 97/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6375 - val_acc: 0.5591\n",
            "Epoch 98/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6363 - val_acc: 0.5591\n",
            "Epoch 99/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 100/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6363 - val_acc: 0.5591\n",
            "Epoch 101/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 102/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 103/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 104/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 105/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 106/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6367 - val_acc: 0.5591\n",
            "Epoch 107/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 108/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 109/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 110/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 111/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 112/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 113/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 114/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 115/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 116/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6365 - val_acc: 0.5591\n",
            "Epoch 117/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 118/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 119/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 120/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6363 - val_acc: 0.5591\n",
            "Epoch 121/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 122/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 123/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 124/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 125/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 126/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 127/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 128/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 129/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 130/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6377 - val_acc: 0.5591\n",
            "Epoch 131/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 132/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 133/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 134/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 135/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 136/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 137/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 138/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 139/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6804 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 140/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 141/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 142/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6367 - val_acc: 0.5591\n",
            "Epoch 143/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 144/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 145/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 146/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 147/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 148/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 149/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 150/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 151/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6364 - val_acc: 0.5591\n",
            "Epoch 152/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 153/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 154/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 155/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 156/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 157/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6804 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 158/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6363 - val_acc: 0.5591\n",
            "Epoch 159/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 160/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 161/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6364 - val_acc: 0.5591\n",
            "Epoch 162/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 163/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 164/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 165/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 166/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 167/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 168/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 169/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 170/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 171/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 172/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 173/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 174/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 175/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 176/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6368 - val_acc: 0.5591\n",
            "Epoch 177/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 178/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 179/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 180/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6368 - val_acc: 0.5591\n",
            "Epoch 181/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 182/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 183/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 184/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 185/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 186/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 187/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 188/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 189/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 190/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 191/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 192/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 193/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 194/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 195/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 196/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 197/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 198/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 199/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 200/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 201/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 202/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 203/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6597 - val_acc: 0.5591\n",
            "Epoch 204/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 205/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 206/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6366 - val_acc: 0.5591\n",
            "Epoch 207/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 208/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 209/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 210/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 211/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 212/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 213/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6364 - val_acc: 0.5591\n",
            "Epoch 214/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 215/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 216/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6363 - val_acc: 0.5591\n",
            "Epoch 217/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 218/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 219/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 220/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 221/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 222/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6415 - val_acc: 0.5591\n",
            "Epoch 223/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 224/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 225/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6387 - val_acc: 0.5591\n",
            "Epoch 226/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 227/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 228/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 229/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 230/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 231/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 232/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 233/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 234/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6363 - val_acc: 0.5591\n",
            "Epoch 235/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 236/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 237/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 238/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6802 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 239/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 240/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 241/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 242/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 243/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6803 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 244/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 245/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 246/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 247/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 248/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 249/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 250/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6807 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 251/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 252/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 253/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 254/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 255/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 256/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 257/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 258/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 259/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 260/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 261/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6380 - val_acc: 0.5591\n",
            "Epoch 262/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6366 - val_acc: 0.5591\n",
            "Epoch 263/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 264/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 265/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 266/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 267/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 268/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6367 - val_acc: 0.5591\n",
            "Epoch 269/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 270/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 271/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 272/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 273/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 274/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 275/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 276/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 277/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6801 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 278/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 279/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 280/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 281/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 282/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 283/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 284/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 285/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 286/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6805 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 287/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 288/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 289/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 290/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6798 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 291/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 292/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 293/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6369 - val_acc: 0.5591\n",
            "Epoch 294/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 295/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 296/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6362 - val_acc: 0.5591\n",
            "Epoch 297/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6800 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 298/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 299/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n",
            "Epoch 300/300\n",
            "2545/2545 [==============================] - 3s 1ms/step - loss: 4.6799 - acc: 0.5540 - val_loss: 4.6361 - val_acc: 0.5591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iiyrefWy-c_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the encoded representation of the train Data and test Data\n",
        "\n",
        "train_data_Encoded= encoderPart.predict(train_data[:,:2])\n",
        "test_data_Encoded= encoderPart.predict(test_data[:,:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnFXGpob30Z2",
        "colab_type": "code",
        "outputId": "4c8388dd-ff2c-43e0-c41f-e7e9b967d817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data[0,2]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1232.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD4krONu36bH",
        "colab_type": "code",
        "outputId": "46d92f2e-e0e9-45d5-b158-b9986c763480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_Encoded[0,1]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDM-0Atc4Bny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#make new train data and test Data\n",
        "newTrainData= []\n",
        "newTestData= []\n",
        "for i in range(train_data.shape[0]):\n",
        "  newTrainData.append([train_data_Encoded[i,0],train_data_Encoded[i,1],train_data[i,2]])\n",
        "  \n",
        "for i in range(test_data.shape[0]):\n",
        "  newTestData.append([test_data_Encoded[i,0],test_data_Encoded[i,1],test_data[i,2]])\n",
        "  \n",
        "newTrainData= np.array(newTrainData)\n",
        "newTestData= np.array(newTestData)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89pr8s-v-mUN",
        "colab_type": "code",
        "outputId": "78678fea-5c59-4fa6-aac3-9d426baf042c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "newTrainData.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3636, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gAJ2TdQ_J60",
        "colab_type": "code",
        "outputId": "0c98b659-11fc-4ab7-b760-ed6216e87e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "newTestData.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svje7_5T_SdK",
        "colab_type": "code",
        "outputId": "6807e5dc-f690-411d-82ea-5f4829f0509f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "newTrainData[0,]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0.,    0., 1232.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsTXAOfz_XiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build the neural Network for prediction\n",
        "\n",
        "neuralNetwork = Sequential()\n",
        "ip_dim = 2\n",
        "#model.add(Dropout(0.1, input_shape=(ip_dim,))\n",
        "neuralNetwork.add(Dense(10, input_dim=ip_dim, kernel_initializer='normal', activation='relu'))\n",
        "neuralNetwork.add(Dense(5, kernel_initializer='normal', activation='relu'))\n",
        "#neuralNetwork.add(Dropout(0.1))\n",
        "neuralNetwork.add(Dense(2, kernel_initializer='normal', activation='relu'))\n",
        "#neuralNetwork.add(Dropout(0.1))\n",
        "#neuralNetwork.add(Dense(10, kernel_initializer='normal', activation='relu'))\n",
        "neuralNetwork.add(Dense(1, kernel_initializer='normal',activation=\"exponential\"))\n",
        "# Compile model\n",
        "neuralNetwork.compile(loss=poisson_dev, optimizer='adam',metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l8ImIMnAPKs",
        "colab_type": "code",
        "outputId": "d04416c0-a31b-472a-a992-9f4db3ca2e19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6727
        }
      },
      "source": [
        "history = neuralNetwork.fit(x=newTrainData[:,:ip_dim], y=newTrainData[:,ip_dim], batch_size=1, epochs=500, verbose=1, callbacks=None, validation_split=0.33, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2436 samples, validate on 1200 samples\n",
            "Epoch 1/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 37927316.9328 - acc: 0.0000e+00 - val_loss: 46996041.7897 - val_acc: 0.0000e+00\n",
            "Epoch 2/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 35573716.0006 - acc: 0.0000e+00 - val_loss: 37853361.8538 - val_acc: 0.0000e+00\n",
            "Epoch 3/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 16823911.2879 - acc: 0.0000e+00 - val_loss: 19403540.2984 - val_acc: 0.0000e+00\n",
            "Epoch 4/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12095596.9749 - acc: 0.0000e+00 - val_loss: 19237485.3545 - val_acc: 0.0000e+00\n",
            "Epoch 5/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090638.6368 - acc: 4.1051e-04 - val_loss: 19253419.7660 - val_acc: 0.0000e+00\n",
            "Epoch 6/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088990.0977 - acc: 0.0012 - val_loss: 19267489.5160 - val_acc: 0.0000e+00\n",
            "Epoch 7/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090714.1401 - acc: 0.0000e+00 - val_loss: 19245498.3822 - val_acc: 0.0000e+00\n",
            "Epoch 8/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12081036.6577 - acc: 0.0000e+00 - val_loss: 19315820.9204 - val_acc: 0.0000e+00\n",
            "Epoch 9/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12083915.8495 - acc: 0.0000e+00 - val_loss: 19299282.1807 - val_acc: 0.0000e+00\n",
            "Epoch 10/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12093940.8814 - acc: 0.0000e+00 - val_loss: 19261383.3765 - val_acc: 0.0000e+00\n",
            "Epoch 11/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12078501.4587 - acc: 8.2102e-04 - val_loss: 19207779.2486 - val_acc: 0.0000e+00\n",
            "Epoch 12/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087889.6177 - acc: 0.0000e+00 - val_loss: 19255481.5247 - val_acc: 0.0000e+00\n",
            "Epoch 13/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088758.1459 - acc: 0.0000e+00 - val_loss: 19242494.6266 - val_acc: 0.0000e+00\n",
            "Epoch 14/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090329.8547 - acc: 0.0000e+00 - val_loss: 19229409.4416 - val_acc: 0.0000e+00\n",
            "Epoch 15/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12081416.8342 - acc: 0.0000e+00 - val_loss: 19283604.7512 - val_acc: 0.0000e+00\n",
            "Epoch 16/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12084410.9714 - acc: 0.0000e+00 - val_loss: 19216561.1520 - val_acc: 0.0000e+00\n",
            "Epoch 17/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12093173.3011 - acc: 0.0000e+00 - val_loss: 19227625.4434 - val_acc: 0.0000e+00\n",
            "Epoch 18/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12083539.0305 - acc: 4.1051e-04 - val_loss: 19309399.6134 - val_acc: 0.0000e+00\n",
            "Epoch 19/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12093373.5962 - acc: 4.1051e-04 - val_loss: 19275428.9147 - val_acc: 0.0000e+00\n",
            "Epoch 20/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12078088.5386 - acc: 0.0000e+00 - val_loss: 19211621.6399 - val_acc: 0.0000e+00\n",
            "Epoch 21/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086013.1098 - acc: 0.0000e+00 - val_loss: 19212087.6725 - val_acc: 0.0000e+00\n",
            "Epoch 22/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090220.1739 - acc: 0.0000e+00 - val_loss: 19232805.1490 - val_acc: 0.0000e+00\n",
            "Epoch 23/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089845.9932 - acc: 0.0000e+00 - val_loss: 19266350.0061 - val_acc: 0.0000e+00\n",
            "Epoch 24/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090338.3532 - acc: 0.0000e+00 - val_loss: 19228122.1229 - val_acc: 0.0000e+00\n",
            "Epoch 25/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12093075.4115 - acc: 0.0000e+00 - val_loss: 19268432.6647 - val_acc: 0.0000e+00\n",
            "Epoch 26/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089841.0937 - acc: 0.0000e+00 - val_loss: 19262080.6560 - val_acc: 0.0000e+00\n",
            "Epoch 27/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090081.0024 - acc: 0.0000e+00 - val_loss: 19253800.7871 - val_acc: 0.0000e+00\n",
            "Epoch 28/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088630.8606 - acc: 0.0000e+00 - val_loss: 19245557.1962 - val_acc: 0.0000e+00\n",
            "Epoch 29/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088425.2706 - acc: 0.0000e+00 - val_loss: 19232174.5226 - val_acc: 0.0000e+00\n",
            "Epoch 30/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12083299.5512 - acc: 4.1051e-04 - val_loss: 19300043.9525 - val_acc: 0.0000e+00\n",
            "Epoch 31/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089362.2529 - acc: 0.0000e+00 - val_loss: 19285595.3886 - val_acc: 0.0000e+00\n",
            "Epoch 32/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088253.5055 - acc: 0.0000e+00 - val_loss: 19275827.2582 - val_acc: 0.0000e+00\n",
            "Epoch 33/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089807.1648 - acc: 0.0000e+00 - val_loss: 19263803.3086 - val_acc: 0.0000e+00\n",
            "Epoch 34/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088424.8240 - acc: 4.1051e-04 - val_loss: 19252557.2155 - val_acc: 0.0000e+00\n",
            "Epoch 35/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087791.6684 - acc: 0.0000e+00 - val_loss: 19221139.6213 - val_acc: 0.0000e+00\n",
            "Epoch 36/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12081341.1793 - acc: 8.2102e-04 - val_loss: 19207772.3330 - val_acc: 0.0000e+00\n",
            "Epoch 37/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088288.1159 - acc: 0.0000e+00 - val_loss: 19278368.6451 - val_acc: 0.0000e+00\n",
            "Epoch 38/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092946.3843 - acc: 0.0000e+00 - val_loss: 19237729.0855 - val_acc: 0.0000e+00\n",
            "Epoch 39/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090597.8885 - acc: 0.0000e+00 - val_loss: 19229300.2879 - val_acc: 0.0000e+00\n",
            "Epoch 40/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12084964.0947 - acc: 0.0000e+00 - val_loss: 19213462.8869 - val_acc: 0.0000e+00\n",
            "Epoch 41/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090901.8459 - acc: 0.0000e+00 - val_loss: 19263417.4421 - val_acc: 0.0000e+00\n",
            "Epoch 42/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090501.8194 - acc: 4.1051e-04 - val_loss: 19233756.7881 - val_acc: 0.0000e+00\n",
            "Epoch 43/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086912.5246 - acc: 4.1051e-04 - val_loss: 19273881.7606 - val_acc: 0.0000e+00\n",
            "Epoch 44/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12085101.8921 - acc: 0.0000e+00 - val_loss: 19256943.8926 - val_acc: 0.0000e+00\n",
            "Epoch 45/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12084384.9477 - acc: 0.0000e+00 - val_loss: 19284925.7779 - val_acc: 0.0000e+00\n",
            "Epoch 46/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089958.3732 - acc: 4.1051e-04 - val_loss: 19241639.0115 - val_acc: 0.0000e+00\n",
            "Epoch 47/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090255.3640 - acc: 0.0000e+00 - val_loss: 19274978.2504 - val_acc: 0.0000e+00\n",
            "Epoch 48/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090940.6991 - acc: 0.0000e+00 - val_loss: 19268590.4680 - val_acc: 0.0000e+00\n",
            "Epoch 49/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090202.2827 - acc: 0.0000e+00 - val_loss: 19254184.9492 - val_acc: 0.0000e+00\n",
            "Epoch 50/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087792.5322 - acc: 0.0000e+00 - val_loss: 19224055.0111 - val_acc: 8.3333e-04\n",
            "Epoch 51/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12093682.9738 - acc: 0.0000e+00 - val_loss: 19239590.2582 - val_acc: 0.0000e+00\n",
            "Epoch 52/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089749.0098 - acc: 0.0000e+00 - val_loss: 19240904.4466 - val_acc: 8.3333e-04\n",
            "Epoch 53/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091849.7685 - acc: 4.1051e-04 - val_loss: 19234420.8314 - val_acc: 0.0000e+00\n",
            "Epoch 54/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086790.5939 - acc: 4.1051e-04 - val_loss: 19224638.9097 - val_acc: 0.0000e+00\n",
            "Epoch 55/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088862.2778 - acc: 0.0000e+00 - val_loss: 19277190.7257 - val_acc: 0.0000e+00\n",
            "Epoch 56/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088101.6974 - acc: 4.1051e-04 - val_loss: 19241253.5614 - val_acc: 0.0000e+00\n",
            "Epoch 57/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090542.3933 - acc: 0.0000e+00 - val_loss: 19233363.1763 - val_acc: 0.0000e+00\n",
            "Epoch 58/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089585.2142 - acc: 0.0000e+00 - val_loss: 19233648.1764 - val_acc: 0.0000e+00\n",
            "Epoch 59/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090053.2136 - acc: 0.0000e+00 - val_loss: 19224658.6380 - val_acc: 0.0000e+00\n",
            "Epoch 60/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12081062.9002 - acc: 0.0000e+00 - val_loss: 19286283.6278 - val_acc: 0.0000e+00\n",
            "Epoch 61/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092975.4328 - acc: 4.1051e-04 - val_loss: 19236317.9206 - val_acc: 0.0000e+00\n",
            "Epoch 62/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088164.8726 - acc: 0.0000e+00 - val_loss: 19259349.4161 - val_acc: 0.0000e+00\n",
            "Epoch 63/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087734.0052 - acc: 4.1051e-04 - val_loss: 19239693.9017 - val_acc: 0.0000e+00\n",
            "Epoch 64/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090460.3881 - acc: 0.0000e+00 - val_loss: 19231576.7476 - val_acc: 0.0000e+00\n",
            "Epoch 65/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12079486.3858 - acc: 0.0000e+00 - val_loss: 19206570.8649 - val_acc: 0.0000e+00\n",
            "Epoch 66/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092355.0905 - acc: 0.0000e+00 - val_loss: 19217028.4491 - val_acc: 0.0000e+00\n",
            "Epoch 67/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089289.1408 - acc: 0.0000e+00 - val_loss: 19224915.9298 - val_acc: 8.3333e-04\n",
            "Epoch 68/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12081538.9301 - acc: 4.1051e-04 - val_loss: 19206544.9140 - val_acc: 0.0000e+00\n",
            "Epoch 69/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12095567.4436 - acc: 0.0000e+00 - val_loss: 19229429.3237 - val_acc: 0.0000e+00\n",
            "Epoch 70/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089072.1616 - acc: 0.0000e+00 - val_loss: 19252037.7600 - val_acc: 0.0000e+00\n",
            "Epoch 71/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087881.3354 - acc: 0.0000e+00 - val_loss: 19235456.8027 - val_acc: 0.0000e+00\n",
            "Epoch 72/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092465.7401 - acc: 4.1051e-04 - val_loss: 19239523.2066 - val_acc: 0.0000e+00\n",
            "Epoch 73/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092354.5397 - acc: 0.0000e+00 - val_loss: 19224710.7974 - val_acc: 8.3333e-04\n",
            "Epoch 74/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086320.2386 - acc: 0.0000e+00 - val_loss: 19272986.3039 - val_acc: 8.3333e-04\n",
            "Epoch 75/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088785.3937 - acc: 0.0000e+00 - val_loss: 19260934.1001 - val_acc: 0.0000e+00\n",
            "Epoch 76/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090565.7407 - acc: 0.0000e+00 - val_loss: 19221018.6496 - val_acc: 0.0000e+00\n",
            "Epoch 77/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092256.3978 - acc: 0.0000e+00 - val_loss: 19227924.8259 - val_acc: 0.0000e+00\n",
            "Epoch 78/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12084319.0442 - acc: 0.0000e+00 - val_loss: 19263948.7018 - val_acc: 0.0000e+00\n",
            "Epoch 79/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089272.7618 - acc: 0.0000e+00 - val_loss: 19249471.7088 - val_acc: 8.3333e-04\n",
            "Epoch 80/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12083674.1512 - acc: 0.0000e+00 - val_loss: 19293340.7617 - val_acc: 0.0000e+00\n",
            "Epoch 81/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12083195.5756 - acc: 0.0000e+00 - val_loss: 19253550.5438 - val_acc: 0.0000e+00\n",
            "Epoch 82/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090159.2165 - acc: 0.0000e+00 - val_loss: 19269123.3680 - val_acc: 8.3333e-04\n",
            "Epoch 83/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092187.8886 - acc: 0.0000e+00 - val_loss: 19252726.5001 - val_acc: 0.0000e+00\n",
            "Epoch 84/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12094044.5383 - acc: 0.0000e+00 - val_loss: 19236841.9906 - val_acc: 0.0000e+00\n",
            "Epoch 85/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089605.2501 - acc: 4.1051e-04 - val_loss: 19231733.8217 - val_acc: 0.0000e+00\n",
            "Epoch 86/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091816.8755 - acc: 0.0000e+00 - val_loss: 19278106.4663 - val_acc: 0.0000e+00\n",
            "Epoch 87/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090521.5908 - acc: 4.1051e-04 - val_loss: 19284225.9349 - val_acc: 0.0000e+00\n",
            "Epoch 88/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091746.5542 - acc: 0.0000e+00 - val_loss: 19263962.4574 - val_acc: 0.0000e+00\n",
            "Epoch 89/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090902.0611 - acc: 4.1051e-04 - val_loss: 19241652.6619 - val_acc: 0.0000e+00\n",
            "Epoch 90/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12084307.0537 - acc: 0.0000e+00 - val_loss: 19287930.8135 - val_acc: 0.0000e+00\n",
            "Epoch 91/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092431.0277 - acc: 0.0000e+00 - val_loss: 19259353.9753 - val_acc: 0.0000e+00\n",
            "Epoch 92/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091347.5731 - acc: 0.0000e+00 - val_loss: 19230268.1341 - val_acc: 0.0000e+00\n",
            "Epoch 93/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088040.5592 - acc: 4.1051e-04 - val_loss: 19281288.2039 - val_acc: 0.0000e+00\n",
            "Epoch 94/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092461.0216 - acc: 0.0000e+00 - val_loss: 19259803.2630 - val_acc: 0.0000e+00\n",
            "Epoch 95/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12078476.1596 - acc: 0.0000e+00 - val_loss: 19317386.4798 - val_acc: 0.0000e+00\n",
            "Epoch 96/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086913.9102 - acc: 0.0000e+00 - val_loss: 19313483.8933 - val_acc: 8.3333e-04\n",
            "Epoch 97/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086208.8659 - acc: 0.0000e+00 - val_loss: 19218800.3162 - val_acc: 0.0000e+00\n",
            "Epoch 98/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091818.1379 - acc: 0.0000e+00 - val_loss: 19224449.3116 - val_acc: 0.0000e+00\n",
            "Epoch 99/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090455.2382 - acc: 0.0000e+00 - val_loss: 19255006.0278 - val_acc: 0.0000e+00\n",
            "Epoch 100/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12085115.8612 - acc: 0.0000e+00 - val_loss: 19284673.8576 - val_acc: 0.0000e+00\n",
            "Epoch 101/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086293.6663 - acc: 0.0000e+00 - val_loss: 19224507.2729 - val_acc: 0.0000e+00\n",
            "Epoch 102/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12084101.1202 - acc: 0.0000e+00 - val_loss: 19256268.4302 - val_acc: 0.0000e+00\n",
            "Epoch 103/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092680.5756 - acc: 0.0000e+00 - val_loss: 19235581.3926 - val_acc: 8.3333e-04\n",
            "Epoch 104/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089980.6886 - acc: 4.1051e-04 - val_loss: 19269519.0733 - val_acc: 0.0000e+00\n",
            "Epoch 105/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091352.3518 - acc: 0.0000e+00 - val_loss: 19250137.7940 - val_acc: 0.0000e+00\n",
            "Epoch 106/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092205.3665 - acc: 0.0000e+00 - val_loss: 19246492.0804 - val_acc: 8.3333e-04\n",
            "Epoch 107/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092816.1162 - acc: 4.1051e-04 - val_loss: 19262538.6494 - val_acc: 0.0000e+00\n",
            "Epoch 108/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086145.0571 - acc: 0.0000e+00 - val_loss: 19223157.8167 - val_acc: 8.3333e-04\n",
            "Epoch 109/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12080305.6694 - acc: 4.1051e-04 - val_loss: 19326868.9696 - val_acc: 0.0000e+00\n",
            "Epoch 110/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12094198.6271 - acc: 0.0000e+00 - val_loss: 19263648.6188 - val_acc: 0.0000e+00\n",
            "Epoch 111/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091032.2668 - acc: 0.0000e+00 - val_loss: 19249902.9259 - val_acc: 0.0000e+00\n",
            "Epoch 112/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089745.8429 - acc: 0.0000e+00 - val_loss: 19265611.7281 - val_acc: 0.0000e+00\n",
            "Epoch 113/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090183.3571 - acc: 0.0000e+00 - val_loss: 19260619.0779 - val_acc: 0.0000e+00\n",
            "Epoch 114/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087030.3402 - acc: 0.0000e+00 - val_loss: 19296385.4320 - val_acc: 0.0000e+00\n",
            "Epoch 115/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088252.3577 - acc: 0.0000e+00 - val_loss: 19319010.3804 - val_acc: 0.0000e+00\n",
            "Epoch 116/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089615.5525 - acc: 0.0000e+00 - val_loss: 19232511.0168 - val_acc: 8.3333e-04\n",
            "Epoch 117/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12078813.1874 - acc: 0.0000e+00 - val_loss: 19311463.5359 - val_acc: 0.0000e+00\n",
            "Epoch 118/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091928.9201 - acc: 4.1051e-04 - val_loss: 19285736.6478 - val_acc: 0.0000e+00\n",
            "Epoch 119/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12094959.1485 - acc: 0.0000e+00 - val_loss: 19245610.0947 - val_acc: 0.0000e+00\n",
            "Epoch 120/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088869.4058 - acc: 0.0000e+00 - val_loss: 19261953.9217 - val_acc: 0.0000e+00\n",
            "Epoch 121/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090993.1475 - acc: 8.2102e-04 - val_loss: 19257612.4974 - val_acc: 0.0000e+00\n",
            "Epoch 122/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091280.4297 - acc: 4.1051e-04 - val_loss: 19246841.7777 - val_acc: 8.3333e-04\n",
            "Epoch 123/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092323.6541 - acc: 0.0000e+00 - val_loss: 19253073.0211 - val_acc: 8.3333e-04\n",
            "Epoch 124/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12086288.2589 - acc: 0.0000e+00 - val_loss: 19216688.7842 - val_acc: 0.0000e+00\n",
            "Epoch 125/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12093453.0524 - acc: 0.0000e+00 - val_loss: 19228077.5805 - val_acc: 0.0000e+00\n",
            "Epoch 126/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091822.2209 - acc: 8.2102e-04 - val_loss: 19241246.0730 - val_acc: 0.0000e+00\n",
            "Epoch 127/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089149.6449 - acc: 8.2102e-04 - val_loss: 19272828.6310 - val_acc: 8.3333e-04\n",
            "Epoch 128/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090951.0524 - acc: 0.0000e+00 - val_loss: 19257448.6675 - val_acc: 0.0000e+00\n",
            "Epoch 129/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089512.0230 - acc: 0.0000e+00 - val_loss: 19228694.1416 - val_acc: 0.0000e+00\n",
            "Epoch 130/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090973.7536 - acc: 4.1051e-04 - val_loss: 19264356.8189 - val_acc: 0.0000e+00\n",
            "Epoch 131/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12077253.8214 - acc: 4.1051e-04 - val_loss: 19215400.6243 - val_acc: 0.0000e+00\n",
            "Epoch 132/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12095185.2670 - acc: 0.0000e+00 - val_loss: 19225284.4694 - val_acc: 0.0000e+00\n",
            "Epoch 133/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12077275.9939 - acc: 0.0000e+00 - val_loss: 19333723.5161 - val_acc: 8.3333e-04\n",
            "Epoch 134/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12085268.1615 - acc: 0.0000e+00 - val_loss: 19234467.9794 - val_acc: 0.0000e+00\n",
            "Epoch 135/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089077.5956 - acc: 0.0000e+00 - val_loss: 19236302.1973 - val_acc: 0.0000e+00\n",
            "Epoch 136/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089090.1340 - acc: 8.2102e-04 - val_loss: 19243145.4940 - val_acc: 0.0000e+00\n",
            "Epoch 137/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089481.4685 - acc: 4.1051e-04 - val_loss: 19267931.2409 - val_acc: 0.0000e+00\n",
            "Epoch 138/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12080445.3305 - acc: 0.0000e+00 - val_loss: 19253441.3885 - val_acc: 0.0000e+00\n",
            "Epoch 139/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088243.9245 - acc: 0.0000e+00 - val_loss: 19252145.3370 - val_acc: 0.0000e+00\n",
            "Epoch 140/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12081323.6367 - acc: 0.0000e+00 - val_loss: 19303751.0182 - val_acc: 0.0000e+00\n",
            "Epoch 141/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12094326.7913 - acc: 0.0000e+00 - val_loss: 19245688.7349 - val_acc: 0.0000e+00\n",
            "Epoch 142/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090684.2350 - acc: 4.1051e-04 - val_loss: 19253127.9911 - val_acc: 8.3333e-04\n",
            "Epoch 143/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087546.1831 - acc: 8.2102e-04 - val_loss: 19301212.6470 - val_acc: 0.0000e+00\n",
            "Epoch 144/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092703.3982 - acc: 0.0000e+00 - val_loss: 19278381.4325 - val_acc: 0.0000e+00\n",
            "Epoch 145/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090506.9469 - acc: 0.0000e+00 - val_loss: 19230895.7236 - val_acc: 0.0000e+00\n",
            "Epoch 146/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088796.1658 - acc: 0.0000e+00 - val_loss: 19230821.5840 - val_acc: 0.0000e+00\n",
            "Epoch 147/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087400.0379 - acc: 0.0000e+00 - val_loss: 19217385.2917 - val_acc: 8.3333e-04\n",
            "Epoch 148/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092367.9171 - acc: 4.1051e-04 - val_loss: 19263503.6124 - val_acc: 0.0000e+00\n",
            "Epoch 149/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087122.6597 - acc: 0.0000e+00 - val_loss: 19233203.1015 - val_acc: 0.0000e+00\n",
            "Epoch 150/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091508.6680 - acc: 8.2102e-04 - val_loss: 19254741.7707 - val_acc: 0.0000e+00\n",
            "Epoch 151/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088752.7099 - acc: 0.0000e+00 - val_loss: 19273758.1420 - val_acc: 0.0000e+00\n",
            "Epoch 152/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090792.7770 - acc: 0.0000e+00 - val_loss: 19234950.6610 - val_acc: 0.0000e+00\n",
            "Epoch 153/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090540.9029 - acc: 4.1051e-04 - val_loss: 19265319.9715 - val_acc: 0.0000e+00\n",
            "Epoch 154/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087360.4113 - acc: 0.0000e+00 - val_loss: 19284258.4360 - val_acc: 0.0000e+00\n",
            "Epoch 155/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12094286.0992 - acc: 4.1051e-04 - val_loss: 19271396.0648 - val_acc: 0.0000e+00\n",
            "Epoch 156/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12082638.3448 - acc: 0.0000e+00 - val_loss: 19211259.5521 - val_acc: 0.0000e+00\n",
            "Epoch 157/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090682.3812 - acc: 0.0000e+00 - val_loss: 19277195.4923 - val_acc: 0.0000e+00\n",
            "Epoch 158/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089742.2830 - acc: 4.1051e-04 - val_loss: 19280775.5355 - val_acc: 0.0000e+00\n",
            "Epoch 159/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12088967.8370 - acc: 4.1051e-04 - val_loss: 19254277.5399 - val_acc: 0.0000e+00\n",
            "Epoch 160/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089015.3363 - acc: 0.0000e+00 - val_loss: 19232548.3932 - val_acc: 8.3333e-04\n",
            "Epoch 161/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087334.8067 - acc: 0.0000e+00 - val_loss: 19280448.4829 - val_acc: 0.0000e+00\n",
            "Epoch 162/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12093734.4722 - acc: 0.0000e+00 - val_loss: 19251362.0465 - val_acc: 0.0000e+00\n",
            "Epoch 163/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12094619.3254 - acc: 0.0000e+00 - val_loss: 19237686.3797 - val_acc: 0.0000e+00\n",
            "Epoch 164/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089612.5693 - acc: 0.0000e+00 - val_loss: 19226782.7708 - val_acc: 0.0000e+00\n",
            "Epoch 165/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090193.8247 - acc: 0.0000e+00 - val_loss: 19262064.2732 - val_acc: 0.0000e+00\n",
            "Epoch 166/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12082190.7239 - acc: 8.2102e-04 - val_loss: 19208441.2640 - val_acc: 0.0000e+00\n",
            "Epoch 167/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12095738.4018 - acc: 0.0000e+00 - val_loss: 19264255.7072 - val_acc: 0.0000e+00\n",
            "Epoch 168/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090913.0866 - acc: 0.0000e+00 - val_loss: 19271484.9305 - val_acc: 0.0000e+00\n",
            "Epoch 169/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092220.3160 - acc: 4.1051e-04 - val_loss: 19260298.1496 - val_acc: 0.0000e+00\n",
            "Epoch 170/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087807.5341 - acc: 0.0000e+00 - val_loss: 19306085.5293 - val_acc: 0.0000e+00\n",
            "Epoch 171/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091893.2550 - acc: 4.1051e-04 - val_loss: 19228388.2595 - val_acc: 8.3333e-04\n",
            "Epoch 172/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091018.6683 - acc: 0.0000e+00 - val_loss: 19248723.9333 - val_acc: 0.0000e+00\n",
            "Epoch 173/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091976.6816 - acc: 0.0000e+00 - val_loss: 19253219.5851 - val_acc: 8.3333e-04\n",
            "Epoch 174/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089289.7000 - acc: 0.0000e+00 - val_loss: 19247449.3587 - val_acc: 0.0000e+00\n",
            "Epoch 175/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089871.2328 - acc: 4.1051e-04 - val_loss: 19247688.8425 - val_acc: 0.0000e+00\n",
            "Epoch 176/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12087268.4668 - acc: 4.1051e-04 - val_loss: 19243338.3829 - val_acc: 0.0000e+00\n",
            "Epoch 177/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12085420.2148 - acc: 0.0000e+00 - val_loss: 19228989.4278 - val_acc: 0.0025\n",
            "Epoch 178/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12091008.5439 - acc: 0.0000e+00 - val_loss: 19233793.3817 - val_acc: 0.0000e+00\n",
            "Epoch 179/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089991.5436 - acc: 4.1051e-04 - val_loss: 19268716.9542 - val_acc: 0.0000e+00\n",
            "Epoch 180/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12089558.8167 - acc: 8.2102e-04 - val_loss: 19244320.4660 - val_acc: 0.0000e+00\n",
            "Epoch 181/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12085012.3535 - acc: 4.1051e-04 - val_loss: 19218949.1138 - val_acc: 0.0000e+00\n",
            "Epoch 182/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12090458.0111 - acc: 0.0000e+00 - val_loss: 19238516.6841 - val_acc: 0.0000e+00\n",
            "Epoch 183/500\n",
            "2436/2436 [==============================] - 3s 1ms/step - loss: 12092056.7356 - acc: 4.1051e-04 - val_loss: 19252575.9727 - val_acc: 0.0000e+00\n",
            "Epoch 184/500\n",
            "1435/2436 [================>.............] - ETA: 0s - loss: 12162814.0171 - acc: 0.0000e+00"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-a65988ed446f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuralNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewTrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mip_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewTrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mip_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWkDqj1dA0iw",
        "colab_type": "code",
        "outputId": "8799fb6f-bfa6-4520-90ad-ab12c552f021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "###########################################\n",
        "# Step 4: Model Prediction\n",
        "#############################################\n",
        "out = neuralNetwork.predict(newTestData[:,:2])\n",
        "print(out)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.1313]\n",
            " [5090.132 ]\n",
            " [5090.132 ]\n",
            " [5090.132 ]\n",
            " [5090.132 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsDwXVAABHDF",
        "colab_type": "code",
        "outputId": "87c1ef05-3374-4922-be64-5718f3a7bcb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#################################\n",
        "# Step 5 : Reserve calculation\n",
        "#################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#print(' Test Data',test_data)\n",
        "#print('pred', model.predict([2,]))\n",
        "#print('C',C)\n",
        "true_reserve = 0\n",
        "for i in range(1,np.shape(triangle)[0]):\n",
        "    j = np.shape(triangle)[1]-1-i\n",
        "    #print(i,j)\n",
        "    #print('last known',C[i,j])\n",
        "    #print(' last estimate',C[i,np.shape(triangle)[1]-1])\n",
        "    true_reserve += (C[i,np.shape(triangle)[1]-1] - C[i,j])\n",
        "    #print(true_reserve)\n",
        "true_reserve= 17325\n",
        "print(\" True reserve\",true_reserve)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " True reserve 17325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ3X4QakBct6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "appended= np.array(list(zip(test_data,np.ravel(out))))\n",
        "len(appended)\n",
        "\n",
        "out_dict = {}\n",
        "for i in range(len(appended)):\n",
        "    #print(tuple(appended[i,0][:2]))\n",
        "    #print(appended[i,1])\n",
        "    out_dict[tuple(appended[i,0][:2])] = appended[i,1]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wQkKmhECMvg",
        "colab_type": "code",
        "outputId": "dcca0e91-8a44-4bc7-c6f7-c847248ae5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "appended[0,0][:2]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 7.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUQKDOqqBkGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_reserve = 0\n",
        "final_pred = []\n",
        "actuals = []\n",
        "for i in range(1,np.shape(triangle)[0]):\n",
        "    j = int(np.shape(triangle)[1]-1-i)    \n",
        "    #print(i,j)\n",
        "    #print('last known',C[i,j])\n",
        "    #print(' last pred', out_dict[(i,np.shape(triangle)[1]-1)])\n",
        "    #print(' last estimate',C[i,np.shape(triangle)[1]-1])\n",
        "    final_pred.append(out_dict[(i,np.shape(triangle)[1]-1)] - C[i,j])\n",
        "    actuals.append(C[i,np.shape(triangle)[1]-1] - C[i,j])\n",
        "    pred_reserve +=(out_dict[(i,np.shape(triangle)[1]-1)] - C[i,j])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsYABGSNBq_7",
        "colab_type": "code",
        "outputId": "be947af1-35b9-45b5-c313-f80bb0fafd35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(\" Predicted reserve\",pred_reserve)\n",
        "print(' Bias',pred_reserve - true_reserve)\n",
        "print('Bias pct',(pred_reserve-true_reserve)/true_reserve)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Predicted reserve 13179.080078125\n",
            " Bias -4145.919921875\n",
            "Bias pct -1.7606972628066377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bLrRyChBv7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}