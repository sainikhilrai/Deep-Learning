{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level language model - Dinosaurus land\n",
    "\n",
    "Welcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment they are back. You are in charge of a special task. Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go beserk, so choose wisely! \n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/dino.jpg\" style=\"width:250;height:300px;\">\n",
    "\n",
    "</td>\n",
    "\n",
    "</table>\n",
    "\n",
    "Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this [dataset](dinos.txt). (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. Hopefully this algorithm will keep you and your team safe from the dinosaurs' wrath! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_value_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "# In this cell you will write code to do the following:\n",
    "# Each line in dinos.txt contains the names of dinosaurs which will be used for training\n",
    "# step 1: read the entire contents of the file as a string\n",
    "# step 2: convert the entire string to lowercase\n",
    "# step 3: extract the characters that make up the string in a set. This will be the vocabulary.\n",
    "# step 4: print the size of the string (which will be total number of characters in the training set) and\n",
    "#         size of vocabulary. Note that '\\n' is part of vocabulary and it will be used as EOS character \n",
    "#         in our model\n",
    "data = []\n",
    "with open(\"dinos.txt\") as fileobj:\n",
    "    for line in fileobj:\n",
    "        line = line.lower()\n",
    "        for ch in line:\n",
    "            data.append(ch)\n",
    "vocab = set(data)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell you will write code to do the following:\n",
    "# step 1: create a dictionary that maps characters to indices\n",
    "# step 2: create another dictionary that maps indices to characters\n",
    "char_to_idx = {}\n",
    "idx_to_char = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    char_to_idx[word] = i\n",
    "    idx_to_char[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize few variables\n",
    "n_hidden_nodes = 50\n",
    "num_iterations = 35000\n",
    "torch.backends.cudnn.enabled = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of the model\n",
    "\n",
    "A Pictorial representation of the model you will build is given below: \n",
    "    \n",
    "<img src=\"images/rnn1.png\" style=\"width:450;height:300px;\">\n",
    "<caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook \"Building a RNN - Step by Step\".  </center></caption>\n",
    "\n",
    "At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set, while $Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is such that at every time-step $t$, we have $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell you will write code to do the following:\n",
    "# You need to create a class inherited from nn.Module that uses nn.RNN and nn.Linear along with F.log_softmax\n",
    "# (instead of softmax) to forward the log probabilites to the caller of the forward method of this class.\n",
    "# Note that you have to accumulate log probabilities with respect to every character in the sequence \n",
    "# and then forward this.\n",
    "# You also have to explicitly initialize the parameters of RNN and Linear modules in the init method of this\n",
    "# class. We will use 1 layer unidirectional RNN. Weight parameters for RNN are weight_ih_l0 and weight_hh_l0. \n",
    "# These are wrongly documented as weight_ih_l[0] and weight_hh_l[0] in the documentation. Bias parameters are \n",
    "# bias_ih_l0 and bias_hh_l0.\n",
    "class cllm(nn.Module):\n",
    "    def __init__(self,n_hidden_nodes,inputsize,vocab_size):\n",
    "        super(cllm,self).__init__()\n",
    "        self.rnn = nn.RNN(inputsize,n_hidden_nodes)\n",
    "        self.linear = nn.Linear(n_hidden_nodes,vocab_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.weight_initialize\n",
    "    \n",
    "    def weight_initialize():\n",
    "        init.normal_(self.rnn.weight_ih_l0)\n",
    "        init.normal_(self.rnn.weight_hh_l0)\n",
    "        init.constant_(self.rnn.bias_ih_l0,1)\n",
    "        init.constant_(self.rnn.bias_hh_l0,1)\n",
    "        \n",
    "    def forward(self,inputs,h0):\n",
    "        out,hidden = self.rnn(inputs,h0)\n",
    "        out = self.linear(out)\n",
    "        out = F.log_softmax(out)\n",
    "        return out,hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell you will write code to do the following:\n",
    "# step 1: Instantiate the model and port it to GPU\n",
    "# step 2: set loss criterion to NLL loss\n",
    "# step 3: set optimizer to SGD with a suitable learning rate\n",
    "inputsize = 27\n",
    "model = cllm(n_hidden_nodes,inputsize,vocab_size).to(device)\n",
    "lossCriterion = nn.NLLLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of all dinosaur names (training examples).\n",
    "# Note that earlier we had just built the vocabulary.\n",
    "# examples variable below is a list of dinosaur names in lowercase with all leading and trainling white spaces\n",
    "# removed.\n",
    "# The examples are randomly shuffled.\n",
    "with open(\"dinos.txt\") as f:\n",
    "    examples = f.readlines()\n",
    "examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "# Shuffle list of all dinosaur names\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampling the name\n",
    "\n",
    "class Sampling(nn.Module):\n",
    "    def __init__(self,myModel,n_hidden_nodes,inputsize,vocab_size):\n",
    "        super(Sampling,self).__init__()\n",
    "        self.rnncell = nn.RNNCell(inputsize,n_hidden_nodes)\n",
    "        self.linear = nn.Linear(n_hidden_nodes,vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def forward(self,one_hot,h_0):\n",
    "        h_n = self.rnncell(one_hot,h_0)\n",
    "        output = self.linear(h_n)\n",
    "        output = self.softmax(output)\n",
    "        return output,h_n\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        self.rnncell.weight_ih_l0 = myModel.state_dict()['rnn.weight_ih_l0']\n",
    "        self.rnncell.weight_hh_l0 = myModel.state_dict()['rnn.weight_hh_l0']\n",
    "        self.rnncell.bias_ih_l0 = myModel.state_dict()['rnn.bias_ih_l0']\n",
    "        self.rnncell.bias_hh_l0 = myModel.state_dict()['rnn.weight_hh_l0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dino_name(model,char_to_idx,idx_to_char):\n",
    "    \n",
    "    h_0 = torch.zeros(1,n_hidden_nodes)\n",
    "    input_0 = torch.zeros(1,vocab_size)\n",
    "\n",
    "    sampleModel = Sampling(model,n_hidden_nodes,27,vocab_size)\n",
    "    dino_name = []\n",
    "    h_n = h_0\n",
    "    vocab_array = np.arange(0,vocab_size)\n",
    "    one_hot = torch.zeros(1,vocab_size)\n",
    "    for i in range(10):\n",
    "        output,h_n = sampleModel.forward(input_0,h_n)\n",
    "        log_prob = output.detach().cpu().numpy().ravel()\n",
    "        index = np.random.choice(vocab_array,p = log_prob)\n",
    "        one_hot[0,index] = 1\n",
    "        if(idx_to_char[index]!='\\n'):\n",
    "            dino_name.append(index)\n",
    "        input_0 = one_hot\n",
    "    \n",
    "    for i in dino_name:\n",
    "        print(\"\".join(idx_to_char[i]),end=\"\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda35/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss 2\n",
      "bpxqejhfei\n",
      "\n",
      "jwmvqsmvfk\n",
      "\n",
      "lixryfwejd\n",
      "\n",
      "mrnbocmxpo\n",
      "\n",
      "wgfvapsvy\n",
      "\n",
      "Iteration 1000 loss 2\n",
      "ebigroqvsr\n",
      "\n",
      "ecdgjvvpsj\n",
      "\n",
      "xvjegcslpd\n",
      "\n",
      "mwtddtrkzz\n",
      "\n",
      "wpbeyaw\n",
      "\n",
      "Iteration 2000 loss 1\n",
      "qpqmblxoso\n",
      "\n",
      "mtfzbdhpk\n",
      "\n",
      "lwkvepbow\n",
      "\n",
      "sgfnapecv\n",
      "\n",
      "croxnuwnwf\n",
      "\n",
      "Iteration 3000 loss 1\n",
      "wpusxbfzj\n",
      "\n",
      "tljecvxdsz\n",
      "\n",
      "mcfawserm\n",
      "\n",
      "cnstvzdsc\n",
      "\n",
      "whtxldct\n",
      "\n",
      "Iteration 4000 loss 1\n",
      "tgfkvhvnu\n",
      "\n",
      "yffksesnzv\n",
      "\n",
      "osffldkttl\n",
      "\n",
      "nsinfkknef\n",
      "\n",
      "wspwetznmk\n",
      "\n",
      "Iteration 5000 loss 1\n",
      "qfsxcaukrl\n",
      "\n",
      "mjzkjeehh\n",
      "\n",
      "gnjvqugbx\n",
      "\n",
      "gkqrrbftqb\n",
      "\n",
      "qstmbjjfn\n",
      "\n",
      "Iteration 6000 loss 1\n",
      "kevbvjurae\n",
      "\n",
      "csabvsfmgi\n",
      "\n",
      "hwdaceyhkk\n",
      "\n",
      "ejcadnrjq\n",
      "\n",
      "sdxroglauz\n",
      "\n",
      "Iteration 7000 loss 1\n",
      "idtfteruw\n",
      "\n",
      "llvrlcxqb\n",
      "\n",
      "tyadquernk\n",
      "\n",
      "ptqvpnosn\n",
      "\n",
      "jrcpnwgzh\n",
      "\n",
      "Iteration 8000 loss 1\n",
      "ogmctuiig\n",
      "\n",
      "wjusbqujch\n",
      "\n",
      "lcwdvbdtl\n",
      "\n",
      "gerstuplty\n",
      "\n",
      "lohillxtfn\n",
      "\n",
      "Iteration 9000 loss 1\n",
      "hnndigppx\n",
      "\n",
      "bndvqmnai\n",
      "\n",
      "gbgvjeqrm\n",
      "\n",
      "rdsovohcet\n",
      "\n",
      "zbqjuhsch\n",
      "\n",
      "Iteration 10000 loss 1\n",
      "cwjgtxfarm\n",
      "\n",
      "swzvkowewt\n",
      "\n",
      "wtyzqusaey\n",
      "\n",
      "ndkopokvvs\n",
      "\n",
      "mzdnflgdpq\n",
      "\n",
      "Iteration 11000 loss 1\n",
      "tosdcxyjqw\n",
      "\n",
      "wkzgbzibp\n",
      "\n",
      "osgndxfuvq\n",
      "\n",
      "jeflxczyfc\n",
      "\n",
      "qwdebynqfz\n",
      "\n",
      "Iteration 12000 loss 1\n",
      "zamjvxvmad\n",
      "\n",
      "dtpdyncwmx\n",
      "\n",
      "zfhvslzjce\n",
      "\n",
      "qwfczrmid\n",
      "\n",
      "purkunzzoj\n",
      "\n",
      "Iteration 13000 loss 2\n",
      "tcnktnylq\n",
      "\n",
      "wzdstdrgnp\n",
      "\n",
      "dfkjqxjut\n",
      "\n",
      "kcsheedpax\n",
      "\n",
      "hwlbnicp\n",
      "\n",
      "Iteration 14000 loss 1\n",
      "majujcdjho\n",
      "\n",
      "gqizqsnuh\n",
      "\n",
      "yubljaeqor\n",
      "\n",
      "ebgmaaqjoc\n",
      "\n",
      "ekchlqvktz\n",
      "\n",
      "Iteration 15000 loss 1\n",
      "hydlmmebzc\n",
      "\n",
      "yrwpesmksw\n",
      "\n",
      "xidiuaksgh\n",
      "\n",
      "gstujfvjpr\n",
      "\n",
      "fbtpqxgkmd\n",
      "\n",
      "Iteration 16000 loss 2\n",
      "oxmlcbdxa\n",
      "\n",
      "rthezihzxx\n",
      "\n",
      "keriupbci\n",
      "\n",
      "fqbwzkozz\n",
      "\n",
      "ejemabxpbi\n",
      "\n",
      "Iteration 17000 loss 1\n",
      "izenjinlvl\n",
      "\n",
      "hgzcxrvwio\n",
      "\n",
      "ruwukfhjpz\n",
      "\n",
      "zvofbxxyos\n",
      "\n",
      "ugxzlhclft\n",
      "\n",
      "Iteration 18000 loss 0\n",
      "wlkxbwwnu\n",
      "\n",
      "vhtonfeij\n",
      "\n",
      "nmteynxyi\n",
      "\n",
      "kmovhsckob\n",
      "\n",
      "wiokrzxjhu\n",
      "\n",
      "Iteration 19000 loss 1\n",
      "sdubdpnief\n",
      "\n",
      "ivvylefypy\n",
      "\n",
      "adyohksi\n",
      "\n",
      "oltqyznkrd\n",
      "\n",
      "yjsvxagol\n",
      "\n",
      "Iteration 20000 loss 1\n",
      "rlmnigybiq\n",
      "\n",
      "zjwuxorukq\n",
      "\n",
      "cpiocxxpf\n",
      "\n",
      "yfjipwddoo\n",
      "\n",
      "esmklxhij\n",
      "\n",
      "Iteration 21000 loss 0\n",
      "cgxpfdhub\n",
      "\n",
      "krehruag\n",
      "\n",
      "xbmnlhhvkh\n",
      "\n",
      "eprtekwtoy\n",
      "\n",
      "fwraxeytk\n",
      "\n",
      "Iteration 22000 loss 0\n",
      "egpjlgubyf\n",
      "\n",
      "bagsmxflw\n",
      "\n",
      "vsnrtlgqd\n",
      "\n",
      "svkkiubno\n",
      "\n",
      "fyjpczfkoo\n",
      "\n",
      "Iteration 23000 loss 1\n",
      "wgaswufhpc\n",
      "\n",
      "tgkxlnaol\n",
      "\n",
      "dkswsobpgo\n",
      "\n",
      "guhdpmyiu\n",
      "\n",
      "aysmduvya\n",
      "\n",
      "Iteration 24000 loss 0\n",
      "cfuylzpim\n",
      "\n",
      "vgmzhhdezf\n",
      "\n",
      "axmyufcle\n",
      "\n",
      "oyjnjouhy\n",
      "\n",
      "lflmlgjlei\n",
      "\n",
      "Iteration 25000 loss 0\n",
      "kwbabyblve\n",
      "\n",
      "fawdnjuqqo\n",
      "\n",
      "iugdxphxk\n",
      "\n",
      "qtpnnxvtyq\n",
      "\n",
      "ylfelcqntu\n",
      "\n",
      "Iteration 26000 loss 1\n",
      "xwexenlgjo\n",
      "\n",
      "msxusdznip\n",
      "\n",
      "pmhlcfpocy\n",
      "\n",
      "mivpxechxv\n",
      "\n",
      "szakxhyb\n",
      "\n",
      "Iteration 27000 loss 0\n",
      "vkxkuwvkig\n",
      "\n",
      "vhstdlsaii\n",
      "\n",
      "qscfnrfzl\n",
      "\n",
      "ynirqcekru\n",
      "\n",
      "ffjqgcavey\n",
      "\n",
      "Iteration 28000 loss 0\n",
      "ktqhrpcwa\n",
      "\n",
      "dbsuzlhcu\n",
      "\n",
      "bwlcdlnu\n",
      "\n",
      "axscyfummv\n",
      "\n",
      "yvoebcmuz\n",
      "\n",
      "Iteration 29000 loss 1\n",
      "vnodiekwjl\n",
      "\n",
      "oomumctofz\n",
      "\n",
      "wizrwnmsic\n",
      "\n",
      "uawpjdata\n",
      "\n",
      "caxhdhgdu\n",
      "\n",
      "Iteration 30000 loss 1\n",
      "hnctwaswpz\n",
      "\n",
      "vtsewvpjkf\n",
      "\n",
      "nfertxyjy\n",
      "\n",
      "erukjhvzrl\n",
      "\n",
      "nounaenwev\n",
      "\n",
      "Iteration 31000 loss 1\n",
      "psseffgwqp\n",
      "\n",
      "nlopanxalr\n",
      "\n",
      "wmzripubs\n",
      "\n",
      "hpkebmzfhh\n",
      "\n",
      "yuuawunbkd\n",
      "\n",
      "Iteration 32000 loss 1\n",
      "zarqwteulb\n",
      "\n",
      "osmjdrqzgj\n",
      "\n",
      "zfmaogbtt\n",
      "\n",
      "guzhpqhlfp\n",
      "\n",
      "fqcbfvyqwj\n",
      "\n",
      "Iteration 33000 loss 0\n",
      "bkblclrubr\n",
      "\n",
      "cmnscmjsxn\n",
      "\n",
      "caofxqcje\n",
      "\n",
      "vjzgjsjmv\n",
      "\n",
      "pbwuyhwlsa\n",
      "\n",
      "Iteration 34000 loss 1\n",
      "nlxrqbef\n",
      "\n",
      "jzacscyigf\n",
      "\n",
      "soflmoane\n",
      "\n",
      "weaijmvsfd\n",
      "\n",
      "wqjdyllwtm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill or complete the code where required.\n",
    "# We will do training in this cell.\n",
    "# Batch size will be 1 (one).\n",
    "batch_size = 1\n",
    "h_0 = torch.zeros(1,1,n_hidden_nodes)# fill h_0 with tensor of zeros. h_0 is the a_0 u saw in your lectures\n",
    "      # see documentation to determine dimension of h_0 and accordingly create the tensor\n",
    "\n",
    "for j in range(num_iterations):      # number of training iterations\n",
    "    index = j % len(examples) # choose the index of a dinosaur name. Index is guarenteed to be in \n",
    "                              # range  of 0 to # examples - 1\n",
    "    data = [char_to_idx[ch] for ch in examples[index]] # create the list of indices of charecters that\n",
    "                                                      # make up the chosen dinosaur name\n",
    "    label = data[1:] + [char_to_idx[\"\\n\"]] # the label list. Here label at each time instant is the character \n",
    "                                          # next to the input character at that time. So y(t) = x(t+1). When\n",
    "                                          # t is final time instant, label will be EOS character which is \n",
    "                                          # '\\n' for us\n",
    "    \n",
    "    # You are required to do the following below:\n",
    "    # Convert data to a tensor of one hot representations\n",
    "    # Convert label to a LongTensor of indices required for NLL loss. See documentation for clarity.\n",
    "    # Do the forward propagation, receive log probabilities, compute loss at every time instant and aggregate \n",
    "    # the loss.\n",
    "    # Print the loss in every iteration.\n",
    "   \n",
    "    #tensor into one hot\n",
    "    seq_len = len(data)\n",
    "    one_hot = torch.zeros(seq_len,1,inputsize)\n",
    "    for i in range(seq_len):        \n",
    "        one_hot[i,0,data[i]] = 1\n",
    "    \n",
    "    one_hot = one_hot.to(device)\n",
    "    \n",
    "    #convert label to a LongTensor of indices required for NLL loss. See documentation for clarity.\n",
    "    label = torch.tensor(label,dtype=torch.long).reshape(seq_len,-1)\n",
    "    label = label.to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    total_loss = 0\n",
    "    hidden = h_0.to(device)\n",
    "    \n",
    "    #back-propagation\n",
    "    logprob,hidden = model.forward(one_hot,hidden)\n",
    "    for i in range(seq_len):\n",
    "        total_loss += lossCriterion(logprob[i],label[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    total_loss.backward()\n",
    "    clip_grad_value_(model.parameters(),5)  #gradient clipping\n",
    "    optimizer.step()\n",
    "    loss = total_loss/seq_len\n",
    "    #print(\"total Loss\",(loss.item()))\n",
    "    \n",
    "    if(j%1000 == 0):\n",
    "        print(\"Iteration %d loss %d\"%(j,loss))\n",
    "        \n",
    "        for index in range(5):\n",
    "            get_dino_name(model,char_to_idx,idx_to_char)\n",
    "        \n",
    "          \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "torch.save(model.state_dict(),'./rnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "myModel = cllm(n_hidden_nodes,inputsize,vocab_size).to(device)\n",
    "myModel.load_state_dict(torch.load('./rnn.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
