{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the pretrained VGG pretrained \n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "#make vgg16 model layers as non trainable\n",
    "for layer in vgg16.parameters():\n",
    "    layer.requires_grad= False\n",
    "\n",
    "features= list(vgg16.classifier.children())[:-1] #drop the last layer\n",
    "#features.extend([nn.Linear(totalFilters,16)])    #add the linear layer\n",
    "vgg16.classifier= nn.Sequential(*features)       #build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collate(samples):\n",
    "    #print(samples[1])\n",
    "    img,label= zip(*samples)\n",
    "    \n",
    "    #sort sequences based on length\n",
    "    seqLengths= [len(seq) for seq in label]\n",
    "    maxSeqLength= max(seqLengths)\n",
    "    sortedList= sorted(zip(list(img),label,seqLengths),key= lambda x:-x[2])\n",
    "    img,label,seqLengths= zip(*sortedList)\n",
    "    images = img[0].unsqueeze(0)\n",
    "    for i in range(1,len(img)):\n",
    "        images = torch.cat((images,img[i].unsqueeze(0)),dim=0)\n",
    "    print(images.size())\n",
    "    \n",
    "    #create tensor with padded sequences\n",
    "    paddedSeqs= torch.LongTensor(len(img),maxSeqLength)\n",
    "    paddedSeqs.fill_(0)\n",
    "    for (i,seq) in enumerate(label):\n",
    "        paddedSeqs[i,:len(seq)]= seq\n",
    "    return images,paddedSeqs,seqLengths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "class myDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, pathToImages,pathToLabels,transform=None):\n",
    "        self.pathToImages= pathToImages\n",
    "        self.pathToLabels= pathToLabels\n",
    "        imageTensor= []\n",
    "        labelTensor= []\n",
    "        pathToImages= self.pathToImages+'*jpg'\n",
    "                \n",
    "        #convert all the images to Tensor\n",
    "        images = glob.glob(\"./JPEGImages/*jpg\")\n",
    "        for image in images:\n",
    "            img= Image.open(image).convert('RGB')\n",
    "            trans= transforms.Resize((224,224))\n",
    "            trans1= transforms.ToTensor()\n",
    "            tensorImg= trans1(trans(img))\n",
    "            imageTensor.append(tensorImg)\n",
    "                \n",
    "        file = open(self.pathToLabels,\"r\")\n",
    "        #convert all the labels to tensor\n",
    "        for line in file:\n",
    "            line = line.replace(\"\\t\",\" \")\n",
    "            line = line.replace(\"\\n\",\"\")\n",
    "            \n",
    "            #line = line[11:] #get only from the label\n",
    "            newLine = line.split()\n",
    "            length = len(newLine)\n",
    "            tensorLab= []\n",
    "            \n",
    "            for i in range(1,length):\n",
    "                tensorLab.append(int(newLine[i]))\n",
    "            [x+1 for x in tensorLab] #increment the label\n",
    "            tensorLab.insert(0,1)    #add 1 for the sos\n",
    "            tensorLab.append(22)     #add 22 for the eos\n",
    "            tensorLab= np.asarray(tensorLab) #convert list to numpyArray\n",
    "            tensorLab= torch.from_numpy(tensorLab) #convert numpyArray to tensor\n",
    "            labelTensor.append(tensorLab)\n",
    "        \n",
    "        self.images= imageTensor\n",
    "        self.labels= labelTensor\n",
    "        self.transform= transform\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        #print(\"The total images in Dataset:__len__\")\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img= self.images[index]\n",
    "        label= self.labels[index]\n",
    "        return img,label\n",
    "                       \n",
    "        \n",
    "      \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the customize dataset class\n",
    "myOwn= myDataset(\"./JPEGImages/\",\"./data.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to predict the class\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self,inputSize,embeddingDim,hiddenSize,num_of_layer):\n",
    "        super(Network,self).__init__()\n",
    "        self.embedding= nn.Embedding(inputSize,embeddingDim,padding_idx=0)\n",
    "        self.lstm= nn.LSTM(embeddingDim,hiddenSize,num_of_layer,batch_first=True)\n",
    "        self.vggLinear = nn.Linear(4096,embeddingDim)\n",
    "        self.linear1= nn.Linear(hiddenSize,embeddingDim)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.FinalLinear= nn.Linear(embeddingDim,23)\n",
    "        self.activation2= nn.LogSoftmax(dim = -1)\n",
    "    \n",
    "    def forward(self,inputs,h_0,c_0,vgginput,originaLength):\n",
    "#         embeddding = (self.embedding(inputs)).view(1,-1)\n",
    "       \n",
    "        embedding= self.embedding(inputs)\n",
    "#        embedding = embedding.permute(1,0,2) #permute the batch size index\n",
    "        packed= torch.nn.utils.rnn.pack_padded_sequence(embedding,originaLength, batch_first=True)\n",
    "        batch_size = packed[1]\n",
    "        output,_ = self.lstm(packed,(h_0,c_0))\n",
    "        unpack,_ = torch.nn.utils.rnn.pad_packed_sequence(output,batch_first=True)\n",
    "        print(unpack.size())\n",
    "        unpack = unpack.permute(1,0,2)\n",
    "        probs = []\n",
    "        for i in range(len(inputs[1])):\n",
    "            original_data = unpack[i,:batch_size[i],:]\n",
    "            original_activations = vgginput[:batch_size[i],:]\n",
    "            out = self.vggLinear(original_activations) + self.linear1(original_data)         \n",
    "            output = self.activation1(out)\n",
    "            output = self.FinalLinear(output)\n",
    "            output = self.activation2(output)\n",
    "            probs.append(output)\n",
    "        return probs,batch_size\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameteres\n",
    "vocab_size= 23\n",
    "embedding_Dim= 16\n",
    "hiddenNodes= 512\n",
    "numLayers= 1\n",
    "model = Network(vocab_size,embedding_Dim,hiddenNodes,numLayers)\n",
    "lossCriterion= nn.NLLLoss()\n",
    "optimizer= torch.optim.Adam(model.parameters(),lr= 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader= torch.utils.data.DataLoader(myOwn,batch_size=32,shuffle=True,num_workers=0,collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize the h_0 and c_0\n",
    "batchSize= 32\n",
    "\n",
    "h_0= torch.zeros(1,batchSize,hiddenNodes) \n",
    "c_0= torch.zeros(1,batchSize,hiddenNodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 10, 512])\n",
      "batch loss 3.141581153869629\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 10, 512])\n",
      "batch loss 8.66195068359375\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 15, 512])\n",
      "batch loss 4.996309916178386\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 13, 512])\n",
      "batch loss 3.9961770864633412\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 3.077191072351792\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 3.161558712230009\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 15, 512])\n",
      "batch loss 3.0747754414876303\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 11, 512])\n",
      "batch loss 3.100066445090554\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 11, 512])\n",
      "batch loss 3.097578915682706\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 3.067233814912684\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 28, 512])\n",
      "batch loss 2.977923257010324\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 14, 512])\n",
      "batch loss 3.084254673549107\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 13, 512])\n",
      "batch loss 3.04518802349384\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 3.0507036096909466\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 22, 512])\n",
      "batch loss 2.935050270774148\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 15, 512])\n",
      "batch loss 2.9560040791829425\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 16, 512])\n",
      "batch loss 3.1232035160064697\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 12, 512])\n",
      "batch loss 3.1350720723470054\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 20, 512])\n",
      "batch loss 2.9488075256347654\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 13, 512])\n",
      "batch loss 2.9662208557128906\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 11, 512])\n",
      "batch loss 3.031852375377308\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 8, 512])\n",
      "batch loss 2.946639060974121\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 18, 512])\n",
      "batch loss 2.8297193315294056\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 18, 512])\n",
      "batch loss 2.907513724433051\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 2.963931812959559\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 12, 512])\n",
      "batch loss 2.930314064025879\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 12, 512])\n",
      "batch loss 3.045066515604655\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 12, 512])\n",
      "batch loss 2.8770694732666016\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 18, 512])\n",
      "batch loss 2.8255242241753473\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 15, 512])\n",
      "batch loss 3.0157435099283854\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 27, 512])\n",
      "batch loss 2.6975617585358798\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 26, 512])\n",
      "batch loss 2.946317819448618\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 22, 512])\n",
      "batch loss 2.8338583165949043\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 13, 512])\n",
      "batch loss 2.853769742525541\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 13, 512])\n",
      "batch loss 2.8053433344914365\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 13, 512])\n",
      "batch loss 2.766063983623798\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 8, 512])\n",
      "batch loss 2.7695772647857666\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 2.8082611981560204\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 39, 512])\n",
      "batch loss 2.7132566403119993\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 15, 512])\n",
      "batch loss 2.976829020182292\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 15, 512])\n",
      "batch loss 2.8319498697916665\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 8, 512])\n",
      "batch loss 2.7985291481018066\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 12, 512])\n",
      "batch loss 2.812013626098633\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 14, 512])\n",
      "batch loss 2.7725794655936107\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 10, 512])\n",
      "batch loss 2.811969757080078\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 14, 512])\n",
      "batch loss 2.7294374193464006\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 2.9427793166216683\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 14, 512])\n",
      "batch loss 2.826758248465402\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 15, 512])\n",
      "batch loss 2.734422810872396\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 24, 512])\n",
      "batch loss 2.7616345087687173\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 9, 512])\n",
      "batch loss 2.6568548414442272\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 2.6661174998563877\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 15, 512])\n",
      "batch loss 2.824876403808594\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 12, 512])\n",
      "batch loss 2.8901189168294272\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 37, 512])\n",
      "batch loss 2.527559641245249\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 14, 512])\n",
      "batch loss 2.7748639242989674\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 12, 512])\n",
      "batch loss 2.728139559427897\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 18, 512])\n",
      "batch loss 2.7586405012342663\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 2.6706491358139934\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 30, 512])\n",
      "batch loss 2.9260635375976562\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 8, 512])\n",
      "batch loss 2.6365885734558105\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 12, 512])\n",
      "batch loss 2.7859153747558594\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 17, 512])\n",
      "batch loss 2.778110952938304\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 13, 512])\n",
      "batch loss 2.902124258188101\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 27, 512])\n",
      "batch loss 2.568007575141059\n",
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for index,(img,paddedSeqs,seqLengths) in (enumerate(trainLoader)):\n",
    "        seqLengths= list(map(int,seqLengths))\n",
    "        \n",
    "        cnnOut = vgg16(img)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss= 0\n",
    "        \n",
    "        #forward propagation \n",
    "        logProbs,batch_size= model.forward(paddedSeqs,h_0,c_0,cnnOut,seqLengths)\n",
    "        \n",
    "        for i in range(batch_size.size(0)):\n",
    "            loss += lossCriterion(logProbs[i],paddedSeqs[:batch_size[i].item(),i])\n",
    "        #backward propagation\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_value_(model.parameters(),10)\n",
    "        \n",
    "        print('batch loss {}'.format(loss.cpu().item()/batch_size.size(0)))\n",
    "        #update\n",
    "        optimizer.step()\n",
    "        \n",
    "        #update\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
